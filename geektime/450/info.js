const firstArticle = 430019;

const chapters = {"430019":{"text_read_version":0,"audio_size":10261983,"article_cover":"https://static001.geekbang.org/resource/image/37/0d/374436611f9304e804fd24a26cf6a40d.jpg","subtitles":[],"product_type":"c1","audio_dubber":"海纳","is_finished":false,"like":{"had_done":false,"count":38},"audio_time":"00:10:40","video_height":0,"article_content":"<p>你好，我是海纳，欢迎来到《编程高手必学的内存知识》的课堂。</p><p>我曾经是Huawei JDK团队的负责人，现在是华为鲲鹏生态的布道师，同时还在负责华为编译器领域的相关创新工作。2019年我出版了《自己动手写Python虚拟机》一书。这两年，我利用业余时间又筹备了一本《从零开始写Linux内核》，预计明年上市。</p><p>计算机内存可以说是我的老朋友了。我和它结缘于我对基础软件开发的热爱。2007年，我本科毕业，在人大金仓从事国产数据库开发。</p><p>记得有一次，我在公司看B+树代码。我之前一直都搞不懂B+树为什么要这么设计，直到那段时间，我研究Linux内核代码，看到磁盘IO和页缓存算法时才恍然大悟。我发现，原来我对很多技术的不理解，是来自于我计算机内存管理这些知识的匮乏。</p><p>后来，我又去读了研。2012年，研究生毕业后，我去了网易做游戏开发，偏离了我曾经热爱的基础软件开发领域，但我居然又遇见了我的“老朋友”，内存管理。网易大量的游戏逻辑都是使用Python虚拟机构建的，但是由于Python采用了引用计数法进行自动内存管理，使用不当的时候，很容易发生大量的模型文件因循环引用不能自动释放的问题。</p><p>为了解决这个问题，我就希望能在Python虚拟机中引入基于复制的内存管理算法。所以，我就打开Python虚拟机的源代码，研究它的内存管理部分，做了一些尝试，虽然最终未能商业落地，但却收获了宝贵的经验。这些经验都总结在《自己动手写Python虚拟机》这本书里了。</p><!-- [[[read_end]]] --><p>两年后，我加入了华为，从事Huawei JDK的研发工作。再一次回到基础软件开发领域的我，又跟内存管理“杠”上了。那时，我遇到过一个非常棘手的问题。</p><p>这个问题的表象是，在鲲鹏CPU上，SynchronousQueue（SQ）会卡死线程。因为SQ这个结构使用了多个volatile变量，volatile的读写规则是在Java内存模型（Java Memory Model, JMM）中定义的，而华为鲲鹏芯片采用的是弱内存模型。所以，我们要解决这个问题，就不光要对JMM非常熟悉，还要对CPU的缓存管理有深入的理解。再加上，SQ的具体实现会经过HotSpot  C2编译器的优化，因此，我们同时还要搞清楚编译器是如何对内存操作进行翻译和优化的。</p><p>为了解决这个问题，我查阅了JSR133文档、CPU的设计手册、JVM的编译器设计和内存管理设计，还有多核CPU的操作系统设计等资料，深刻感受到想要分析这种涉及多个领域的问题，必须全面掌握内存相关的软硬件知识。</p><p>如果说，我与计算机内存的缘分，来自于我对基础软件开发和计算机底层原理的偏爱。那么今天，我想从更理性的、自身发展规划的角度看，和你谈谈为什么要系统地学习内存知识？</p><h2>为什么我建议你系统学习内存知识？</h2><p>现在，聊到程序员这个行业，逃不开的话题就是“职业生涯的35岁危机”。而且，你也能感受到，每年都有大量的人转行到计算机，初级的、写CRUD逻辑的人数已经逐渐见顶，出现供大于求的情况。</p><p>但是另一方面，结构性缺人又是非常常见的现象，很多公司还是感觉很难招到合适的开发人员。就拿我们编译器领域来说，招聘合适的人才，周期往往都要拉长到一年。</p><p>这是因为，在互联网的早期发展阶段，国内的ICT公司更多还是直接使用欧美的基础设施和技术，例如大型服务器、操作系统、数据库、编程语言、编译器、网络协议等等。随着国内企业的规模越来越大，各种定制化的需求也层出不穷，很多企业越来越希望自己能维护和修改基础设施。所以，当前市场对能进行底层开发的系统级程序员的需求也就越来越大。这对我们个人来说，是一个突破职业瓶颈的好机会。</p><p>那么，如何从一个应用开发程序员成长为资深的系统级程序员呢？</p><p>绕不开的一点是，我们需要熟练掌握CPU的工作原理、操作系统原理、编译器原理、分布式软件、图形渲染和数据库原理。<strong>如果从这些庞大的知识体系中选择一条脉络的话，我还是会推荐以内存管理为线索去进行学习。</strong></p><p>举个例子，在C语言编写的程序中，一个变量要经过编译器、链接器、加载器和操作系统的进程管理，然后再经过CPU的MMU模块，才能最终出现在真正的物理内存里。如果你能把这个过程讲清楚了，那就说明你对编译、链接、加载、操作系统和CPU的工作原理有了相当的理解。所以说，内存管理的知识就相当于纲领，纲举则目张。</p><h2>为什么内存相关知识这么难学？</h2><p>从我个人的学习经历中也可以看出来，想学好内存管理，必须要掌握CPU核设计知识、CPU的段页式管理等知识，还要深入了解操作系统的内存管理模块、编译器的内存分配、并发锁、基础库的设计原理等等。整体的知识结构，你可以参考下面这张图：</p><p><img src=\"https://static001.geekbang.org/resource/image/6c/d2/6c8d90837c6b213b3eb927d1c1cb3bd2.jpg?wh=2000x1266\" alt=\"\"></p><p>这里面涉及的知识覆盖了《数字电路设计》、《计算机原理》、《微处理器架构》、《体系结构》、《操作系统》、《编译原理》，还有各种编译语言库和虚拟机等等。每一点拎出来，都是一门独立的课程。即使你是在国内最好的计算机院校，想在这些课程里都取得高分也是非常困难的。更不要说，这些课程还没有涉及最新、最前沿的体系结构。</p><p>所以，我们不难看出，内存相关的知识体系的特点是：</p><ol>\n<li><strong>内容非常多，涉及的学科特别多</strong>。软硬件的工作原理都得掌握才能学得好。</li>\n<li><strong>内容非常散，不成体系</strong>。高校里传统的课程都是分层水平设置的。想学好内存管理的知识，需要将它从不同的课程拉取出来，自己进行合并总结才能最终形成知识体系。</li>\n</ol><p>针对这些难点，我设计了这套课程，希望能避免水平式讲解方式难以形成体系的缺点，通过垂直化地学习软硬件相关知识，能让你快速地形成知识体系。</p><h2>这门课是怎么设计的？</h2><p>我们的课程分为三个部分：软件篇，硬件篇和自动内存管理篇。</p><h3>软件篇</h3><p>这一部分，<strong>我们会以操作系统为核心，将进程和内存的关系彻底讲清楚，</strong>让你对操作系统、编译器，以及应用程序的运行原理有深入的理解。这样，当你遇到进程crash时，分析coredump、查看内存映射等都能游刃有余。</p><p>具体来讲，操作系统是软硬件设计的核心，它管理着所有的硬件资源，同时又为各种运行在它上面的应用程序提供服务。而内存是计算机的核心资源之一，如何高效地管理内存是操作系统的基础任务，甚至可以说是最重要的任务，它深刻影响着应用程序使用内存的方式。</p><p>而且，编译器和应用程序都是围绕这个核心来构建的。所以，在具体的讲述上，我们会采取由核心向外延的方式，把软件篇的知识都给你串联起来，如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/24/32/2409476b2e9a8b35473ea5b2yy6dec32.jpg?wh=2000x1062\" alt=\"\"></p><p>另外，我们这个专栏所举的例子都是运行在Linux系统上的，采用的编译器是gcc，Java语言使用的JVM是Huawei JDK。我希望你在学习时候也准备好相关的运行环境，只要Linux内核不低于4.1，gcc版本不低于4.8，JDK版本不低于8就可以了。</p><h3>硬件篇</h3><p>这一部分，我们将会学习与内存相关的计算机体系架构，包括存储器电路、存储体系结构和多核CPU内存模型。</p><p>通过这部分的学习，你可以快速掌握缓存原理、多核CPU通信的核心知识，让你最大限度地使用好缓存，写出正确而高效的代码。同时，我们课程里对硬件知识的讲解，还可以让你掌握CPU设计一些简单原理，并且在此基础上，可以深入地学习RISC-V等开源CPU的实现。</p><p>我们都知道，随着CPU主频的提升越来越困难，厂商的主流做法是使用更多的CPU和更多的核对计算进行并行加速。这个时候，如何保证CPU核间的数据同步就成了硬件工程师的一大挑战，系统程序员必须得对这些原理有足够了解，才能写出正确高效的代码。</p><p>在讲存储器电路的时候，你会发现，根据不同的物理特性原理，人们制作了不同的存储器件，有的成本高、容量小，但速度快；有的则是成本低、容量大，但速度慢。而如何合理使用不同的器件构建一套高效的系统，让各种存储器件“扬长避短”，就是我们在计算机体系结构方面要重点研究的内容。</p><p>从下面这张图你也可以看到，硬件的结构是从下到上层层搭建的。所以，我会采用自底向上的讲解方式，带你理解存储系统是怎样由简单的器件一步步变得越来越复杂的。</p><p><img src=\"https://static001.geekbang.org/resource/image/7e/09/7e8d1cb36b7392bd9f58e76db224f009.jpg?wh=2000x1062\" alt=\"\"></p><h3>自动内存管理篇</h3><p>在这一部分，我们的关注点会从计算机软硬件，转向对具体内存管理算法的学习。</p><p>学习内存管理算法，能够让我们正确地使用各种不同的语言，例如在Python和Swift中你要注意解循环引用等等。而且，你还可以通过学习原理处理各种现实的问题，例如我们前面提到的Java程序的STW问题，等等。</p><p>我们知道，Java、Go、Python、JavaScript等语言的内存都是自动管理的，也就是由语言虚拟机托管的，所以，开发者不需要关心内存的申请和释放的问题，我们把这种类型的语言称为Managed Language。</p><p>自动内存管理极大地减少了程序员的心智负担。但是，作为程序员，如果你不了解自动内存管理的原理，还是会遇到各种各样的问题，比如，垃圾回收时stop the world带来的服务器失去响应、out of memory error等等。这也是为什么，JVM的内存管理一直都是Java程序员的热门话题。在这个课程里，我们将在内存管理篇把这部分内容一次性讲透。</p><p>总的来说，内存管理算法一直在不断克服以前算法的缺点，持续向前演进，而我们这部分整体的讲述逻辑，就是按照算法的演进逻辑进行的。</p><p>以上就是软件篇，硬件篇和自动内存管理篇的主要内容，整个课程的详细目录，我也放在了这里，你可以看一下：</p><p><img src=\"https://static001.geekbang.org/resource/image/73/87/731233583a82be2yy538ff6d0e98e687.png?wh=750x2146\" alt=\"\"></p><p>在最后，我想说成长为系统程序员，不仅仅能帮你突破职业瓶颈，提升个人的薪酬待遇，更重要的是，这是顺应国情和行业发展大趋势的选择。在当前复杂的国际环境下，我们要走出一条自主可控的芯片设计和基础软件设计之路，这是迫在眉睫的事情。我希望，未来能有越来越多的开发者从事CPU设计，操作系统和编译器开发。如果你也是这么想的，那么这个专栏可能就是你最好的启蒙，一起开始吧。</p>","float_qrcode":"https://static001.geekbang.org/resource/image/83/1c/83e244537b714e899fd2e7896e22371c.png","article_cover_hidden":false,"is_required":true,"score":"11635151200","like_count":39,"article_subtitle":"","audio_download_url":"https://static001.geekbang.org/resource/audio/79/16/798ab2e2434dcca51a77829406e12116.mp3","had_viewed":false,"article_title":"开篇词｜为什么你要系统学习计算机的内存知识？","column_bgcolor":"#F6F7FB","offline_package":"{}","audio_title":"编程高手必学的内存知识.MP3","audio_time_arr":{"m":"10","s":"40","h":"00"},"text_read_percent":0,"cid":450,"article_cshort":"<p>你好，我是海纳，欢迎来到《编程高手必学的内存知识》的课堂。</p><p>我曾经是Huawei JDK团队的负责人，现在是华为鲲鹏生态的布道师，同时还在负责华为编译器领域的相关创新工作。2019年我出版了《自己动手写Python虚拟机》一书。这两年，我利用业余时间又筹备了一本《从零开始写Linux内核》，预计明年上市。</p><p>计算机内存可以说是我的老朋友了。我和它结缘于我对基础软件开发的热爱。2007年，我本科毕业，在人大金仓从事国产数据库开发。</p><p>记得有一次，我在公司看B+树代码。我之前一直都搞不懂B+树为什么要这么设计，直到那段时间，我研究Linux内核代码，看到磁盘IO和页缓存算法时才恍然大悟。我发现，原来我对很多技术的不理解，是来自于我计算机内存管理这些知识的匮乏。</p><p>后来，我又去读了研。2012年，研究生毕业后，我去了网易做游戏开发，偏离了我曾经热爱的基础软件开发领域，但我居然又遇见了我的“老朋友”，内存管理。网易大量的游戏逻辑都是使用Python虚拟机构建的，但是由于Python采用了引用计数法进行自动内存管理，使用不当的时候，很容易发生大量的模型文件因循环引用不能自动释放的问题。</p><p>为了解决这个问题，我就希望能在Python虚拟机中引入基于复制的内存管理算法。所以，我就打开Python虚拟机的源代码，研究它的内存管理部分，做了一些尝试，虽然最终未能商业落地，但却收获了宝贵的经验。这些经验都总结在《自己动手写Python虚拟机》这本书里了。</p>","video_width":0,"column_could_sub":true,"video_id":"","sku":"100094901","video_cover":"","author_name":"海纳","column_is_onboard":true,"audio_url":"https://res001.geekbang.org/media/audio/79/16/798ab2e2434dcca51a77829406e12116/ld/ld.m3u8","chapter_id":"2312","column_had_sub":true,"column_cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg","neighbors":{"left":[],"right":{"article_title":"导学（一）| 拆解CPU的基本结构和运行原理","id":430173}},"rate_percent":0,"footer_cover_data":{"img_url":"https://static001.geekbang.org/resource/image/f0/fe/f0c47daf3262a8df6cf352d89896abfe.png","link_url":"https://time.geekbang.org/article/427012","mp_url":""},"float_app_qrcode":"","column_is_experience":false,"rate":{"1":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"2":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"3":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0}},"product_id":100094901,"had_liked":false,"id":430019,"free_get":false,"is_video_preview":false,"article_summary":"如果从这些庞大的知识体系中选择一条脉络的话，我还是会推荐以内存管理为线索去进行学习。","column_sale_type":0,"float_qrcode_jump":"https://time.geekbang.org/serv/v4/misc/jump?uri=https%3A%2F%2Ftime.geekbang.org%2Fhybrid%2Fmp%2Fjump%3Furl%3Dhttps%253A%252F%252Fstatic001.geekbang.org%252Fresource%252Fimage%252Fee%252Fb2%252Feedd4090668aabfac91b2b445d27cbb2.png","column_id":450,"article_sharetitle":"为什么你要系统学习计算机的内存知识？","article_poster_wxlite":"https://static001.geekbang.org/render/screen/dd/7e/dd88f5cb5d1cd2535b27b4b394a8697e.jpeg","article_features":0,"comment_count":9,"audio_md5":"798ab2e2434dcca51a77829406e12116","offline":{"size":11380953,"file_name":"e79f608a5e743947320176e29b311f41","download_url":"https://static-acl-001.geekbang.org/resource/zip/article/430019/e79f608a5e743947320176e29b311f41.zip?auth_key=1641481997-045e81aa0dac455ca4560b9f311314d8-0-0216f8ebfb1e788841225739e9cf3e23"},"video_size":0,"hls_videos":[],"video_time":"","article_could_preview":true,"article_ctime":1635151200,"share":{"content":"学好内存，掌握系统开发诀窍","title":"海纳 · 编程高手必学的内存知识","poster":"https://static001.geekbang.org/resource/image/bc/c5/bcaf3ff69e538f8749970edf2301bbc5.jpg","cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg"}},"430073":{"text_read_version":0,"audio_size":16573462,"article_cover":"https://static001.geekbang.org/resource/image/4c/cc/4c1bd6870b2ec0f19765987cc1ab67cc.jpg","subtitles":[],"product_type":"c1","audio_dubber":"海纳","is_finished":false,"like":{"had_done":false,"count":19},"audio_time":"00:17:15","video_height":0,"article_content":"<p>你好，我是海纳。</p><p>今天是我们的第1节课，我想用一个比较有趣的、很多人都遇到过的问题作为我们这门课的开场，带你正式迈入计算机内存的学习课堂。</p><p>我不知道在你刚接触计算机的时候，有没有这么一个疑问：“为什么我的机器上只有两个G的物理内存，但我却可以使用比这大得多的内存，比如256T？”</p><p>反正我当时还是挺疑惑的，不过现在我可以来告诉你这个答案了。这个问题背后的机制是十分复杂的，但它的核心是计算机中物理内存和虚拟内存的关系，尤其是虚拟内存的运行原理。只要你搞懂了它们，这个问题也就迎刃而解了。</p><p>不止如此，虚拟内存的运行原理还是打开计算机底层知识大门的钥匙，只有掌握好它，我们才能继续学习更多的底层原理。我们整个课程的目的，就是让你在遇到进程崩溃、内存访问错误、SIGSEGV、double free、内存泄漏等与内存相关的错误时，可以有的放矢，把握分析问题的方向。而今天的第一课就是把打开这扇门的钥匙交到你手上。</p><p>在回答虚拟内存的相关问题之前，我们需要先看看物理内存的含义。</p><h2>物理内存</h2><p>计算机的物理内存，简单说就是那根内存条，你的内存条是1G的，那计算机可用的物理内存就是1G。这个内存条加电以后就可以存储数据了，CPU运算的数据都是存储在主存里的。</p><!-- [[[read_end]]] --><p>计算机的主存是由多个连续的单元组成的，每个单元称为一个字节，每个字节都有一个唯一的物理地址(Physical Address， PA)，地址编码是从0开始的。所以，如果计算机上配有2G的内存，那么，这个计算机可用的物理内存空间就是0到2G。</p><p>在早期的CPU指令集里，从内存中加载数据，向内存中写入数据都是直接操作物理内存的。也就是说每一个数据存储在内存的什么位置，都由程序员自己负责。例如，8086这款40年前的CPU的mov指令就可以直接访问物理内存。至今，X86架构的CPU在上电以后，为了与8086保持兼容，还是运行在16位实模式下，实模式的特点是所有访存指令访问的都是物理内存地址。你可以先看看这条代码：</p><pre><code>movb ($0x10), %ax\n</code></pre><p>这条汇编代码的作用，就是将物理地址为0x10的那个字节里的内容送入到ax寄存器。（实际上，这里默认使用了数据段寄存器，但并不影响我们理解物理地址（内存）的概念。关于段寄存器，我们下节课会讲解）。不过这里你要注意，上面这句代码是AT&amp;T风格的汇编代码，与Intel风格的汇编不同，其目标操作数和源操作数的位置是相反的。</p><p>但是直接访问物理内存，存在着一个很大的问题。</p><p>因为这种模式下，必然要求程序员手动对数据进行布局，那么内存不够用怎么办呢？而且，每个进程分配多少内存、如何保证指令中访存地址的正确性，这些问题都全部要程序员来负责。</p><p>这是难以忍受的。随着我们后面的讲解，你会发现，如果上面这些工作都全部交由开发者手动来做的话，就相当于每一个开发者要把linker和loader的事情从头做一遍，效率会非常低。</p><p>那既然直接访问物理内存效率那么低，现在还有开发人员用这种模式吗？</p><p>其实也还是有的。在嵌入式设备中，手动管理内存的操作还是广泛存在的。这是因为在嵌入式开发中，往往没有进程的概念，也就是说整个应用独享全部内存，所以手动管理内存才有可能性。在单进程的系统中，所有的物理资源都是单一进程在管理，直接管理物理内存的操作复杂度还可以接受。尽管如此，嵌入式开发中手动管理内存仍然是一项对程序员要求极高的工作。</p><p>不过，对于我们普通软件工程师来说，系统中经常有多个进程，多进程之间的协同分配内存和释放内存就没那么容易了，这个时候我们要怎么办呢？</p><p>幸好，局部性原理成了我们的救命稻草。基于局部性原理，CPU为程序员虚拟化了一层内存，我们只需要与虚拟内存打交道就可以了。所以接下来，我们就来讨论局部性原理说的是什么，聪明的CPU设计人员又是如何将这个原理完美应用的。</p><h2>局部性原理</h2><p>在绝大多数程序的运行过程中，当前指令大概率都会引用最近访问过的数据。也就是说，程序的数据访问会表现出明显的倾向性。这种倾向性，我们就称之为局部性原理(Principle of locality)。</p><p>我们可以从两个方面来理解局部性原理。第一个方面是时间局部性，也就是说被访问过一次的内存位置很可能在不远的将来会被再次访问；另一方面是空间局部性，说的是如果一个内存位置被引用过，那么它邻近的位置在不远的将来也有很大概率会被访问。</p><p>基于这个原理，我们可以做出一个合理的推论：<strong>无论一个进程占用的内存资源有多大，在任一时刻，它需要的物理内存都是很少的</strong>。在这个推论的基础上，CPU为每个进程只需要保留很少的物理内存就可以保证进程的正常执行了。</p><p>而且，为了让程序员编程方便，CPU和操作系统还联手编织了一个假象：<strong>每个进程都独享128T的虚拟内存空间，并且每个进程的地址空间都是相互隔离的</strong>。什么意思呢？比如说，现在进程A中有个变量a，它的地址是0x100，但是进程B中也有个变量b，它的地址也是0x100。但这并不会造成冲突，因为进程A的地址空间与进程B的地址空间是独立的，相互不影响。</p><p>这就极大地解放了程序员的生产力。我们可以对比一下直接操作物理内存和操作虚拟内存，程序员要关心的事情都有哪些。</p><p>在直接操作物理内存的情况下，你需要知道每一个变量的位置都安排在了哪里，而且还要注意和当前这个进程同时工作的进程，不能共用同一个地址，否则就会造成地址冲突。你想，一个项目中会有成百万的变量和函数，我们都要给它计算一个合理的位置，还不能与其他进程冲突，这是根本不可能完成的任务。</p><p>而直接操作虚拟内存的情况就变得简单多了。你可以独占128T内存，任意地使用，系统上还运行了哪些进程已经与我们完全没有关系了。为变量和函数分配地址的活，我们交给链接器去自动安排就可以了。这一切都是因为虚拟内存能够提供内存地址空间的隔离，极大地扩展了可用空间。</p><p>这是什么意思呢？就是说虚拟内存不仅让每个进程都有独立的、私有的内存空间，而且这个地址空间比可用的物理内存要大得多。不过，任何一个虚拟内存里所存储的数据，还是保存在真实的物理内存里的。换句话说，<strong>任何虚拟内存最终都要映射到物理内存，但虚拟内存的大小又远超真实的物理内存的大小</strong>。</p><p>那虚拟内存具体是怎么做到的呢？</p><h2>虚拟内存与程序局部性原理</h2><p>答案很简单，就是CPU充分利用程序局部性原理，提出了虚拟内存和物理内存的映射(Mapping)机制。这也是我们开头那个问题的答案，更具体的原理，我们接着往下看。</p><p>操作系统管理着这种映射关系，所以你在写代码的时候，就不用再操心物理内存的使用情况了，你看到的内存就是虚拟内存。</p><p>这种映射关系是以页为单位的。你看看下面这张图就很好理解了，多个进程的虚拟内存中的页都被映射到物理内存页上。</p><p><img src=\"https://static001.geekbang.org/resource/image/4b/48/4bae735761c77bd0efa26974c8f53548.jpg?wh=2284x1238\" alt=\"\"></p><p>我希望你可以从图中看到这两点。第一，虽然虚拟内存提供了很大的空间，但实际上进程启动之后，这些空间并不是全部都能使用的。开发者必须要使用malloc等分配内存的接口才能将内存从待分配状态变成已分配状态。</p><p>在你得到一块虚拟内存以后，这块内存就是未映射状态，因为它并没有被映射到相应的物理内存，直到对该块内存进行读写时，操作系统才会真正地为它分配物理内存。然后这个页面才能成为正常页面。</p><p>第二，在虚拟内存中连续的页面，在物理内存中不必是连续的。只要维护好从虚拟内存页到物理内存页的映射关系，你就能正确地使用内存了。这种映射关系是操作系统通过页表来自动维护的，不必你操心。</p><p>不过你还要注意一点，计算机的虚拟内存大小是不一样的。虚拟地址空间往往与机器字宽有关系。例如32位机器上，指向内存的指针是32位的，所以它的虚拟地址空间是2的32次方，也就是4G。在64位机器上，指向内存的指针就是64位的，但在64位系统里只使用了低48位，所以它的虚拟地址空间是2的48次方，也就是256T。</p><h3>页表的结构</h3><p>不过，虽然大多数情况下，CPU和操作系统会一起完成页面的自动映射，不需要你关心其中的机制。但是当我们在做系统性能优化的时候，理解内存映射的过程就是十分必要的了。</p><p>例如，我就曾经遇到过一个性能很差的程序，经过perf工具分析后，我发现是因为缺页中断过多导致的。这个时候，那么掌握页的结构和映射过程的知识就非常有必要了。所以我也想跟你来探讨一下这方面的内容。</p><p>我们刚才也说了，映射的过程，是由CPU的内存管理单元(Memory Management Unit, MMU)自动完成的，但它依赖操作系统设置的页表。</p><p>页表的本质是页表项(Page Table Entry, PTE)的数组，虚拟空间中的每一个页在页表中都有一个PTE与之对应，PTE中会记录这个虚拟内存页所对应的实际物理页的起始地址。为方便理解，我这举了个例子，下面这张图描述的是i7处理器中的页面映射机制。</p><p><img src=\"https://static001.geekbang.org/resource/image/53/ef/534bf413b66765e1a9cc8d79b62b75ef.jpg?wh=2000x1062\" alt=\"\"></p><p>你可以看到，i7处理器的页表也是存储在内存页里的，每个页表项都是4字节。所以，人们就将1024个页表项组成一张页表。这样一张页表的大小就刚好是4K，占据一个内存页，这样就更加方便管理。而且，当前市场上主流的处理器也都选择将页大小定为4K。</p><p>一个页表项对应着一个大小为4K的页，所以1024个页表项所能支持的空间就是4M。那为了编码更多地址，我们必须使用更多的页表。而且，为了管理这些页表，我们还可以继续引入页表的数组：<strong>页目录表</strong>。</p><p>页目录表中的每一项叫做页目录项(Page Directory Entry, PDE)，每个PDE都对应一个页表，它记录了页表开始处的物理地址，这就是多级页表结构。现代的64位处理器上，为了编码更大的空间，还存在更多级的页表。</p><p><img src=\"https://static001.geekbang.org/resource/image/38/34/388b34c8942f87ef87c51fyy99fd6d34.jpg?wh=2000x1062\" alt=\"\"></p><p>好了，我们现在已经搞清楚页面映射的机制原理了，那接下来，我们再用一个例子让你更具体地感受一下页面映射的过程。为了论述方便，我们以32位操作系统为例，看看CPU是如何通过一个虚拟地址找到物理内存中的真实位置的。</p><h3>一个CPU怎么找到真实地址？</h3><p>一个CPU要通过虚拟地址，找到物理地址需要几个步骤呢？大概是下面这四个。</p><p><img src=\"https://static001.geekbang.org/resource/image/9e/57/9eed9b0e3e7823c28d80c36e535e5d57.jpg?wh=2000x1266\" alt=\"\"></p><p><strong>第一步是确定页目录基址</strong>。每个CPU都有一个页目录基址寄存器，最高级页表的基地址就存在这个寄存器里。在X86上，这个寄存器是CR3。每一次计算物理地址时，MMU都会从CR3寄存器中取出页目录所在的物理地址。</p><p><strong>第二步是定位页目录项（PDE）</strong>。一个32位的虚拟地址可以拆成10位，10位和12位三段，上一步找到的页目录表基址加上高10位的值乘以4，就是页目录项的位置。这是因为，一个页目录项正好是4字节，所以1024个页目录项共占据4096字节，刚好组成一页，而1024个页目录项需要10位进行编码。这样，我们就可以通过最高10位找到该地址所对应的PDE了。</p><p><strong>第三步是定位页表项（PTE）</strong>。页目录项里记录着页表的位置，CPU通过页目录项找到页表的位置以后，再用中间10位计算页表中的偏移，可以找到该虚拟地址所对应的页表项了。页表项也是4字节的，所以一页之内刚好也是1024项，用10位进行编码。所以计算公式与上一步相似，用页表基址加上中间10位乘以4，可以得到页表项的地址。</p><p><strong>最后一步是确定真实的物理地址</strong>。上一步CPU已经找到页表项了，这里存储着物理地址，<strong>这才真正找到该虚拟地址所对应的物理页</strong>。虚拟地址的低12位，刚好可以对一页内的所有字节进行编码，所以我们用低12位来代表页内偏移。计算的公式是物理页的地址直接加上低12位。</p><p>前面我们分析的是32位操作系统，那对于64位机器是不是有点不同呢？在64位的机器上，使用了48位的虚拟地址，所以它需要使用4级页表。它的结构与32位的3级页表是相似的，只是多了一级页目录，定位的过程也从32位的4步变成了5步。这个你可以课后自己去分析一下。</p><h3>页面的换入换出</h3><p>不过我们前面也说到，由于程序运行符合局部性原理，CPU访问内存会有很明显的重复访问的倾向性。那对于那些没有被经常使用到的内存，我们可以把它换出到主存之外，比如硬盘上的swap区域。新的虚拟内存页可以被映射到刚腾出来的这个物理页。这就涉及到了页面换入换出的调度问题。</p><p>我们举个例子来说明一下。假如进程A一开始将虚拟内存的0至4K，映射到物理内存的0至4K空间。基于局部性原理，4K以后的虚拟地址大概率是不会被访问的，我们可以让程序一直运行。</p><p>直到程序开始访问4K ~ 8K之间的虚拟地址了，我们就可以将现在的物理地址里的内容换出到磁盘的swap区域，然后再将虚拟内存的4K ~ 8K这一个区域映射到0~4K的这一块物理内存。在理想情况下，虽然进程A的虚拟内存非常大，比如256T，但CPU只需要一个4K大小的物理内存页就能满足它的需求了。</p><p>当然在实际情况中肯定不会这么理想，所以一个进程所占用的物理内存不可能只有一个页。从效率的角度看，当物理内存足够时，操作系统也会尽量让尽可能多的页驻留在物理内存中。毕竟将内存中的数据写到磁盘里是非常耗时的操作。</p><p>如何能最大化地在空间和时间上都取得平衡，这就要精心地设计页面的调度算法。我们会在第9节课讲解如何通过缺页中断来进行页的分配回收和调度。</p><h2>总结</h2><p>好了，到这里我们今天这节课的内容讲完了，我们再来简单回顾一下。</p><p>虚拟内存是软硬件一体化设计的一个典型代表。围绕虚拟内存这个核心概念，CPU，操作系统，编译器等所有的软硬件都在不断地进化。举个例子，我们遇到进程coredump的时候，使用gdb去查看内存时，看到的地址全都是虚拟内存的，如果你没有掌握虚拟内存这个概念的话，在排查一些隐藏得很深的BUG时，就会无从下手。</p><p>虚拟内存的出现，是为了解决直接操作物理内存的系统无法支持多进程的问题。这里的难点主要是进程的地址空间非常小，而且多个进程的地址很容易发生冲突。所以在局部性原理的基础上，CPU设计者提出虚拟内存的方案将多个进程的地址空间隔离开，并且提供了巨大的内存空间。</p><p>我们可以总结一下，虚拟内存主要有下面两个特点：</p><p>第一，由于每个进程都有自己的页表，所以每个进程的虚拟内存空间就是相互独立的。进程也没有办法访问其他进程的页表，所以这些页表是私有的。这就解决了多进程之间地址冲突的问题。</p><p>第二，PTE中除了物理地址之外，还有一些标记属性的比特，比如控制一个页的读写权限，标记该页是否存在等。在内存访问方面，操作系统提供了更好的安全性。</p><p>另外，虚拟内存可以充分使用CPU提供的机制来完成很多重要的任务。例如，fork借用写保护来实现写时复制，JVM中借用改变某一个页的读权限来实现safepoint查询等等。这些内容我们都会在以后的课程加以介绍。</p><p>由于CPU对内存提供了更多保护的能力，所以X86架构的CPU把这种工作模式称为<strong>保护模式</strong>，与可以直接访问物理内存的<strong>实模式</strong>形成了对比。除此之外，虚拟内存还有很多的好处在我们后面的课程中都会慢慢展开，你可以先自己思考一下。</p><h2>思考题</h2><p>Linux操作系统会为每一个进程都在/proc目录下创建一个目录，目录名就是进程号。我们可以通过打开这个目录下的一些文件来查看该进程的内存使用情况，例如：</p><pre><code>$ cat /proc/1464/maps\n</code></pre><p>上述命令就是查看1464号进程的内存映射的情况。</p><ol>\n<li>请你仿照上述例子自己创建一个进程，并查看该进程的maps和smaps文件。</li>\n<li>查找资料，确认这个目录下面各个文件的作用。<br>\n欢迎你在留言区和我交流你的想法，我在留言区等你。</li>\n</ol><p><img src=\"https://static001.geekbang.org/resource/image/20/dc/205d4e678f9acfba4d92cecb618dcddc.jpg?wh=2284x1361\" alt=\"\"></p><p>好啦，这节课到这就结束啦。欢迎你把这节课分享给更多对计算机内存感兴趣的朋友。我是海纳，我们下节课再见！</p>","float_qrcode":"https://static001.geekbang.org/resource/image/83/1c/83e244537b714e899fd2e7896e22371c.png","article_cover_hidden":false,"is_required":true,"score":"31635151620","like_count":19,"article_subtitle":"","audio_download_url":"https://static001.geekbang.org/resource/audio/38/c1/385856e116f317469b5d9e9b8877e3c1.mp3","had_viewed":false,"article_title":"01｜虚拟内存：为什么可用内存会远超物理内存？","column_bgcolor":"#F6F7FB","offline_package":"{}","audio_title":"10.24 海纳01_virtual_mem.MP3_R.mp3","audio_time_arr":{"m":"17","s":"15","h":"00"},"text_read_percent":0,"cid":450,"article_cshort":"<p>你好，我是海纳。</p><p>今天是我们的第1节课，我想用一个比较有趣的、很多人都遇到过的问题作为我们这门课的开场，带你正式迈入计算机内存的学习课堂。</p><p>我不知道在你刚接触计算机的时候，有没有这么一个疑问：“为什么我的机器上只有两个G的物理内存，但我却可以使用比这大得多的内存，比如256T？”</p><p>反正我当时还是挺疑惑的，不过现在我可以来告诉你这个答案了。这个问题背后的机制是十分复杂的，但它的核心是计算机中物理内存和虚拟内存的关系，尤其是虚拟内存的运行原理。只要你搞懂了它们，这个问题也就迎刃而解了。</p><p>不止如此，虚拟内存的运行原理还是打开计算机底层知识大门的钥匙，只有掌握好它，我们才能继续学习更多的底层原理。我们整个课程的目的，就是让你在遇到进程崩溃、内存访问错误、SIGSEGV、double free、内存泄漏等与内存相关的错误时，可以有的放矢，把握分析问题的方向。而今天的第一课就是把打开这扇门的钥匙交到你手上。</p><p>在回答虚拟内存的相关问题之前，我们需要先看看物理内存的含义。</p><h2>物理内存</h2><p>计算机的物理内存，简单说就是那根内存条，你的内存条是1G的，那计算机可用的物理内存就是1G。这个内存条加电以后就可以存储数据了，CPU运算的数据都是存储在主存里的。</p>","video_width":0,"column_could_sub":true,"video_id":"","sku":"100094901","video_cover":"","author_name":"海纳","column_is_onboard":true,"audio_url":"https://res001.geekbang.org/media/audio/38/c1/385856e116f317469b5d9e9b8877e3c1/ld/ld.m3u8","chapter_id":"2314","column_had_sub":true,"column_cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg","neighbors":{"left":{"article_title":"导学（三）| 一个CPU是怎么寻址的？","id":431373},"right":{"article_title":"02｜聊聊x86体系架构中的实模式和保护模式","id":431400}},"rate_percent":0,"footer_cover_data":{"img_url":"https://static001.geekbang.org/resource/image/f0/fe/f0c47daf3262a8df6cf352d89896abfe.png","link_url":"https://time.geekbang.org/article/427012","mp_url":""},"float_app_qrcode":"","column_is_experience":false,"rate":{"1":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"2":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"3":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0}},"product_id":100094901,"had_liked":false,"id":430073,"free_get":false,"is_video_preview":false,"article_summary":"任何虚拟内存最终都要映射到物理内存，但虚拟内存的大小又远超真实的物理内存的大小。","column_sale_type":0,"float_qrcode_jump":"https://time.geekbang.org/serv/v4/misc/jump?uri=https%3A%2F%2Ftime.geekbang.org%2Fhybrid%2Fmp%2Fjump%3Furl%3Dhttps%253A%252F%252Fstatic001.geekbang.org%252Fresource%252Fimage%252Fee%252Fb2%252Feedd4090668aabfac91b2b445d27cbb2.png","column_id":450,"article_sharetitle":"虚拟内存：为什么可用内存会远超物理内存？","article_poster_wxlite":"https://static001.geekbang.org/render/screen/1e/3e/1edb4f4a99f44dd96eb7745a4b46b43e.jpeg","article_features":0,"comment_count":36,"audio_md5":"385856e116f317469b5d9e9b8877e3c1","offline":{"size":17363670,"file_name":"573320bc8ed8fb76bd8df7e008e57660","download_url":"https://static-acl-001.geekbang.org/resource/zip/article/430073/573320bc8ed8fb76bd8df7e008e57660.zip?auth_key=1641482061-89b9885a9bed49ed9a679b91455d4ec5-0-b568ad20d0005f72314de058b8973b6b"},"video_size":0,"hls_videos":[],"video_time":"","article_could_preview":false,"article_ctime":1635151620,"share":{"content":"学好内存，掌握系统开发诀窍","title":"海纳 · 编程高手必学的内存知识","poster":"https://static001.geekbang.org/resource/image/bc/c5/bcaf3ff69e538f8749970edf2301bbc5.jpg","cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg"}},"430173":{"text_read_version":0,"audio_size":1568784,"article_cover":"https://static001.geekbang.org/resource/image/4e/35/4eb01630d9705eeb8365ac11797cf335.jpg","video_time_arr":{"m":"09","s":"24","h":"01"},"product_type":"c1","audio_dubber":"海纳","is_finished":false,"like":{"had_done":false,"count":16},"audio_time":"00:01:37","video_height":1080,"article_content":"<p>你好，我是海纳。</p><p>在开始正式的课程之前，我专门为你设置了三节导学课，目的是帮你打好基础，扫清一些知识盲区，这样你学习这个课程会更加轻松、高效。</p><p>导学部分共有三节课，都是以视频方式讲述的，内容比较多，但我保证干货十足，非常值得你花时间好好观看学习。</p><p>今天这节导学课，我们会来拆解CPU的基本结构和运行原理。我们知道，CPU 作为计算机的总司令官，它管理着计算机的所有资源。它有两个主要的作用，分别是计算和控制。其中，计算主要是指逻辑数值运算，控制则体现在对数据传输，输入输出的控制上。可以想象，CPU在内存管理方面一定发挥着重要的作用。</p><p>CPU的基本架构，包含了运算器、寄存器、内存管理单元等模块。我在视频中会对这些模块的构成原理、运行原理以及其作用进行介绍。</p><p>了解了基本架构之后，我们还会从CPU运行的角度出发，来深入理解机器码和中断的原理。为什么我要给你讲机器码和中断呢？</p><p>这是因为程序员可以通过编程来指挥CPU为人们工作，而对 CPU 发号施令的就是机器码。此外，CPU 在与外设交互时，最重要的机制就是中断，内存、磁盘 IO、网络 IO 有很多功能都是依赖中断完成的，所以，不管是系统级程序员，还是应用开发程序员，深刻地理解中断也是你的必备技能之一。</p><!-- [[[read_end]]] --><p>好，不啰嗦了，现在就请你点开下面的视频，来学习今天的课程吧！</p><p><video poster=\"https://media001.geekbang.org/3ca2b614cc6c4fbcab0b6ba3a25e1f64/snapshots/d241fb180a394c4c98a2e7aac70639fe-00005.jpg\" preload=\"none\" controls=\"\"><source src=\"https://media001.geekbang.org/customerTrans/7e27d07d27d407ebcc195a0e78395f55/28f46a46-17ccae5d807-0000-0000-01d-dbacd.mp4\" type=\"video/mp4\"><source src=\" https://media001.geekbang.org/647d5951db0048dbbc7875e328e18e91/0c99a4dc3b204557ab88e1f750dfd208-97d79ddd66e04dddc645e76aacb835e0-sd.m3u8\" type=\"application/x-mpegURL\"></video></p><p>点击<a href=\"https://pan.baidu.com/s/1Zg0YBri3TJJasOC2KX3spQ\">这里</a>获取课件，提取码：j9e5。</p>","float_qrcode":"https://static001.geekbang.org/resource/image/83/1c/83e244537b714e899fd2e7896e22371c.png","article_cover_hidden":false,"is_required":false,"score":"21635151380","like_count":18,"article_subtitle":"","audio_download_url":"https://static001.geekbang.org/resource/audio/77/ec/7730da0487bc6471b52e819b1ce40aec.mp3","had_viewed":false,"article_title":"导学（一）| 拆解CPU的基本结构和运行原理","column_bgcolor":"#F6F7FB","offline_package":"{}","audio_title":"11.29 海纳导学1.MP3","audio_time_arr":{"m":"01","s":"37","h":"00"},"text_read_percent":0,"cid":450,"article_cshort":"<p>你好，我是海纳。</p><p>在开始正式的课程之前，我专门为你设置了三节导学课，目的是帮你打好基础，扫清一些知识盲区，这样你学习这个课程会更加轻松、高效。</p><p>导学部分共有三节课，都是以视频方式讲述的，内容比较多，但我保证干货十足，非常值得你花时间好好观看学习。</p><p>今天这节导学课，我们会来拆解CPU的基本结构和运行原理。我们知道，CPU 作为计算机的总司令官，它管理着计算机的所有资源。它有两个主要的作用，分别是计算和控制。其中，计算主要是指逻辑数值运算，控制则体现在对数据传输，输入输出的控制上。可以想象，CPU在内存管理方面一定发挥着重要的作用。</p><p>CPU的基本架构，包含了运算器、寄存器、内存管理单元等模块。我在视频中会对这些模块的构成原理、运行原理以及其作用进行介绍。</p><p>了解了基本架构之后，我们还会从CPU运行的角度出发，来深入理解机器码和中断的原理。为什么我要给你讲机器码和中断呢？</p><p>这是因为程序员可以通过编程来指挥CPU为人们工作，而对 CPU 发号施令的就是机器码。此外，CPU 在与外设交互时，最重要的机制就是中断，内存、磁盘 IO、网络 IO 有很多功能都是依赖中断完成的，所以，不管是系统级程序员，还是应用开发程序员，深刻地理解中断也是你的必备技能之一。</p>","video_width":1920,"column_could_sub":true,"video_id":"3ca2b614cc6c4fbcab0b6ba3a25e1f64","sku":"100094901","video_cover":"https://media001.geekbang.org/3ca2b614cc6c4fbcab0b6ba3a25e1f64/snapshots/d241fb180a394c4c98a2e7aac70639fe-00005.jpg","author_name":"海纳","column_is_onboard":true,"audio_url":"https://res001.geekbang.org/media/audio/77/ec/7730da0487bc6471b52e819b1ce40aec/ld/ld.m3u8","chapter_id":"2313","column_had_sub":true,"column_cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg","neighbors":{"left":{"article_title":"开篇词｜为什么你要系统学习计算机的内存知识？","id":430019},"right":{"article_title":"导学（二）| 汇编语言是怎么一回事？","id":430184}},"rate_percent":0,"footer_cover_data":{"img_url":"https://static001.geekbang.org/resource/image/f0/fe/f0c47daf3262a8df6cf352d89896abfe.png","link_url":"https://time.geekbang.org/article/427012","mp_url":""},"float_app_qrcode":"","column_is_experience":false,"rate":{"1":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"2":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"3":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0}},"product_id":100094901,"had_liked":false,"id":430173,"free_get":false,"is_video_preview":true,"article_summary":"CPU做为计算机的总司令官，它管理着计算机的所有资源。它有两个主要的作用，分别是计算和控制。","column_sale_type":0,"float_qrcode_jump":"https://time.geekbang.org/serv/v4/misc/jump?uri=https%3A%2F%2Ftime.geekbang.org%2Fhybrid%2Fmp%2Fjump%3Furl%3Dhttps%253A%252F%252Fstatic001.geekbang.org%252Fresource%252Fimage%252Fee%252Fb2%252Feedd4090668aabfac91b2b445d27cbb2.png","subtitles":[],"column_id":450,"audio_md5":"7730da0487bc6471b52e819b1ce40aec","article_sharetitle":"拆解CPU的基本结构和运行原理","share":{"content":"学好内存，掌握系统开发诀窍","title":"海纳 · 编程高手必学的内存知识","poster":"https://static001.geekbang.org/resource/image/bc/c5/bcaf3ff69e538f8749970edf2301bbc5.jpg","cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg"},"video_time":"01:09:24","video_preview":{"sd":{"url":"https://res001.geekbang.org/files/vod/3ca2b614cc6c4fbcab0b6ba3a25e1f64/c8a63ca5ca9496b066909a6fcb753c05-sd.m3u8","size":8237972}},"article_could_preview":true,"comment_count":15,"article_poster_wxlite":"https://static001.geekbang.org/render/screen/59/bf/5904237808eaf505f59adae834af7fbf.jpeg","offline":{"size":2259547,"file_name":"47fbdf324624d643384868dc2e75c2e0","download_url":"https://static-acl-001.geekbang.org/resource/zip/article/430173/47fbdf324624d643384868dc2e75c2e0.zip?auth_key=1641482013-61b3682282454cd3b8fb406ad2bf4b0c-0-cb514cad358e66c55ad362a9a57aa8f1"},"video_size":738228695,"hls_videos":{"sd":{"url":"https://media001.geekbang.org/d2f8f4e87bf143a288558a82eb8c0b12/b00a48702400e2c28cf3e128be775ee8-ld-encrypt-stream.m3u8?MtsHlsUriToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJjb2RlIjoiYjllMTE0ZGMyMjAwNWQ2N2VmYjg2OGZhOWUzNThlMmQ3NWIxZmJjNCIsImV4cCI6MTY0MTQ4MDMzMzY1OSwiZXh0cmEiOnsidmlkIjoiZDJmOGY0ZTg3YmYxNDNhMjg4NTU4YTgyZWI4YzBiMTIiLCJhaWQiOjQzMDE3MywidWlkIjoyODgyMzgyLCJzIjoiIn0sInMiOjIsInQiOjEsInYiOjF9.Acv9-Gh1u4oBfqu9ngUP8HRIrIoqebzjMRPNxhNWa2g","size":644542960},"hd":{"url":"https://media001.geekbang.org/d2f8f4e87bf143a288558a82eb8c0b12/f56197238cb67425951a8119ec84bd22-sd-encrypt-stream.m3u8?MtsHlsUriToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJjb2RlIjoiYjllMTE0ZGMyMjAwNWQ2N2VmYjg2OGZhOWUzNThlMmQ3NWIxZmJjNCIsImV4cCI6MTY0MTQ4MDMzMzY1OSwiZXh0cmEiOnsidmlkIjoiZDJmOGY0ZTg3YmYxNDNhMjg4NTU4YTgyZWI4YzBiMTIiLCJhaWQiOjQzMDE3MywidWlkIjoyODgyMzgyLCJzIjoiIn0sInMiOjIsInQiOjEsInYiOjF9.Acv9-Gh1u4oBfqu9ngUP8HRIrIoqebzjMRPNxhNWa2g","size":1220796236},"ld":{"url":"https://media001.geekbang.org/d2f8f4e87bf143a288558a82eb8c0b12/cad8841e377cc15eb42d59b5ead01acc-fd-encrypt-stream.m3u8?MtsHlsUriToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJjb2RlIjoiYjllMTE0ZGMyMjAwNWQ2N2VmYjg2OGZhOWUzNThlMmQ3NWIxZmJjNCIsImV4cCI6MTY0MTQ4MDMzMzY1OSwiZXh0cmEiOnsidmlkIjoiZDJmOGY0ZTg3YmYxNDNhMjg4NTU4YTgyZWI4YzBiMTIiLCJhaWQiOjQzMDE3MywidWlkIjoyODgyMzgyLCJzIjoiIn0sInMiOjIsInQiOjEsInYiOjF9.Acv9-Gh1u4oBfqu9ngUP8HRIrIoqebzjMRPNxhNWa2g","size":346594356}},"video_media_map":{"sd":{"size":195729056},"ld":{"size":145663152},"hd":{"size":369931924}},"article_features":0,"article_ctime":1635151380,"video_preview_second":180},"430184":{"text_read_version":0,"audio_size":1373628,"article_cover":"https://static001.geekbang.org/resource/image/25/7b/258d00f02088296f8562529daf43bd7b.jpg","video_time_arr":{"m":"56","s":"03","h":"01"},"product_type":"c1","audio_dubber":"海纳","is_finished":false,"like":{"had_done":false,"count":6},"audio_time":"00:01:25","video_height":1080,"article_content":"<p>你好，我是海纳。</p><p>这是我们导学的第二节，今天我们来学习汇编语言的基本原理，这节课是以视频形式讲述的，内容比较多，但我保证干货十足，非常值得你花时间好好观看学习。</p><p>你可能好奇，为什么我要花这么长时间给你讲汇编语言呢？</p><p>这是因为，汇编器将汇编语言翻译成 CPU 可以执行的机器码，汇编语言中的指令与机器码几乎是一一对应的，所以，我们学习汇编语言，其实就是在学习各种汇编指令。汇编指令大致可以分为逻辑运算和位运算指令、分支指令、字符串处理指令等三大类，这节课我都会做详细介绍。</p><p>另外，在编程语言发展过程中，人们按照编程语言中所暴露的硬件细节的多少，还会将编程语言划分为高级语言和低级语言。汇编，由于其本身包含了大量的 CPU 硬件细节，可读性非常差，被认为是低级语言，而C/C++ 等更具可读性的编程语言，则被认为是高级语言。</p><p>不过，所有的高级语言最终还是要翻译成机器码运行在 CPU 上的，查看高级语言所翻译成的机器码是程序员进行代码调试时的重要手段。所以，这节课，我还会通过实战案例向你讲解如何进行机器码级的调试。</p><p>好，现在就请你点开下面的视频，来学习今天的课程吧！</p><p><video poster=\"https://media001.geekbang.org/d6b8a91a9b8f49e986a8ee1b8174b782/snapshots/1af0874d9288477f8db972301c08e23f-00005.jpg\" preload=\"none\" controls=\"\"><source src=\"https://media001.geekbang.org/customerTrans/7e27d07d27d407ebcc195a0e78395f55/143c5ec6-17cdfb13b52-0000-0000-01d-dbacd.mp4\" type=\"video/mp4\"><source src=\" https://media001.geekbang.org/c5625184226643d4bace5c69c5644fdc/d9c118255ef34e05bad4817b74bac212-d0e207b6f87584234fb715ffe3d1daf7-sd.m3u8\" type=\"application/x-mpegURL\"></video></p><p>点击<a href=\"https://pan.baidu.com/s/1tgpvlvpoETa90-ENxNymUw\">这里</a>获取课件，提取码：olth。</p><!-- [[[read_end]]] -->","float_qrcode":"https://static001.geekbang.org/resource/image/83/1c/83e244537b714e899fd2e7896e22371c.png","article_cover_hidden":false,"is_required":false,"score":"21635152820","like_count":10,"article_subtitle":"","audio_download_url":"https://static001.geekbang.org/resource/audio/14/e9/14e8688c894215a9bd80105abc3ce1e9.mp3","had_viewed":false,"article_title":"导学（二）| 汇编语言是怎么一回事？","column_bgcolor":"#F6F7FB","offline_package":"{}","audio_title":"11.29 海纳导学2.MP3","audio_time_arr":{"m":"01","s":"25","h":"00"},"text_read_percent":0,"cid":450,"article_cshort":"<p>你好，我是海纳。</p><p>这是我们导学的第二节，今天我们来学习汇编语言的基本原理，这节课是以视频形式讲述的，内容比较多，但我保证干货十足，非常值得你花时间好好观看学习。</p><p>你可能好奇，为什么我要花这么长时间给你讲汇编语言呢？</p><p>这是因为，汇编器将汇编语言翻译成 CPU 可以执行的机器码，汇编语言中的指令与机器码几乎是一一对应的，所以，我们学习汇编语言，其实就是在学习各种汇编指令。汇编指令大致可以分为逻辑运算和位运算指令、分支指令、字符串处理指令等三大类，这节课我都会做详细介绍。</p><p>另外，在编程语言发展过程中，人们按照编程语言中所暴露的硬件细节的多少，还会将编程语言划分为高级语言和低级语言。汇编，由于其本身包含了大量的 CPU 硬件细节，可读性非常差，被认为是低级语言，而C/C++ 等更具可读性的编程语言，则被认为是高级语言。</p><p>不过，所有的高级语言最终还是要翻译成机器码运行在 CPU 上的，查看高级语言所翻译成的机器码是程序员进行代码调试时的重要手段。所以，这节课，我还会通过实战案例向你讲解如何进行机器码级的调试。</p><p>好，现在就请你点开下面的视频，来学习今天的课程吧！</p><p><video poster=\"https://media001.geekbang.org/d6b8a91a9b8f49e986a8ee1b8174b782/snapshots/1af0874d9288477f8db972301c08e23f-00005.jpg\" preload=\"none\" controls=\"\"><source src=\"https://media001.geekbang.org/customerTrans/7e27d07d27d407ebcc195a0e78395f55/143c5ec6-17cdfb13b52-0000-0000-01d-dbacd.mp4\" type=\"video/mp4\"><source src=\" https://media001.geekbang.org/c5625184226643d4bace5c69c5644fdc/d9c118255ef34e05bad4817b74bac212-d0e207b6f87584234fb715ffe3d1daf7-sd.m3u8\" type=\"application/x-mpegURL\"></video></p><p>点击<a href=\"https://pan.baidu.com/s/1tgpvlvpoETa90-ENxNymUw\">这里</a>获取课件，提取码：olth。</p>","video_width":1920,"column_could_sub":true,"video_id":"d6b8a91a9b8f49e986a8ee1b8174b782","sku":"100094901","video_cover":"https://media001.geekbang.org/d6b8a91a9b8f49e986a8ee1b8174b782/snapshots/1af0874d9288477f8db972301c08e23f-00005.jpg","author_name":"海纳","column_is_onboard":true,"audio_url":"https://res001.geekbang.org/media/audio/14/e9/14e8688c894215a9bd80105abc3ce1e9/ld/ld.m3u8","chapter_id":"2313","column_had_sub":true,"column_cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg","neighbors":{"left":{"article_title":"导学（一）| 拆解CPU的基本结构和运行原理","id":430173},"right":{"article_title":"导学（三）| 一个CPU是怎么寻址的？","id":431373}},"rate_percent":0,"footer_cover_data":{"img_url":"https://static001.geekbang.org/resource/image/f0/fe/f0c47daf3262a8df6cf352d89896abfe.png","link_url":"https://time.geekbang.org/article/427012","mp_url":""},"float_app_qrcode":"","column_is_experience":false,"rate":{"1":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"2":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"3":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0}},"product_id":100094901,"had_liked":false,"id":430184,"free_get":false,"is_video_preview":true,"article_summary":"在编程语言发展过程中，人们按照编程语言中所暴露的硬件细节的多少，将编程语言划分为高级语言和低级语言。","column_sale_type":0,"float_qrcode_jump":"https://time.geekbang.org/serv/v4/misc/jump?uri=https%3A%2F%2Ftime.geekbang.org%2Fhybrid%2Fmp%2Fjump%3Furl%3Dhttps%253A%252F%252Fstatic001.geekbang.org%252Fresource%252Fimage%252Fee%252Fb2%252Feedd4090668aabfac91b2b445d27cbb2.png","subtitles":[],"column_id":450,"audio_md5":"14e8688c894215a9bd80105abc3ce1e9","article_sharetitle":"汇编语言是怎么一回事？","share":{"content":"学好内存，掌握系统开发诀窍","title":"海纳 · 编程高手必学的内存知识","poster":"https://static001.geekbang.org/resource/image/bc/c5/bcaf3ff69e538f8749970edf2301bbc5.jpg","cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg"},"video_time":"01:56:03","video_preview":{"sd":{"url":"https://res001.geekbang.org/files/vod/d6b8a91a9b8f49e986a8ee1b8174b782/095cac9a4c1a7294d8e555d49e9af9c2-sd.m3u8","size":8010680}},"article_could_preview":true,"comment_count":4,"article_poster_wxlite":"https://static001.geekbang.org/render/screen/27/85/2758e5da9983d9c1670b546e31fa5a85.jpeg","offline":{"size":2086721,"file_name":"e7338b1917b016bf076dafba68bfb2b1","download_url":"https://static-acl-001.geekbang.org/resource/zip/article/430184/e7338b1917b016bf076dafba68bfb2b1.zip?auth_key=1641482029-f05f5c64a86241469dd9195818873c6d-0-aa0d5c0454c4fdc6493e7ebc1739b528"},"video_size":726239141,"hls_videos":{"sd":{"url":"https://media001.geekbang.org/65afbccfc27c4bcaa0308bfd507074bf/c6a5f5ae9efe25b354466a4c9d5271cd-ld-encrypt-stream.m3u8?MtsHlsUriToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJjb2RlIjoiMGZhZTdlNGZlZGUyY2Q4NjcxNDk0MjEzMThjNDc4YTRmYmE4ZTg5YiIsImV4cCI6MTY0MTQ4MDM0OTY1NCwiZXh0cmEiOnsidmlkIjoiNjVhZmJjY2ZjMjdjNGJjYWEwMzA4YmZkNTA3MDc0YmYiLCJhaWQiOjQzMDE4NCwidWlkIjoyODgyMzgyLCJzIjoiIn0sInMiOjIsInQiOjEsInYiOjF9.c6x-oG6yczy8HydUii9UsW_0e21WRB9kD0-gWqWAZV4","size":1062961964},"hd":{"url":"https://media001.geekbang.org/65afbccfc27c4bcaa0308bfd507074bf/25f25db284d973d2d24146bd984ad6b4-sd-encrypt-stream.m3u8?MtsHlsUriToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJjb2RlIjoiMGZhZTdlNGZlZGUyY2Q4NjcxNDk0MjEzMThjNDc4YTRmYmE4ZTg5YiIsImV4cCI6MTY0MTQ4MDM0OTY1NCwiZXh0cmEiOnsidmlkIjoiNjVhZmJjY2ZjMjdjNGJjYWEwMzA4YmZkNTA3MDc0YmYiLCJhaWQiOjQzMDE4NCwidWlkIjoyODgyMzgyLCJzIjoiIn0sInMiOjIsInQiOjEsInYiOjF9.c6x-oG6yczy8HydUii9UsW_0e21WRB9kD0-gWqWAZV4","size":2018167780},"ld":{"url":"https://media001.geekbang.org/65afbccfc27c4bcaa0308bfd507074bf/fe50eda8aededa0684c7cbd3202348d9-fd-encrypt-stream.m3u8?MtsHlsUriToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJjb2RlIjoiMGZhZTdlNGZlZGUyY2Q4NjcxNDk0MjEzMThjNDc4YTRmYmE4ZTg5YiIsImV4cCI6MTY0MTQ4MDM0OTY1NCwiZXh0cmEiOnsidmlkIjoiNjVhZmJjY2ZjMjdjNGJjYWEwMzA4YmZkNTA3MDc0YmYiLCJhaWQiOjQzMDE4NCwidWlkIjoyODgyMzgyLCJzIjoiIn0sInMiOjIsInQiOjEsInYiOjF9.c6x-oG6yczy8HydUii9UsW_0e21WRB9kD0-gWqWAZV4","size":574705284}},"video_media_map":{"sd":{"size":293124336},"ld":{"size":224614692},"hd":{"size":513072868}},"article_features":0,"article_ctime":1635152820,"video_preview_second":180},"431373":{"text_read_version":0,"audio_size":1111335,"article_cover":"https://static001.geekbang.org/resource/image/42/fe/424628fbcd422c192edfd0862d24d7fe.jpg","video_time_arr":{"m":"36","s":"30","h":"01"},"product_type":"c1","audio_dubber":"海纳","is_finished":false,"like":{"had_done":false,"count":6},"audio_time":"00:01:09","video_height":1080,"article_content":"<p>你好，我是海纳。</p><p>这是我们导学的第三节课，我们一起来探寻一个CPU是怎么寻址的。这节课是以视频形式讲述的，内容比较多，但都是纯干货，非常值得你花时间好好观看学习。</p><p>我们知道，CPU的设计思路大致分为两个流派，一个是复杂指令集（Complex Instruction Set Computing， CISC）；另一个是精简指令集（Reduced Instruction Set Computing，RISC），前者的代表是 X86，后者的代表是 Arm 和 RISC-V。</p><p>其中，RISC 的特点是指令长度短，运行速度快，但每条指令能做的事情比较少；CISC 的特点是指令长度长，运行速度慢，但指令能做的事情多。相同的C代码要是翻译成RISC指令，往往会比 CISC 最终翻译出的二进制文件的体积更大。这节课，我会帮你理清CISC和RISC指令的区别。</p><p>此外，我还会给你深入讲解寻址模式。寻址就是在程序中如何定位地址，我们可以把它类比成现实生活中在地图上定位某个地址的过程。这也是你在学习内存管理时的必备知识。</p><p>好，现在就请你点开下面的视频，来学习今天的课程吧！</p><p><video poster=\"https://media001.geekbang.org/079a67d18f1742199b84e27a39177c88/snapshots/b45938a689d94646850c06e98f3050c6-00005.jpg\" preload=\"none\" controls=\"\"><source src=\"https://media001.geekbang.org/customerTrans/7e27d07d27d407ebcc195a0e78395f55/5fea62a1-17cfe56881b-0000-0000-01d-dbacd.mp4\" type=\"video/mp4\"><source src=\" https://media001.geekbang.org/bc200aa3ea214c3fac3487f9ec7d7550/209ba6d41f1b4262b055cab2e4f4732a-e2d7ed0f82ea3c4aa7b0025cf46bd6c6-sd.m3u8\" type=\"application/x-mpegURL\"></video></p><p>点击<a href=\"https://pan.baidu.com/s/1NW2JKYz-ecCm2wRLz3kntQ\">这里</a>获取课件，提取码：4ffc。</p><!-- [[[read_end]]] -->","float_qrcode":"https://static001.geekbang.org/resource/image/83/1c/83e244537b714e899fd2e7896e22371c.png","article_cover_hidden":false,"is_required":false,"score":"21635247800","like_count":8,"article_subtitle":"","video_time":"01:36:30","had_viewed":false,"article_title":"导学（三）| 一个CPU是怎么寻址的？","column_bgcolor":"#F6F7FB","offline_package":"{}","audio_title":"11.29 海纳导学3.MP3","audio_time_arr":{"m":"01","s":"09","h":"00"},"text_read_percent":0,"cid":450,"article_cshort":"<p>你好，我是海纳。</p><p>这是我们导学的第三节课，我们一起来探寻一个CPU是怎么寻址的。这节课是以视频形式讲述的，内容比较多，但都是纯干货，非常值得你花时间好好观看学习。</p><p>我们知道，CPU的设计思路大致分为两个流派，一个是复杂指令集（Complex Instruction Set Computing， CISC）；另一个是精简指令集（Reduced Instruction Set Computing，RISC），前者的代表是 X86，后者的代表是 Arm 和 RISC-V。</p><p>其中，RISC 的特点是指令长度短，运行速度快，但每条指令能做的事情比较少；CISC 的特点是指令长度长，运行速度慢，但指令能做的事情多。相同的C代码要是翻译成RISC指令，往往会比 CISC 最终翻译出的二进制文件的体积更大。这节课，我会帮你理清CISC和RISC指令的区别。</p><p>此外，我还会给你深入讲解寻址模式。寻址就是在程序中如何定位地址，我们可以把它类比成现实生活中在地图上定位某个地址的过程。这也是你在学习内存管理时的必备知识。</p><p>好，现在就请你点开下面的视频，来学习今天的课程吧！</p><p><video poster=\"https://media001.geekbang.org/079a67d18f1742199b84e27a39177c88/snapshots/b45938a689d94646850c06e98f3050c6-00005.jpg\" preload=\"none\" controls=\"\"><source src=\"https://media001.geekbang.org/customerTrans/7e27d07d27d407ebcc195a0e78395f55/5fea62a1-17cfe56881b-0000-0000-01d-dbacd.mp4\" type=\"video/mp4\"><source src=\" https://media001.geekbang.org/bc200aa3ea214c3fac3487f9ec7d7550/209ba6d41f1b4262b055cab2e4f4732a-e2d7ed0f82ea3c4aa7b0025cf46bd6c6-sd.m3u8\" type=\"application/x-mpegURL\"></video></p><p>点击<a href=\"https://pan.baidu.com/s/1NW2JKYz-ecCm2wRLz3kntQ\">这里</a>获取课件，提取码：4ffc。</p>","video_width":1920,"column_could_sub":true,"video_id":"079a67d18f1742199b84e27a39177c88","sku":"100094901","video_cover":"https://media001.geekbang.org/079a67d18f1742199b84e27a39177c88/snapshots/b45938a689d94646850c06e98f3050c6-00005.jpg","author_name":"海纳","column_is_onboard":true,"audio_url":"https://res001.geekbang.org/media/audio/69/13/69527441be802e402a4b71b942330c13/ld/ld.m3u8","chapter_id":"2313","column_had_sub":true,"column_cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg","neighbors":{"left":{"article_title":"导学（二）| 汇编语言是怎么一回事？","id":430184},"right":{"article_title":"01｜虚拟内存：为什么可用内存会远超物理内存？","id":430073}},"rate_percent":0,"footer_cover_data":{"img_url":"https://static001.geekbang.org/resource/image/f0/fe/f0c47daf3262a8df6cf352d89896abfe.png","link_url":"https://time.geekbang.org/article/427012","mp_url":""},"float_app_qrcode":"","column_is_experience":false,"rate":{"1":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"2":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"3":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0}},"product_id":100094901,"had_liked":false,"id":431373,"free_get":false,"is_video_preview":true,"article_summary":"CPU的设计思路大致分为两个流派，一个是复杂指令集，另一个是精简指令集，前者的代表是X86，后者的代表是Arm和RISC-V。","column_sale_type":0,"float_qrcode_jump":"https://time.geekbang.org/serv/v4/misc/jump?uri=https%3A%2F%2Ftime.geekbang.org%2Fhybrid%2Fmp%2Fjump%3Furl%3Dhttps%253A%252F%252Fstatic001.geekbang.org%252Fresource%252Fimage%252Fee%252Fb2%252Feedd4090668aabfac91b2b445d27cbb2.png","subtitles":[],"column_id":450,"audio_md5":"69527441be802e402a4b71b942330c13","video_media_map":{"sd":{"size":244997464},"ld":{"size":186408204},"hd":{"size":442529252}},"share":{"content":"学好内存，掌握系统开发诀窍","title":"海纳 · 编程高手必学的内存知识","poster":"https://static001.geekbang.org/resource/image/bc/c5/bcaf3ff69e538f8749970edf2301bbc5.jpg","cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg"},"article_could_preview":true,"article_poster_wxlite":"https://static001.geekbang.org/render/screen/62/52/6217d96d8e9d659c0875d53dc7de9e52.jpeg","video_preview":{"sd":{"url":"https://res001.geekbang.org/files/vod/079a67d18f1742199b84e27a39177c88/32a352ae483fb59156958a650e27de4b-sd.m3u8","size":8166532}},"comment_count":1,"video_size":355587753,"offline":{"size":2260421,"file_name":"427fc8abee14d198eeb321cb4420c9b9","download_url":"https://static-acl-001.geekbang.org/resource/zip/article/431373/427fc8abee14d198eeb321cb4420c9b9.zip?auth_key=1641482045-bb3a9f94f48e4292a1cb55f6e4d4327f-0-3bd3116145e34a858a711daaf02abd73"},"video_preview_second":180,"hls_videos":{"sd":{"url":"https://media001.geekbang.org/82c768aed61d48b3a4a49561a17217d8/ef554c2a75c885e12cbe07308c5caf51-ld-encrypt-stream.m3u8?MtsHlsUriToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJjb2RlIjoiZTY1ZjcwZTNjZDk0NDQyMjYyYWYzNTI5NGYxNzNlYjE3MjljZTAyMSIsImV4cCI6MTY0MTQ4MDM2NTY4NiwiZXh0cmEiOnsidmlkIjoiODJjNzY4YWVkNjFkNDhiM2E0YTQ5NTYxYTE3MjE3ZDgiLCJhaWQiOjQzMTM3MywidWlkIjoyODgyMzgyLCJzIjoiIn0sInMiOjIsInQiOjEsInYiOjF9.x0kOD4eTrScfexKieQtGNzOXCRTh-1tcMH-NEmKO4uI","size":893882096},"hd":{"url":"https://media001.geekbang.org/82c768aed61d48b3a4a49561a17217d8/b71355cbcebba7e9ecc0bb693b526915-sd-encrypt-stream.m3u8?MtsHlsUriToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJjb2RlIjoiZTY1ZjcwZTNjZDk0NDQyMjYyYWYzNTI5NGYxNzNlYjE3MjljZTAyMSIsImV4cCI6MTY0MTQ4MDM2NTY4NiwiZXh0cmEiOnsidmlkIjoiODJjNzY4YWVkNjFkNDhiM2E0YTQ5NTYxYTE3MjE3ZDgiLCJhaWQiOjQzMTM3MywidWlkIjoyODgyMzgyLCJzIjoiIn0sInMiOjIsInQiOjEsInYiOjF9.x0kOD4eTrScfexKieQtGNzOXCRTh-1tcMH-NEmKO4uI","size":1681487792},"ld":{"url":"https://media001.geekbang.org/82c768aed61d48b3a4a49561a17217d8/b4bb090fe53ab6ccf6ce0fae81eedb46-fd-encrypt-stream.m3u8?MtsHlsUriToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJjb2RlIjoiZTY1ZjcwZTNjZDk0NDQyMjYyYWYzNTI5NGYxNzNlYjE3MjljZTAyMSIsImV4cCI6MTY0MTQ4MDM2NTY4NiwiZXh0cmEiOnsidmlkIjoiODJjNzY4YWVkNjFkNDhiM2E0YTQ5NTYxYTE3MjE3ZDgiLCJhaWQiOjQzMTM3MywidWlkIjoyODgyMzgyLCJzIjoiIn0sInMiOjIsInQiOjEsInYiOjF9.x0kOD4eTrScfexKieQtGNzOXCRTh-1tcMH-NEmKO4uI","size":481788164}},"audio_download_url":"https://static001.geekbang.org/resource/audio/69/13/69527441be802e402a4b71b942330c13.mp3","article_features":0,"article_ctime":1635247800,"article_sharetitle":"一个CPU是怎么寻址的？"},"431400":{"text_read_version":0,"audio_size":19182447,"article_cover":"https://static001.geekbang.org/resource/image/6c/7b/6c1ab9ea1b3c0f5077e8fcff5f5bc07b.jpg","subtitles":[],"product_type":"c1","audio_dubber":"海纳","is_finished":false,"like":{"had_done":false,"count":9},"audio_time":"00:19:58","video_height":0,"article_content":"<p>你好，我是海纳。</p><p>上一节课我们讲了虚拟内存的概念，分析了线性地址（虚拟地址）是如何映射到物理地址上的。</p><p>不过，在x86架构诞生之初，其实是没有虚拟内存的概念的。1978年发行的8086芯片是x86架构的首款芯片，它在内存管理上使用的是直接访问物理内存的方式，这种工作方式，有一个专门的名称，那就是<strong>实模式</strong>（Real Mode）。上节课我们也曾简单提到过，直接访问物理内存的工作方式让程序员必须要关心自己使用的内存会不会与其他进程产生冲突，为程序员带来极大的心智负担。</p><p>后来，CPU上就出现虚拟内存的概念，它可以将每个进程的地址空间都隔离开，极大地减轻了程序员的负担，同时由于页表项中有多种权限保护标志，极大地提高了应用程序的数据安全。所以人们把CPU的这种工作模式称为<strong>保护模式</strong>（Protection Mode）。</p><p>从实模式演进到保护模式，x86体系架构的内存管理发生了重大的变化，最大的不同就体现在段式管理和中断的管理上。所以今天这节课，我们会围绕这两个重点，让你彻底理解x86体系架构下的内存管理演进。你也能通过这节课的学习，学会阅读Linux内核源码的段管理和中断管理的相关部分，还可以增加调试coredump文件的能力。</p><!-- [[[read_end]]] --><p>这里我们就按照时间顺序，从8086芯片中的实模式开始讲起。</p><h3>8086中的实模式</h3><p>8086芯片是Intel公司在1978年推出的CPU芯片，它定义的指令集对计算机的发展历程影响十分巨大，之后的286、386、486、奔腾处理器等等都是在8086的基础上演变而来。这一套指令集也被称为x86指令集。直到今天，很多大学里的微机原理课和汇编语言课还是使用8086进行讲解。</p><p>8086的寄存器只有16位，我们也习惯于称8086的工作模式是16位模式。而且，后面的CPU为了保持兼容，在芯片上电了以后，还必须运行在16位模式之下，这种模式有个正式的名字，叫做<strong>实模式</strong>（Real Mode）。<strong>在实模式下，程序员是不能通过内存管理单元（Memory Management Unit, MMU）访问地址的，程序必须直接访问物理内存。</strong></p><p>那实模式下，我们是怎么访问存储的物理地址的呢？</p><p>8086的寄存器位宽是16位，但地址总线却有20位，地址的编码可以从20位0到20位1，这意味着8086的寻址空间是2^20 = 1M。但是在写程序的时候，我们没有办法把一个地址完整地放到一个寄存器里，因为它的寄存器相比地址少了4位。</p><p>为了解决这个问题，8086就引入了<strong>段寄存器</strong>，例如cs、ds、es、gs、ss等。段寄存器中记录了一个段基地址，通过计算可以得到我们存储的真实地址，也就是物理地址。物理地址可以使用“段寄存器:段内偏移”这样的格式来表示，计算的公式是：</p><p><strong>物理地址 =段寄存器 &lt;&lt; 4 + 段内偏移</strong></p><p>不过，在我们写汇编代码的时候（如果你对汇编不熟悉，可以先去看看我前面讲的<a href=\"https://time.geekbang.org/column/article/430173\">导学（一）</a>和<a href=\"https://time.geekbang.org/column/article/430184\">导学（二）</a>），也不一定就要使用段寄存器来表示段基址，也可以使用“段基址:段内偏移”这样的立即数的写法，比如你可以看下这个节选自Linux的bootsect中的代码：</p><pre><code>BOOTSEG   = 0x7c0\n\n_start:\n  jmpl $BOOTSEG, $start2\n\nstart2:\n  movw $BOOTSEG, %ax\n  movw %ax, %ds\n  ...\n</code></pre><p>这块代码里，它跳转的目标地址就是0x7c0 &lt;&lt; 4 + OFFSET(start2)。跳转成功以后，cs段寄存器中的值就是段基址0x7c0，start2的偏移值是8，所以记录当前执行指令地址的ip寄存器中的值就是实际地址0x7c08。</p><p>而且，这块代码里也包含了段基址和段内偏移值这种地址形式，这显然有别于我们所讲的虚拟地址。这种包含了段基址和段内偏移值的地址形式有一个专门的名字，叫做<strong>逻辑地址</strong>。你可以看到，虚拟地址是一个整数，而逻辑地址是一对整数。所以说，在8086芯片中，逻辑地址要经过一步计算才可以得到物理地址。</p><p>在8086中，cs被用来做为代码段基址寄存器，比如上面示例代码中的jmp指令，跳转成功就会把段基址自动存入cs寄存器。ds被用来做为数据段基址寄存器，你可以看看下面这个代码：</p><pre><code>INITSEG = 0x9000\n    ....\n    movw $INITSEG, %ax\n    movw %ax, %ds\n    movb $0x03, %ah\n    xor  %bh,  %bh\n    int  $0x10\n    movw %dx, (0)\n    movb $0x88, %ah\n    int  $0x15\n    movw %ax, (2)\n</code></pre><p>上述代码的第7行执行0x10号BIOS中断，它的结果存放在dx寄存器中，然后第8行，将结果存入内存0x90000，9至11行再把0x15号BIOS中断的结果存到0x90002处。</p><p>在寻址时，我们并没有明确地声明数据段基址存储在段寄存器ds中，但是CPU在执行时会默认使用ds做为数据段寄存器。类似的还有ss，它是做为栈基址寄存器，当我们在使用push指令的时候，要保存的数据会放在ss:(sp)的位置。</p><p>CPU没有强制规定代码段和数据段分离，也就意味着，你使用ds段寄存器去访问指令，CPU也是允许的。但在实际编程时，我们还是会把数据和代码分到不同的段里，并且将数据段的起始地址放到ds寄存器，把代码段的地址放到cs寄存器。这种按功能分段的管理内存方式就是段式管理。关于段式管理和页式管理的对比，我们稍后会加以介绍。</p><p>到这里8086的实模式，我们已经基本讲完了。8086是最古老的x86芯片，在实模式下，它只能直接操作物理内存，非常不便于编程，这一点，我们在<a href=\"https://time.geekbang.org/column/article/430073\">第1节课</a>也提到了。接下来，我们把目光转向x86体系架构中的保护模式，它是实模式的进一步发展。</p><h3>i386中的保护模式</h3><p>经过十年的发展，x86 CPU迎来了历史上使用最广泛、影响力最大的32位CPU，这就是i386芯片。i386与8086的一个很大的不同，就是它采用了全新的保护模式。这个体现在，i386中的段式管理机制，相比8086发生了重大变化；同时，i386芯片在段式管理的基础上，还引入了页式管理。</p><p>i386在完成各种初始化动作以后，就会开启页表，从此程序员就不必再直接操作物理内存的地址空间了，代替它的是线性地址空间。而且由于段和页都能提供对内存的保护，安全性也得到了提升，所以这种工作模式被称为保护模式（Protection Mode）。i386的保护模式是一种段式管理和页式管理混合使用的模式。</p><p>至于页式管理，我们<a href=\"https://time.geekbang.org/column/article/430073\">第1节课</a>已经讲过了，所以这里我们就来看一下相比8086，段式管理在i386上有了哪些变化。</p><p><strong>变化一：段选择子和全局描述符表</strong></p><p>在i386上，地址总线是32位的，通用寄存器也变成32位的，这就意味着因为寄存器位数不够而产生的段基址寄存器已经失去了作用。</p><p>但是i386没有直接放弃掉段寄存器，而是将它进化成了新的段式内存管理。段寄存器仍然是16位寄存器，但是其中存的不再是段基址，而是被称为段选择子的东西。</p><p>相比8086芯片，i386中多了一个叫全局描述符表（Global Descriptor Table, GDT）的结构。它本质上是一个数组，其中的每一项都是一个全局描述符，32位的段基址就存储在这个描述符里。段选择子本质上就是这个数组的下标。具体你可以看看下面这张图：</p><p><img src=\"https://static001.geekbang.org/resource/image/e4/46/e4abd9ca17763b5094990fb9ef108846.jpg?wh=3480x1918\" alt=\"\"></p><p>GDT的地址也要保存在寄存器里，这个寄存器就是GDTR，这个做法和<a href=\"https://time.geekbang.org/column/article/430073\">第1节课</a>我们讲到的CR3寄存器的做法十分相似。</p><p>在上面这张图中，CPU在处理一个逻辑地址“cs:offset”的时候，就会将GDTR中的基址加上cs中的下标值来得到一个段描述符，再从这个段描述符中取出段基址，最后将段基址与偏移值相加，这样就可以得到线性地址了。这个线性地址就是我们<a href=\"https://time.geekbang.org/column/article/430073\">第1节课</a>中所讲的虚拟地址。</p><p>得到线性地址以后，剩下的工作我们就非常熟悉了：<strong>由CPU的MMU将线性地址映射为物理地址，然后就可以交给地址总线去进行读写了。</strong></p><p><strong>变化二：段寄存器对段的保护能力增强</strong></p><p>在8086中，段寄存器只起到了段基址的作用，对于段的各种属性并没有加以定义。例如，在实模式下，任何指令都可以对代码段进行随意地更改。</p><p>但在i386中，对段的保护能力加强了，我们先来看一下i386中段描述符（也就是GDT中的每一项）的结构。</p><p><img src=\"https://static001.geekbang.org/resource/image/16/38/16b2787068bb721ae5366ee286414138.jpg?wh=1932x845\" alt=\"\"></p><p>你会看到，描述符中除了记录了段基址之外，还记录了段的长度，以及定义了一些与段相关的属性，其中比较重要的属性有P位、DPL、S位、G位和Type。我们接下来一个个来分析。</p><p>P位是一个比特，指示了段在内存中是否存在，1表示段在内存中存在，0则表示不存在。</p><p>DPL，占据了两个比特，指的是描述符特权级，英文是Descriptor Privilege Level。Intel规定了CPU工作的4个特权级，分别是0、1、2、3，数字越小，权限越高。</p><p>以Linux为例，Linux只使用了0和3两个特权级，并且规定0是内核态，3是用户态。特权级的切换是比较复杂的一种机制，但Linux只使用了中断这一种，后面我们会再讲到中断。</p><p>接下来我们再看S位，S为1代表该描述符是数据段/代码段描述符，为0则代表系统段/门描述符。门是i386提供的用于切换特权级的机制，有调用门、陷阱门、中断门、任务门等。在Linux系统中，只使用了中断门描述符。</p><p>然后是G位，它指的是定义段颗粒度（Granularity），它的值为0时，段界限的单位是字节，为1时段界限以4KB为单位，也就是一页。</p><p>我们也可以从图中看出定义段长度的“段界限”字段并不是连续的，它一共有20位，分散在两个地方。当G=1时，段界限的最大值是2^20 * 4K = 4G，这是i386一个段的最大长度。</p><p>最后是Type属性，它定义了描述符类型，我把比较重要的类型用表列在了下面，你可以看看。</p><p><img src=\"https://static001.geekbang.org/resource/image/b8/f5/b80ddc894798e31d077394df613d5bf5.jpg?wh=1980x804\" alt=\"\"></p><p>到这里，我们已经解释清楚了，i386中保护模式相比8086实模式在段式管理上的升级。那么在现代的CPU和操作系统中，段式管理和页式管理又是怎样的关系呢？要讲清楚这一点就要先对比这两种内存管理方式的优缺点。</p><h3>段式管理对比页式管理</h3><p>段式管理会按功能把内存空间分割成不同段，有代码段、数据段、只读数据段、堆栈段，等等，为不同的段赋予了不同的读写权限和特权级。通过段式管理，操作系统可以进一步区分内核数据段、内核代码段、用户态数据段、用户态代码段等，为系统提供了更好的安全性。</p><p>但是段的长度往往是不能固定的，例如不同的应用程序中，代码段的长度各不相同。如果以段为单位进行内存的分配和回收的话，数据结构非常难于设计，而且难免会造成各种内存空间的浪费。页式管理则不按照功能区分，而是按照固定大小将内存分割成很多大小相同的页面，不管是存放数据，还是存放代码，都要先分配一个页，再将内容存进页里。</p><p>所以，你可以看到，相比页式管理，段式管理的优点是提供更好的安全性，按照内存的用途进行划分更符合人的直观思维。它的缺点就是由于不定长，难于进行分配、回收调度。</p><p>而页式管理的优点是大小固定，分配回收都比较容易。而且段式管理所能提供的安全性，在现代CPU上也可以被页表项中的属性替代，所以现在段式管理已经变得越来越不重要了。像64位Linux系统，它把所有段的基地址都设成了从0开始，段长度设置为最大。这样段式管理的重要性就大大下降了。</p><p>但是，如果我们以x86的历史演进来看，你会发现段式管理其实是最早出现的（8086芯片），然后才出现了页式管理（i386芯片）。而且，我们现代的x86架构的CPU，也同时兼容段式管理和页式管理，我们可以认为是一种混合的段页式管理（当然，并不是所有人都认可这种命名方式）。</p><p>总的来说，现代的操作系统都是采用段式管理来做基本的权限管理，而对于内存的分配、回收、调度都是依赖页式管理。</p><p>到这里，我们就讲清楚了8086实模式到i386保护模式下段式管理的演进，并且进一步分析了段式管理和页式管理的对比和现状。</p><p>保护模式相比实模式，发生重大变化的不止是内存管理，同时还有中断管理。因为管理中断的结构与段式管理的全局描述符表的结构非常相似，所以我们在讲保护模式时也一起讲一下。你可以将中断机制与段管理机制比较着一起学习。</p><h3>中断描述符表</h3><p>中断描述符表（Interruption Description Table, IDT），是i386中一个非常重要的描述符表，它也是保护模式对比实模式的另一大不同。你在后面学习fork、execve的实现时，涉及到的写保护中断，缺页中断等机制都要依赖它。</p><p>CPU与外设之间的协同工作是以中断机制来进行的。例如，我们敲击键盘的时候，键盘的控制器就会向CPU发起一个中断请求。CPU在接到请求以后，就会停下正在做的工作，把当前的寄存器状态全部保存好，然后去调用中断服务程序。当然，这个过程中有一些是CPU的工作，有一些是操作系统的工作，但因为我们关注的重点是内存，所以就没必要计较这里面细微的差别了。</p><p>中断根据中断来源的不同，又可以细分为Fault、Trap、Abort以及普通中断。我们这门课对它们也不加区分，例如执行除法的时候除数为0的情况、访问数据时权限不足引发的保护错误、由用户使用int指令产生的中断等，虽然中断源不同，它们的类型也不相同，但我们统一称它们为中断。</p><p>硬件负责产生中断，CPU会响应中断，但是中断来了以后要做什么事情是由操作系统定义的。操作系统要通过设置某个中断号的中断描述符，来指定中断到达以后要调用的函数。<strong>中断描述符表（IDT）的作用就体现在这了，它的本质就是中断描述符的数组</strong>。</p><p>IDT的基地址存储在idtr寄存器中，这和GDTR的设计如出一辙。每个中断都有一个编号与其对应，我们称之为<strong>中断向量号</strong>。中断向量号是CPU提前分配好的，我也把比较重要的中断向量号放在了下表里，你可以看看。</p><p><img src=\"https://static001.geekbang.org/resource/image/c3/3d/c361fe908d647f4227d4b4e2f447993d.jpg?wh=1980x1080\" alt=\"\"></p><p>在这个表里，我们没有看到前边所提到的键盘中断，这是因为键盘中断都是由一个名为8259A的芯片在管理。</p><p>两片级联的8259A芯片可以管理16个中断，其中包括了时钟中断、键盘中断，还有软盘、硬盘、鼠标的中断等等。这些中断的中断向量号是可以通过对8259A编程进行设置的。虽然8259A的编程比较繁琐，但好在只需要操作系统开机引导时设置一次。</p><p>你也可以看到，Linux系统把中断向量表的32号中断（用户自定义中断的第一位）设置成8259A的0号中断，也就是说IDT的32号至47号都分配给了8259A所管理的中断。键盘、软盘、硬盘、鼠标的中断服务程序就设置在这里。</p><p>关于中断，我们掌握这么多就已经足够了，更多的知识我们会在后面的课程按需讲解。</p><p>现在，我们可以通过一个例子，体验一下中断的使用。在Linux系统上，我们把下面这个代码保存到文件hello.c中，并且使用\"gcc -o hello hello.c\"编译，得到可执行程序hello。再运行它，你就可以看到屏幕上打印出一行\"hello\"。</p><pre><code>// compile command : gcc -o hello hello.c\nvoid sayHello() {\n    const char* s = &quot;hello\\n&quot;;\n    __asm__(&quot;int $0x80\\n\\r&quot;\n            ::&quot;a&quot;(4), &quot;b&quot;(1), &quot;c&quot;(s), &quot;d&quot;(6):);\n}\n\nint main() {\n    sayHello();\n    return 0\n}\n</code></pre><p>相比于使用printf进行打印，需要引入头文件\"stdio.h\"，我们这段代码里没有使用任何头文件，但一样可以在控制台上进行打印。</p><p>这是因为，我们使用了0x80号中断进行了Linux系统调用。系统调用号在eax中，也就是4，代表write这个调用。第一个参数在ebx中，其值为1，代表控制台的标准输出；第二个参数是字符串\"hello\"的地址，在rcx中；第三个参数是字符串的长度，也就是6，存储在edx中。</p><p>这样，我们就通过中断，就不必再使用C语言的printf进行输出，这就绕过了C语言的基础库，完成了向控制台打印的功能。</p><h3>总结</h3><p>今天我们拆解了x86体系架构下的实模式和保护模式，也认识了两个x86演进史上非常重要的CPU。</p><p>8086是16位的CPU，我们称8086的工作模式为实模式，它的特点是直接操作物理内存，内存管理容易出错，要十分小心，代码编写和调试都很困难。</p><p>之后出现的i386，则采用了和实模式不同的保护模式。相比实模式，i386中的保护模式，采用了页式管理，但它没有彻底放弃8086的段式管理，而是将段寄存器中的值由段基址变成了段选择子。段选择子本质是GDT表的下标值，段基址都转移到GDT中去了。</p><p>段式管理负责将逻辑地址转换为线性地址，或者称为虚拟地址，页式管理负责将线性地址映射到物理地址。i386的保护模式采用了段页式混合管理的模式，兼具了段式管理和页式管理的优点。</p><p>除了段页式内存管理这个不同之外，保护模式和实模式的区别还体现在中断描述符表（IDT）上。IDT是保护模式的一个重要组成部分，它保存着i386中断服务程序的入口地址。</p><p>8086和i386对x86架构的CPU影响巨大。直到今天，x86架构的CPU在上电以后，为了与8086保持兼容，还是运行在16位实模式下，也就是说所有访存指令访问的都是物理内存地址。在启动操作系统后，才会切换到保护模式下进行工作。</p><h3>思考题</h3><p>我们今天这节课只讲了16位CPU和32位CPU，并没有讲64位CPU的段式管理是怎么做的。实际上64位CPU的段式管理和32位的结构非常相似，惟一的区别是段描述符的段基址和段长度字段都被废弃了，也就是说不管你将段基址设置成什么，都会被CPU自动识别为0。</p><p>那么请你思考，CPU为什么要这么设计呢？一方面，它还保留了段寄存器，另一方面，它又不再起到逻辑地址转换线性地址的作用，这不是很奇怪吗？请你站在CPU架构师的角度思考一下原因。欢迎你在留言区分享你的想法和收获，我在留言区等你。</p><p><img src=\"https://static001.geekbang.org/resource/image/17/8d/175d317a156b977c0c013909dcc3f68d.jpg?wh=2284x1361\" alt=\"\"></p><p>好啦，这节课到这就结束啦。欢迎你把这节课分享给更多对计算机内存感兴趣的朋友。我是海纳，我们下节课再见！</p>","float_qrcode":"https://static001.geekbang.org/resource/image/83/1c/83e244537b714e899fd2e7896e22371c.png","article_cover_hidden":false,"is_required":true,"score":"31635264000","like_count":9,"article_subtitle":"","audio_download_url":"https://static001.geekbang.org/resource/audio/d5/eb/d5cd31abe3369bf0971df6820da332eb.mp3","had_viewed":false,"article_title":"02｜聊聊x86体系架构中的实模式和保护模式","column_bgcolor":"#F6F7FB","offline_package":"{}","audio_title":"10.20 海纳-02_01.MP3","audio_time_arr":{"m":"19","s":"58","h":"00"},"text_read_percent":0,"cid":450,"article_cshort":"<p>你好，我是海纳。</p><p>上一节课我们讲了虚拟内存的概念，分析了线性地址（虚拟地址）是如何映射到物理地址上的。</p><p>不过，在x86架构诞生之初，其实是没有虚拟内存的概念的。1978年发行的8086芯片是x86架构的首款芯片，它在内存管理上使用的是直接访问物理内存的方式，这种工作方式，有一个专门的名称，那就是<strong>实模式</strong>（Real Mode）。上节课我们也曾简单提到过，直接访问物理内存的工作方式让程序员必须要关心自己使用的内存会不会与其他进程产生冲突，为程序员带来极大的心智负担。</p><p>后来，CPU上就出现虚拟内存的概念，它可以将每个进程的地址空间都隔离开，极大地减轻了程序员的负担，同时由于页表项中有多种权限保护标志，极大地提高了应用程序的数据安全。所以人们把CPU的这种工作模式称为<strong>保护模式</strong>（Protection Mode）。</p><p>从实模式演进到保护模式，x86体系架构的内存管理发生了重大的变化，最大的不同就体现在段式管理和中断的管理上。所以今天这节课，我们会围绕这两个重点，让你彻底理解x86体系架构下的内存管理演进。你也能通过这节课的学习，学会阅读Linux内核源码的段管理和中断管理的相关部分，还可以增加调试coredump文件的能力。</p>","video_width":0,"column_could_sub":true,"video_id":"","sku":"100094901","video_cover":"","author_name":"海纳","column_is_onboard":true,"audio_url":"https://res001.geekbang.org/media/audio/d5/eb/d5cd31abe3369bf0971df6820da332eb/ld/ld.m3u8","chapter_id":"2314","column_had_sub":true,"column_cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg","neighbors":{"left":{"article_title":"01｜虚拟内存：为什么可用内存会远超物理内存？","id":430073},"right":{"article_title":"03 | 内存布局：应用程序是如何安排数据的？","id":431904}},"rate_percent":0,"footer_cover_data":{"img_url":"https://static001.geekbang.org/resource/image/f0/fe/f0c47daf3262a8df6cf352d89896abfe.png","link_url":"https://time.geekbang.org/article/427012","mp_url":""},"float_app_qrcode":"","column_is_experience":false,"rate":{"1":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"2":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"3":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0}},"product_id":100094901,"had_liked":false,"id":431400,"free_get":false,"is_video_preview":false,"article_summary":"在实模式下，程序员是不能通过内存管理单元（Memory Management Unit, MMU）访问地址的，程序必须直接访问物理内存。","column_sale_type":0,"float_qrcode_jump":"https://time.geekbang.org/serv/v4/misc/jump?uri=https%3A%2F%2Ftime.geekbang.org%2Fhybrid%2Fmp%2Fjump%3Furl%3Dhttps%253A%252F%252Fstatic001.geekbang.org%252Fresource%252Fimage%252Fee%252Fb2%252Feedd4090668aabfac91b2b445d27cbb2.png","column_id":450,"article_sharetitle":"聊聊x86体系架构中的实模式和保护模式","article_poster_wxlite":"https://static001.geekbang.org/render/screen/43/e6/4339e72911f7088864f8a6433b05d3e6.jpeg","article_features":0,"comment_count":15,"audio_md5":"d5cd31abe3369bf0971df6820da332eb","offline":{"size":20485859,"file_name":"c1e534768bdfcb67b0a411eb52a21c0f","download_url":"https://static-acl-001.geekbang.org/resource/zip/article/431400/c1e534768bdfcb67b0a411eb52a21c0f.zip?auth_key=1641482077-a5c0bcc456a5422ab95232046d5daade-0-d5617c5ccc8bb704f6a8a1a3999a171e"},"video_size":0,"hls_videos":[],"video_time":"","article_could_preview":false,"article_ctime":1635264000,"share":{"content":"学好内存，掌握系统开发诀窍","title":"海纳 · 编程高手必学的内存知识","poster":"https://static001.geekbang.org/resource/image/bc/c5/bcaf3ff69e538f8749970edf2301bbc5.jpg","cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg"}},"431904":{"text_read_version":0,"audio_size":20426092,"article_cover":"https://static001.geekbang.org/resource/image/c9/76/c9bfb8007e76efa1d2ed31c954213176.jpg","subtitles":[],"product_type":"c1","audio_dubber":"海纳","is_finished":false,"like":{"had_done":false,"count":11},"audio_time":"00:21:16","video_height":0,"article_content":"<p>你好，我是海纳。</p><p>在前边的课程里，我们学习了计算机物理地址和虚拟地址的概念。有了虚拟地址之后，运行在系统里的用户进程看到的地址空间范围，都是虚拟地址空间范围（32位计算机的地址范围是4G；64位计算机的地址范围是256T）。这样的话，就不用再担心内存地址不够用，以及与其他进程之间产生内存地址冲突的问题了。</p><p>前面几节课，我们关注的是如何解决进程之间的冲突，从这节课起，我们一起来看下进程内部的虚拟内存布局，或者说单一进程是如何安排自己的各种数据的。</p><p>学习了这节课，你将理解全局变量和static变量在内存中的位置以及初始化时机，在这个基础上，你还将明白在栈上创建对象和在堆上创建对象有什么不同等问题。这些问题的核心都可以归结到“内存是如何布局的”这个问题上，所以只有深刻地掌握了内存布局的知识，你才能做到以不变应万变，面对各种具体问题才有了分析的方向和思路，进而，你才能写出更加“内存安全”的代码。</p><p>首先，我们来看一下，对于一个典型的进程来说，它的内存空间是由哪些部分组成的？每个部分又被安置在空间的什么位置？</p><h3>抽象内存布局</h3><p>我们知道，CPU运行一个程序，实质就是在顺序执行该程序的机器码。一个程序的机器码会被组织到同一个地方，这个地方就是<strong>代码段</strong>。</p><!-- [[[read_end]]] --><p>另外，程序在运行过程中必然要操作数据。这其中，对于有初值的变量，它的初始值会存放在程序的二进制文件中，而且，这些数据部分也会被装载到内存中，即程序的<strong>数据段</strong>。数据段存放的是程序中已经初始化且不为0的全局变量和静态变量。</p><p>对于未初始化的全局变量和静态变量，因为编译器知道它们的初始值都是0，因此便不需要再在程序的二进制映像中存放这么多0了，只需要记录他们的大小即可，这便是<strong>BSS段</strong>。BSS段这个缩写名字是Block Started by Symbol，但很多人可能更喜欢把它记作Better Save Space的缩写。</p><p>数据段和BSS段里存放的数据也只能是部分数据，主要是全局变量和静态变量，但程序在运行过程中，仍然需要记录大量的临时变量，以及运行时生成的变量，这里就需要新的内存区域了，即程序的<strong>堆空间</strong>跟<strong>栈空间</strong>。与代码段以及数据段不同的是，堆和栈并不是从磁盘中加载，它们都是由程序在运行的过程中申请，在程序运行结束后释放。</p><p>总的来说，一个程序想要运行起来所需要的几块基本内存区域：代码段、数据段、BSS段、堆空间和栈空间。下面就是内存布局的示意图：</p><p><img src=\"https://static001.geekbang.org/resource/image/fc/c0/fcb6231d9cc3841643e4b84462e5b3c0.jpg?wh=2284x1980\" alt=\"\"></p><p>这是程序运行起来所需要的最小功能集，如果你尝试去看Linux 0.11的内核代码的话，会发现它所支持的a.out文件格式和内存布局就是上边的样子。</p><p>除了上面所讲的基本内存区域外，现代应用程序中还会包含其他的一些内存区域，主要有以下几类：</p><ul>\n<li><strong>存放加载的共享库的内存空间</strong>：如果一个进程依赖共享库，那对应的，该共享库的代码段、数据段、BSS段也需要被加载到这个进程的地址空间中。</li>\n<li><strong>共享内存段</strong>：我们可以通过系统调用映射一块匿名区域作为共享内存，用来进行进程间通信。</li>\n<li><strong>内存映射文件</strong>：我们也可以将磁盘的文件映射到内存中，用来进行文件编辑或者是类似共享内存的方式进行进程通信。</li>\n</ul><p>这样我们就初步了解了一个进程内存中需要哪些区域。</p><p>在上面的讨论中，我们并没有区分磁盘的程序段(Section)，以及内存程序段(Segment)的概念，这两个词在国内往往都被翻译成“段”，导致大多数同学会混淆它们。这里我来给你做一个区分。</p><p><img src=\"https://static001.geekbang.org/resource/image/bc/c9/bca1533a0af7ee8476yy12f4b04083c9.jpg?wh=2284x1319\" alt=\"\"></p><p>上图从两个视角展示了应用程序的分布，左边是程序在磁盘中的文件布局结构，右边是程序加载到内存中的内存布局结构。</p><p>对于磁盘的程序，每一个单元结构称为Section。我们可以通过readelf -S的选项，来查看二进制文件中所有的Section信息。对于右边的内存镜像，每一个单元结构称为Segment。我们可以通过readelf -l的选项，来查看二进制文件加载到内存之后的Segment布局信息。</p><p>同时我们也可以看到，往往多个Section会对应一个Segment，例如.text、.rodata等一些只读的Section，会被映射到内存的一个只读/执行的Segment里；而.data、.bss等一些可读写的Section，则会被映射到内存的一个具有读写权限的Segment里。并且对于磁盘二进制中一些辅助信息的Section，例如.symtab、.strtab等，不需要在内存中进行映射。</p><p>总的来说，<strong>Section主要是指在磁盘中的程序段，而Segment则用来指代内存中的程序段，Segment是将具有相同权限属性的Section集合在一起，系统为它们分配的一块内存空间。</strong></p><p>接下来，我们就具体看下Linux系统下内存布局是怎样的。</p><h3>IA-32机器上的Linux进程内存布局</h3><p>在32位机器上，每个进程都具有4GB的寻址能力。Linux系统会默认将高地址的1GB空间分配给内核，剩余的低3GB是用户可以使用的用户空间。下图是32位机器上Linux进程的一个典型的内存布局。在实践中，我们可以通过<code>cat /proc/pid/maps</code>来查看某个进程的实际虚拟内存布局。</p><p><img src=\"https://static001.geekbang.org/resource/image/61/b2/61ee74faa861797b34397ed837a027b2.jpg?wh=2284x1808\" alt=\"\"></p><p>现在，我们从低地址到高地址，依次来解释下图中的布局情况。</p><p>首先，我们发现在32位Linux系统下，从0地址开始的内存区域并不是直接就是代码段区域，而是一段不可访问的保留区。这是因为在大多数的系统里，我们认为比较小数值的地址不是一个合法地址，例如，我们通常在C的代码里会将无效的指针赋值为NULL。因此，这里会出现一段不可访问的内存保留区，防止程序因为出现bug，导致读或写了一些小内存地址的数据，而使得程序跑飞。</p><p>接下来，我们可以看到，代码段从0x08048000的位置开始排布（需要注意的是，以上地址需要gcc编译的时候不开启pie的选项）。就像我们前面提到的，代码段、数据段都是从可执行文件映像中装载到内存中；BSS段则是根据BSS段所需的大小，在加载时生成一段0填充的内存空间。</p><p>紧接着，排在BSS段后边的就是堆空间了。在图中，堆的空间里有一个向上的箭头，这里标明了堆地址空间的增长方向，也就是说，<strong>每次在进程向内核申请新的堆地址时候，其地址的值是在增大的</strong>。与之对应的是栈空间，有一个向下的箭头，说明栈增长的方向是向低地址方向增长，也就是说，<strong>每次进程申请新的栈地址时，其地址值是在减少的</strong>。</p><p>对此，我们可以想象堆和栈分别由两个指针控制，堆指针指明了当前堆空间的边界，栈指针指明了当前栈空间的边界。当堆申请新的内存空间时，只需要将堆指针增加对应的大小，回收地址时减少对应的大小即可。而栈的申请刚好相反。这其实就是内核对堆跟栈使用的最根本的方式，其中，堆的指针叫做“Program break”，栈的指针叫做“Stack pointer”，也就是x86架构下的sp寄存器。我们在后续的课程中会分别展开堆空间跟栈空间的实现原理。</p><p>继续往下看，就到了内存映射区域，这里最常见的就是程序所依赖的共享库，例如libc.so。共享库的代码段、数据段、BSS段都会被装载到这里。</p><p>这里我要说明一点，我们上述的布局分析都是基于Linux系统下关闭了进程地址随机化的选项。如果打开进程地址随机化的模式，其中的堆空间、栈空间和共享库映射的地址，在每次程序运行下都会不一样。这是因为内核在加载的过程中，会对这些区域的起始地址增加一些随机的偏移值，这能增加缓冲区溢出的难度。</p><p>对于这个进程地址随机化选项，我们可以通过<code> sudo sysctl -w kernel.randomize_va_space=val</code>的命令来设置。其中，val=0表示关闭内存地址随机化；val=1表示使得mmap的基地址、栈地址和VDSO的地址随机化；val=2则是在1的基础上增加堆地址的随机化。</p><p>到这里，我们对32位机器下Linux进程的内存布局有了一个清晰的认知。对于64位系统而言，它的基本框架与32位架构是一致的，但在一些细节上，还是有所不同。</p><h3>Intel 64机器上的Linux进程内存布局</h3><p>64位系统理论的寻址范围是2^64，也就是16EB。但是，从目前来看，我们的系统和应用往往用不到这么庞大的地址空间。因此，在目前的Intel 64架构里定义了canonical address的概念，即在64位的模式下，如果地址位63到地址的最高有效位被设置为全 1 或全零，那么该地址被认为是canonical form。目前，Intel 64处理器往往支持48位的虚拟地址，这意味着canonical address必须将第 63 位到第 48 位设置为零或一（这取决于第 47 位是零还是一）。</p><p>所以，目前的64系统下的寻址空间是2^48，即256TB。而且根据canonical address的划分，地址空间天然地被分割成两个区间，分别是0x0 - 0x00007fffffffffff和0xffff800000000000 - 0xffffffffffffffff。这样就直接将低128T的空间划分为用户空间，高128T划分为内核空间。下面这张图展示了Intel 64机器上的Linux进程内存布局：</p><p><img src=\"https://static001.geekbang.org/resource/image/12/1c/1258dabe44e33c66c0f423d8d24a8f1c.jpg?wh=2284x1578\" alt=\"\"></p><p>从图中你可以看到，在用户空间和内核空间之间有一个巨大的内存空洞。这块空间之所以用更深颜色来区分，是因为这块空间的不可访问是由CPU来保证的（这里的地址都不满足Intel 64的Canonical form）。</p><p>对于64位的程序，你在查看/proc/pid/maps的过程中，会发现代码段跟数据段的中间还有一段不可以读写的保护段，它的作用也是防止程序在读写数据段的时候越界访问到代码段，这个保护段可以让越界访问行为直接崩溃，防止它继续往下运行。</p><p>在所有的内存区域中，程序员打交道最多、接触最广泛的就是堆空间。所以，我们接下来重点关注操作系统所提供的，用于管理堆的系统调用是怎样的。这里我会先给你讲如何通过系统调用申请堆空间，关于堆空间更精细的管理，我们将在第9节课介绍。</p><h3>申请堆空间</h3><p>其实，不管是32位系统还是64位系统，内核都会维护一个变量brk，指向堆的顶部，所以，<strong>brk的位置实际上就决定了堆的大小</strong>。Linux系统为我们提供了两个重要的系统调用来修改堆的大小，分别是sbrk和mmap。接下来，我们来学习这两个系统调用是如何使用的。我们先来看sbrk。</p><h4>sbrk</h4><p>sbrk函数的头文件和原型定义如下：</p><pre><code>#include &lt;unistd.h&gt;\n\nvoid* sbrk(intptr_t incr);\n</code></pre><p><strong>sbrk通过给内核的brk变量增加incr，来改变堆的大小，incr可以为负数</strong>。当incr为正数时，堆增大，当incr为负数时，堆减小。如果sbrk函数执行成功，那返回值就是brk的旧值；如果失败，就会返回-1，同时会把errno设置为ENOMEM。</p><p>在实际应用中，我们很少直接使用sbrk来申请堆内存，而是使用C语言提供的malloc函数进行堆内存的分配，然后用free进行内存释放。关于malloc和free的具体实现，我们将在第8节课进行详细讲解。这里你要注意的是，malloc和free函数不是系统调用，而是C语言的运行时库。Linux上的主流运行时库是glibc，其他影响力比较大的运行时库还有musl等。C语言的运行时库多是以动态链接库的方式实现的，关于动态链接库的相关知识，我们会在第7节课加以介绍。</p><p>在C语言的运行时库里，malloc向程序提供分配一小块内存的功能，当运行时库的内存分配完之后，它会使用sbrk方法向操作系统再申请一块大的内存。我们可以将C语言的运行时库类比为零售商，它从操作系统那里批发一块比较大的内存，然后再通过零售的方式一点点地提供给程序员使用。</p><h4>mmap</h4><p>另一个可以申请堆内存的系统调用是mmap，它是最重要的内存管理接口。mmap的头文件和原型如下所示：</p><pre><code>  #include &lt;unistd.h&gt;\n    #include &lt;sys/mman.h&gt;\n\nvoid* mmap(void* addr, size_t length, int prot, int flags, int fd, off_t offset);\n</code></pre><p>我来解释一下上述代码中的各个变量的意义：</p><ul>\n<li>addr代表该区域的起始地址；</li>\n<li>length代表该区域长度；</li>\n<li>prot描述了这块新的内存区域的访问权限；</li>\n<li>flags描述了该区域的类型；</li>\n<li>fd 代表文件描述符；</li>\n<li>offset 代表文件内的偏移值。</li>\n</ul><p>mmap的功能非常强大，根据参数的不同，它可以用于创建共享内存，也可以创建文件映射区域用于提升IO效率，还可以用来申请堆内存。决定它的功能的，主要是prot, flags和fd这三个参数，我们分别来看看。</p><p>prot 的值可以是以下四个常量的组合：</p><ul>\n<li>PROT_EXEC，表示这块内存区域有可执行权限，意味着这部分内存可以看成是代码段，它里面存储的往往是CPU可以执行的机器码。</li>\n<li>PROT_READ，表示这块内存区域可读。</li>\n<li>PROT_WRITE，表示这块内存区域可写。</li>\n<li>PROT_NONE，表示这块内存区域的页面不能被访问。</li>\n</ul><p>而flags的值可取的常量比较多，你可以通过 man mmap查看，这里我只列举一下最重要的四种可取值常量：</p><ul>\n<li><strong>MAP_SHARED</strong>：创建一个共享映射的区域，多个进程可以通过共享映射的方式，来共享同一个文件。这样一来，一个进程对该文件的修改，其他进程也可以观察到，这就实现了数据的通讯。</li>\n<li><strong>MAP_PRIVATE</strong>：创建一个私有的映射区域，多个进程可以使用私有映射的方式，来映射同一个文件。但是，当一个进程对文件进行修改时，操作系统就会为它创建一个独立的副本，这样它对文件的修改，其他进程就看不到了，从而达到映射区域私有的目的。</li>\n<li><strong>MAP_ANONYMOUS</strong>：创建一个匿名映射，也就是没有关联文件。使用这个选项时，fd参数必须为空。</li>\n<li><strong>MAP_FIXED</strong>：一般来说，addr参数只是建议操作系统尽量以addr为起始地址进行内存映射，但如果操作系统判断addr作为起始地址不能满足长度或者权限要求时，就会另外再找其他适合的区域进行映射。如果flags的值取是MAP_FIXED的话，就不再把addr看成是建议了，而是将其视为强制要求。如果不能成功映射，就会返回空指针。</li>\n</ul><p>通常，我们使用私有匿名映射来进行堆内存的分配，具体的原理我们会在第9节课详细分析。</p><p>我们再来看参数fd。当参数fd不为0时，mmap映射的内存区域将会和文件关联，如果fd为0，就没有对应的相关文件，此时就是匿名映射，flags的取值必须为MAP_ANONYMOUS。</p><p>明白了mmap及其各参数的含义后，你肯定想知道什么场景下才会使用mmap，我们又该怎么使用它。</p><h4>mmap的其他应用场景</h4><p>mmap这个系统调用的能力非常强大，我们在后面还会经常遇到它。在这节课里，我们先来了解一下它最常见的用法。</p><p>根据映射的类型，mmap有四种最常用的组合：<br>\n<img src=\"https://static001.geekbang.org/resource/image/98/93/98fcb5aa607b8be9ffa037e9f7eea593.jpg?wh=2284x1285\" alt=\"\"></p><p>其中，私有匿名映射常用于分配内存，也就是我们上文讲的申请堆内存，具体原理我们会在第9节课讲解。而私有文件映射常用于加载动态库，它的原理我们会在第7节课和第8节课进行分析。</p><p>这里我们重点看看共享匿名映射。我们通过一个例子，来了解一下mmap是如何用于父子进程之间的通信的，其他的例子我会在后面的章节陆续给你介绍。它的用法示例代码如下：</p><pre><code>#include &lt;sys/mman.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;unistd.h&gt;\n\nint main() {\n    pid_t pid;\n\n    char* shm = (char*)mmap(0, 4096, PROT_READ | PROT_WRITE,\n        MAP_SHARED | MAP_ANONYMOUS, -1, 0);\n\n    if (!(pid = fork())){\n        sleep(1);\n        printf(&quot;child got a message: %s\\n&quot;, shm);\n        sprintf(shm, &quot;%s&quot;, &quot;hello, father.&quot;);\n        exit(0);\n    }\n  \n    sprintf(shm, &quot;%s&quot;, &quot;hello, my child&quot;);\n    sleep(2);\n    printf(&quot;parent got a message: %s\\n&quot;, shm);\n\n    return 0;\n}\n</code></pre><p>在这个过程中，我们先是用mmap方法创建了一块共享内存区域，命名为 shm（第9行代码），接着，又通过fork这个系统调用创建了子进程。从第13行到第16行代码是子进程的执行逻辑，具体来讲，子进程休眠一秒后，从shm中取出一行字符并打印出来，然后又向共享内存中写入了一行消息（第15行）。</p><p>在子进程的执行逻辑之后，是父进程的执行逻辑（第19行以后）：父进程先写入一行消息，然后休眠两秒，等待子进程完成读取消息和发消息的过程并退出后，父进程再从共享内存中取出子进程发过来的消息。</p><p>这就是共享匿名映射在父子进程间通信的运用。我们使用gcc编译运行上面的例子，可以得到这样的结果：</p><pre><code>$ gcc -o mm mmap_shm.c\n$ ./mm\nchild got a message: hello, my child\nparent got a message: hello, father.\n</code></pre><p>我想请你结合我刚才的讲解，来分析一下这个程序运行的结果，这样你就理解的更透彻了。</p><p>关于共享匿名映射，我们就讲到这里，至于mmap的另一个组合共享文件映射。它的作用其实和共享匿名映射相似，也可以用于进程间通讯。不同的是，共享文件映射是通过文件名来创建共享内存区域的，这就让没有父子关系的进程，也可以通过相同的文件创建共享内存区域，从而可以使用共享内存进行进程间通讯。更具体的原理分析我放在了第10章。</p><h3>总结</h3><p>好，这节课我们就讲到这里，现在我们来总结一下。</p><p>在这节课中，我们从抽象到具体逐步了解了程序运行时的内存布局模型。我们了解到，<strong>一个进程的内存可以分为内核区域和用户区域</strong>。内核区域是由操作系统内核维护的，我们通常并不关心这一块内存是如何使用的。</p><p>程序员最关心的是用户空间，用户空间大致可以分为栈、堆、bss段、数据段和代码段：</p><ul>\n<li><strong>代码段</strong>保存的是程序的机器指令，这一段区域的内存往往是可读可执行，但不可写；</li>\n<li><strong>数据段</strong>保存的是程序的静态变量和全局变量；</li>\n<li><strong>bss段</strong>用于无初值的变量区域；</li>\n<li><strong>堆</strong>是程序员可以自由申请的空间，当我们在写程序时要保存数据，优先会选择堆；</li>\n<li><strong>栈</strong>是函数执行时的活跃记录，这将是我们下一节课要重点分析的内容。</li>\n</ul><p>这5个内存区域通常是由高地址向低地址顺序排列的。但这并不是绝对的，以后我们会看到各种反例，比如代码段的位置完全可以比堆的位置还要高。</p><p>接着，我们以Linux为例，分别研究了IA-32架构和Intel64架构上的内存布局。在这两种情况下，各个段都是按照上述功能进行划分的，区别在于64架构中地址空间更大，而且内核空间和用户空间是不连续的。</p><p>此外，我们还初步学习了两个用于堆管理的系统调用sbrk和mmap。其中，mmap的用法非常复杂，根据调用时传的参数，它有4种常见的用法，分别是私有匿名映射、私有文件映射、共享匿名映射和共享文件映射。其中，共享匿名映射是我们这节课的重点，它可以用于父子进程之间的通讯。关于mmap的其他功能，我们会在后面的课程逐渐展开。</p><p>在接下来的课程中，我会给你详细介绍内存布局中的堆跟栈，这两块也是我们开发人员最常打交道的内存区域，让你对程序运行时的环境和内存状态有一个更深入的理解。</p><h3>思考题</h3><p>在这节课的最后，我给你留一道思考题。</p><p>一块内存区域的权限一般包括可读，可写，可执行三类，请你思考一下，代码段应该被授予怎么样的权限呢？数据段和堆又该被授予怎样的权限呢？欢迎你在留言区和我交流你的想法，我在留言区等你。</p><p><img src=\"https://static001.geekbang.org/resource/image/65/df/658ed22f6d65bc0aac3297139c9bb9df.jpg?wh=2733x1659\" alt=\"\"></p><p>好啦，这节课到这就结束啦。欢迎你把这节课分享给更多对计算机内存感兴趣的朋友。我是海纳，我们下节课再见！</p>","float_qrcode":"https://static001.geekbang.org/resource/image/83/1c/83e244537b714e899fd2e7896e22371c.png","article_cover_hidden":false,"is_required":true,"score":"31635436800","like_count":9,"article_subtitle":"","audio_download_url":"https://static001.geekbang.org/resource/audio/2a/ac/2a29453728e1e9fe2dc3fe69a6f993ac.mp3","had_viewed":false,"article_title":"03 | 内存布局：应用程序是如何安排数据的？","column_bgcolor":"#F6F7FB","offline_package":"{}","audio_title":"10.25 海纳_03.MP3_R.mp3","audio_time_arr":{"m":"21","s":"16","h":"00"},"text_read_percent":0,"cid":450,"article_cshort":"<p>你好，我是海纳。</p><p>在前边的课程里，我们学习了计算机物理地址和虚拟地址的概念。有了虚拟地址之后，运行在系统里的用户进程看到的地址空间范围，都是虚拟地址空间范围（32位计算机的地址范围是4G；64位计算机的地址范围是256T）。这样的话，就不用再担心内存地址不够用，以及与其他进程之间产生内存地址冲突的问题了。</p><p>前面几节课，我们关注的是如何解决进程之间的冲突，从这节课起，我们一起来看下进程内部的虚拟内存布局，或者说单一进程是如何安排自己的各种数据的。</p><p>学习了这节课，你将理解全局变量和static变量在内存中的位置以及初始化时机，在这个基础上，你还将明白在栈上创建对象和在堆上创建对象有什么不同等问题。这些问题的核心都可以归结到“内存是如何布局的”这个问题上，所以只有深刻地掌握了内存布局的知识，你才能做到以不变应万变，面对各种具体问题才有了分析的方向和思路，进而，你才能写出更加“内存安全”的代码。</p><p>首先，我们来看一下，对于一个典型的进程来说，它的内存空间是由哪些部分组成的？每个部分又被安置在空间的什么位置？</p><h3>抽象内存布局</h3><p>我们知道，CPU运行一个程序，实质就是在顺序执行该程序的机器码。一个程序的机器码会被组织到同一个地方，这个地方就是<strong>代码段</strong>。</p>","video_width":0,"column_could_sub":true,"video_id":"","sku":"100094901","video_cover":"","author_name":"海纳","column_is_onboard":true,"audio_url":"https://res001.geekbang.org/media/audio/2a/ac/2a29453728e1e9fe2dc3fe69a6f993ac/ld/ld.m3u8","chapter_id":"2314","column_had_sub":true,"column_cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg","neighbors":{"left":{"article_title":"02｜聊聊x86体系架构中的实模式和保护模式","id":431400},"right":{"article_title":"04 | 深入理解栈：从CPU和函数的视角看栈的管理","id":433530}},"rate_percent":0,"footer_cover_data":{"img_url":"https://static001.geekbang.org/resource/image/f0/fe/f0c47daf3262a8df6cf352d89896abfe.png","link_url":"https://time.geekbang.org/article/427012","mp_url":""},"float_app_qrcode":"","column_is_experience":false,"rate":{"1":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"2":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"3":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0}},"product_id":100094901,"had_liked":false,"id":431904,"free_get":false,"is_video_preview":false,"article_summary":"一个进程的内存可以分为内核区域和用户区域。","column_sale_type":0,"float_qrcode_jump":"https://time.geekbang.org/serv/v4/misc/jump?uri=https%3A%2F%2Ftime.geekbang.org%2Fhybrid%2Fmp%2Fjump%3Furl%3Dhttps%253A%252F%252Fstatic001.geekbang.org%252Fresource%252Fimage%252Fee%252Fb2%252Feedd4090668aabfac91b2b445d27cbb2.png","column_id":450,"article_sharetitle":"内存布局：应用程序是如何安排数据的？","article_poster_wxlite":"https://static001.geekbang.org/render/screen/29/36/29e177c68a72c84d9ee5a6f669f40c36.jpeg","article_features":0,"comment_count":22,"audio_md5":"2a29453728e1e9fe2dc3fe69a6f993ac","offline":{"size":21298010,"file_name":"aa83f87aabbd3366c322b99f1a24d75d","download_url":"https://static-acl-001.geekbang.org/resource/zip/article/431904/aa83f87aabbd3366c322b99f1a24d75d.zip?auth_key=1641482093-51177455a76f489fbad037181f72cb2f-0-e74d534c7bc120087efddea61961d1d7"},"video_size":0,"hls_videos":[],"video_time":"","article_could_preview":false,"article_ctime":1635436800,"share":{"content":"学好内存，掌握系统开发诀窍","title":"海纳 · 编程高手必学的内存知识","poster":"https://static001.geekbang.org/resource/image/bc/c5/bcaf3ff69e538f8749970edf2301bbc5.jpg","cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg"}},"433530":{"text_read_version":0,"audio_size":19270851,"article_cover":"https://static001.geekbang.org/resource/image/34/4b/34b30816b975d882e5e028f80f00914b.jpg","subtitles":[],"product_type":"c1","audio_dubber":"海纳","is_finished":false,"like":{"had_done":false,"count":7},"audio_time":"00:20:03","video_height":0,"article_content":"<p>你好，我是海纳。</p><p>上节课，我们讲到，栈被操作系统安排在进程的高地址处，它是向下增长的。但这只是对栈相关知识的“浅尝辄止”。那我们今天这节课，就会跟着前面的脉络，让你可以更深刻地理解栈的运行原理。</p><p>栈是每一个程序员都很熟悉的话题，但你敢说你真的完全了解它吗？我相信，你在工作中肯定遇到过栈溢出（StackOverflow）的错误，比如在写递归函数的时候，当漏掉退出条件，或者退出条件不小心写错了，就会出现栈溢出错误。我们也经常听说缓冲区溢出带来的严重的安全问题，这在日常的工作中都是要避免的。</p><p>所以，今天这节课，我们继续深入探讨一下栈这个话题，我会带你基于“符合人的直观思维”，也就是函数的层面和CPU的机器指令层面，多角度来理解栈相关的概念。这样，你以后遇到与栈相关的问题的时候，才知道如何着手进行排查。最后，我们还会通过一个缓冲区溢出攻击栈的案例，看看我们在日常工作中如何提升代码的健壮度和安全性。</p><h2>函数与栈帧</h2><p>当我们在调用一个函数的时候，CPU会在栈空间（这当然是线性空间的一部分）里开辟一小块区域，这个函数的局部变量都在这一小块区域里存活。当函数调用结束的时候，这一小块区域里的局部变量就会被回收。</p><p>这一小块区域很像一个框子，所以大家就命名它为stack frame。frame本意是框子的意思，在翻译的时候被译为帧，现在它的中文名字就是栈帧了。</p><!-- [[[read_end]]] --><p>所以，我们可以说，<strong>栈帧本质上是一个函数的活动记录。</strong>当某个函数正在执行时，它的活动记录就会存在，当函数执行结束时，活动记录也被销毁。</p><p>不过，你要注意的是，在一个函数执行的时候，它可以调用其他函数，这个时候它的栈帧还是存在的。例如，A函数调用B函数，此时A的栈帧不会被销毁，而是会在A栈帧的下方，再去创建B函数的栈帧。只有当B函数执行完了，B的栈帧也被销毁了，CPU才会回到A的栈帧里继续执行。</p><p>我们举个例子说明一下，就很好理解了。你可以看一下这个代码：</p><pre><code>#include &lt;stdio.h&gt;\n\nvoid swap(int a, int b) {\n    int t = a;\n    a = b;\n    b = t;\n}\n\nvoid main() {\n    int a = 2;\n    int b = 3;\n    swap(a, b);\n    printf(&quot;a is %d, b is %d\\n&quot;, a, b);\n}\n</code></pre><p>你可以看到，在swap函数中，a和b的值做了一次交换，但是在main函数里，打印a和b的值，a还是2，b还是3。这是为什么呢？从栈帧的角度，这个问题就非常容易理解：</p><p><img src=\"https://static001.geekbang.org/resource/image/3a/68/3a1d6eed77df6233ef0b527c37d34f68.jpg?wh=2284x946\" alt=\"\"></p><p>在main函数执行的时候，main的栈帧里存在变量a和b。当main在调用swap方法的时候，会在main的帧下面新建swap的栈帧。swap的帧里也有局部变量a和b，但是明显这个a、b与main函数里的a、b没有任何关系，不管对swap的帧里的a/b变量做任何操作都不会影响main函数的栈帧。</p><p>接下来，我们再通过一个递归的例子来加深对栈的理解。由于递归执行的过程会出现函数自己调用自己的情况，也就是说，一个函数会对应多个同时活跃的记录（即栈帧）。所以，理解了递归函数的执行过程，我们就能更加深刻地理解栈帧与函数的关系。</p><h2>当我们在谈递归时，我们在谈什么</h2><p>我们先看一下最经典的递归问题：<strong>汉诺塔</strong>。汉诺塔问题是这样描述的：有三根柱子，记为A、B、C，其中A柱子上有n个盘子，从上到下的编号依次为1到n，且上面的盘子一定比下面的盘子小。要求一次只能移动一只盘子，且大的盘子不能压在小的盘子上，那么将所有盘子从A移到C总共需要多少步？</p><p>这道题的详细分析过程是一种递归推导的过程，不是我们这节课的重点，如果你对解法感兴趣的话，可以自己查找相关资料。我们这里，重点来讲解递归程序执行的过程中，栈是怎么样变化的，这样可以帮助我们理解栈的基本工作原理。</p><p>你先看一下汉诺塔问题的求解程序：</p><pre><code>#include &lt;stdio.h&gt;\n\nvoid move(char src, char dst, int n) {\n    printf(&quot;move plate %d form %c to %c\\n&quot;, n, src, dst);\n}\n\nvoid hanoi(char src, char dst, char aux, int n) {\n    if (n == 1) {\n        move(src, dst, 1);\n        return;\n    }\n\n    hanoi(src, aux, dst, n-1);\n    move(src, dst, n);\n    hanoi(aux, dst, src, n-1);\n}\n\nint main() {\n    hanoi('A', 'C', 'B', 5);\n}\n</code></pre><p>这段代码可以打印出借由B柱子将5个盘子从A搬移到C的所有步骤。这个的核心是hanoi函数，在深入分析代码的执行过程之前，我们可以先从符合直观思维的角度尝试理解hanoi函数。</p><p>hanoi函数有四个参数。第一个src代表要搬的起始柱子（开始时是A），第二个代表目标柱子（开始时是C），第三个代表可以借用的中间的那个柱子（开始时是B），第四个参数代表一共要搬的盘子总数（开始时是5）。</p><p>代码的第13行的意义是，如果要从A搬5个盘子到C，可以先将4个盘子搬到B上，然后第14行代表将第5个盘子从A搬到C，第15行代表把B上面的4个盘子搬到C上去。第8行的判断是说当只搬一个盘子的时候，就可以直接调用move方法。</p><p>以上就是递归程序的设计思路。下面我们再具体分析这个代码的执行过程。为了简便起见，我们选择n=3进行分析。</p><p><img src=\"https://static001.geekbang.org/resource/image/c2/4b/c252544e2dcf7fb2049b7dc32009a34b.jpg?wh=2284x1471\" alt=\"\"></p><p>可以看到，当程序在执行hanoi(A, C, B, 3)时，CPU会为其创建一个栈帧，这一帧里记录着变量src、dst、aux和n。</p><p>此时n为3，所以，代码可以执行到第13行，然后就会调用执行hanoi(A, B, C, 2)。这代表着将2个盘子从A搬到B，同样CPU也会为这次调用创建一个栈帧；当这一次调用执行到第13行时，会再调用执行hanoi(A, C, B, 1)，代表把一个盘子从A搬到C。不过，由于这一次调用n为1，所以会直接调用move函数，打印第一个步骤“把盘子1从A搬到C”。</p><p>接下来，程序就会回到hanoi(A, B, C, 2)的栈帧，继续执行第14行，打印第二个步骤”把盘子2从A搬到B”。然后再执行第15行，也就是执行hanoi(C, B, A, 1)。这一步的栈帧变化，你可以看下面这张图。</p><p><img src=\"https://static001.geekbang.org/resource/image/51/72/515de73500c959923c737274a1d67572.jpg?wh=2284x1378\" alt=\"\"></p><p>我们看到，在调用hanoi(C, B, A, 1)的时候，由于n等于1，所以就会打印第三个步骤“把盘子1从C搬到B”，此时hanoi(C, B, A, 1)就执行完了。</p><p>那么接下来，程序就退回到hanoi(A, B, C, 2)的第15行的下一行继续执行，也就是函数的结束，这就意味着hanoi(A, B, C, 2)也执行完了。这个时候，程序就会回退到最高的一层hanoi(A, C, B, 3)的第14行继续执行。这一次就打印了第四个步骤“把盘子3从A搬到C”，此时的栈帧如上图(b)所示。</p><p>然后，程序会执行第15行，再次进入递归调用，创建hanoi(B, C, A, 2)的栈帧。当它执行到第13行时，就会再创建hanoi(B, A, C, 1)的栈帧，此时栈的结构如上图（c）所示。由于n等于1，这一次调用就会打印第五个步骤“把盘子1从B搬到A”。</p><p>再接着就开始退栈了，回到hanoi(B, C, A, 2)的栈帧，继续执行第14行，打印第六个步骤“把盘子2从B搬到C”。然后执行第15行，也就是hanoi(A, C, B, 1)，此时n等于1，直接打印第七个步骤“把盘子1从A搬到C”。接下来就执行退栈，这一次每一个栈帧都执行到了最后一行，所以会一直退到main函数的栈帧中去。退栈的过程比较简单，你自己思考一下就好了。</p><p>这样我们就完成了一次汉诺塔的求解过程。在这个过程中呢，我们观察到，<strong>先创建的帧最后才销毁，后创建的帧最先被销毁</strong>，这就是<strong>先入后出</strong>的规律，也是程序执行时的活跃记录要被叫做栈的原因。</p><p>那么在这里呢，我还想让你做一个小练习。我想让你试着用我们上面分析栈变化的方法，来分析使用深度优先算法打印全排列的程序，这会让你更加深入地理解栈的运行规律，同时掌握深度优先算法的递归写法。</p><pre><code>res = []\n\ndef make(n, level):\n    if n == level:\n        print(res)\n        return\n        \n    for i in range(1, n+1):\n        if i not in res:\n            res.append(i)\n            make(n, level+1)\n            res.pop()\n            \nmake(3, 0)\n</code></pre><h2>从指令的角度理解栈</h2><p>好了，前面递归的例子，是从人的直观思维的角度去理解栈，但是在CPU层面，机器指令又是怎样去理解栈的呢？我们还是通过一个例子来考察一下：</p><pre><code>int fac(int n) {\n    return n == 1 ? 1 : n * fac(n-1);\n}\n</code></pre><p>这是一个使用递归的写法求阶乘的例子，源码是比较简单的，我们可以使用gcc对其进行编译，然后使用objdump对其反编译，观察它编译后的机器码。</p><pre><code># gcc -o fac fac.c\n# objdump -d fac\n</code></pre><p>然后你可以得到以下输出：</p><pre><code>  40052d:       55                      push   %rbp\n  40052e:       48 89 e5                mov    %rsp,%rbp\n  400531:       48 83 ec 10             sub    $0x10,%rsp\n  400535:       89 7d fc                mov    %edi,-0x4(%rbp)\n  400538:       83 7d fc 01             cmpl   $0x1,-0x4(%rbp)\n  40053c:       74 13                   je     400551 &lt;fac+0x24&gt;\n  40053e:       8b 45 fc                mov    -0x4(%rbp),%eax\n  400541:       83 e8 01                sub    $0x1,%eax\n  400544:       89 c7                   mov    %eax,%edi\n  400546:       e8 e2 ff ff ff          callq  40052d &lt;fac&gt;\n  40054b:       0f af 45 fc             imul   -0x4(%rbp),%eax\n  40054f:       eb 05                   jmp    400556 &lt;fac+0x29&gt;\n  400551:       b8 01 00 00 00          mov    $0x1,%eax\n  400556:       c9                      leaveq\n  400557:       c3                      retq\n</code></pre><p>我们来分析一下这段汇编代码。</p><p>第1行是将当前栈基址指针存到栈顶，第2行是把栈指针保存到栈基址寄存器，这两行的作用是把当前函数的栈帧创建在调用者的栈帧之下。保存调用者的栈基址是为了在return时可以恢复这个寄存器。</p><p>第3行的作用呢，是把栈向下增长0x10，这是为了给局部变量预留空间。从这里，你可以看出来运行fac函数要是消耗栈空间的。</p><p>试想一下，如果我们不加n==1的判断，那么fac函数将无法正常返回，会出现一直递归调用回不来的情况，这样栈上就会出现很多fac的帧栈，会造成栈空间耗尽，出现StackOverflow。这里的原理是，操作系统会在栈空间的尾部设置一个禁止读写的页，一旦栈增长到尾部，操作系统就可以通过中断探知程序在访问栈末端。</p><p>第4行是把变量n存到栈上。其中变量n一开始是存储在寄存器edi中的，存储的目标地址是栈基址加上0x4的位置，也就是这个函数栈帧的第一个局部变量的位置。变量n在寄存器edi中是X86的ABI决定的，第一个整型参数一定要使用edi来传递。</p><p>第5行将变量n与常量0x1进行比较。在第6行，如果比较的结果是相等的，那么程序就会跳转到0x400551位置继续执行。我们看到，在这块代码里，0x400551是第13行，它把0x1送到寄存器eax中，然后返回，就是说当n==1时，返回值为1。</p><p>如果第5行的比较结果是不相等的，又会怎么办呢？那第6行就不会跳转，而是继续执行第7行。7、8、9这三行的作用，就是把n-1送到edi寄存器中，也就是说以n-1为参数调用fac函数。这个时候，调用的返回值在eax中，第11行会把返回值与变量n相乘，结果仍然存储在eax中。然后程序就可以跳转到0x400556处结束这次调用。</p><p>理解了fac函数的汇编指令以后，我们再重点讨论callq指令。</p><p>执行callq指令时，CPU会把rip寄存器中的内容，也就是call的下一条指令的地址放到栈上（在这个例子中就是0x40054b），然后跳转到目标函数处执行。当目标函数执行完成后，会执行ret指令，这个指令会从栈上找到刚才存的那条指令，然后继续恢复执行。</p><p>栈空间中的rbp、rsp，以及返回时所用的指令都是非常敏感的数据，一旦被破坏就会造成不可估量的损失。</p><p>不过，你在重现这个例子一定要注意，我们使用不同的优化等级，产生的汇编代码也是不同的。比如如果你用以下命令进行编译，得到的二进制文件中将不再使用rbp寄存器。</p><pre><code># gcc -O1 -o fac fac.c\n</code></pre><p>至于这个结果，我这里就不再展示了，我想让你自己动手试一下，然后在留言区和我们分享。</p><p>到这里，我们已经从人的大脑的理解角度和机器指令的角度，让你加深了对栈和栈帧的理解。现在，我们就从理论转向实操，举一个通过缓冲区溢出来破坏栈的例子。通过这个例子，你就知道在平时的工作中，应该如何避免写出被黑客攻击的不安全代码。</p><h2>栈溢出</h2><p>下面这个测试是我精心构造的例子。因为是演示用的，所以我就把各种无关的代码去掉了，只保留了关键路径上的代码。你先看一下代码：</p><pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\n#define BUFFER_LEN 24\n\nvoid bad() {\n    printf(&quot;Haha, I am hacked.\\n&quot;);\n    exit(1);\n}\n\nvoid copy(char* dst, char* src, int n) {\n    int i;\n    for (i = 0; i &lt; n; i++) {\n        dst[i] = src[i];\n    }\n}\n\nvoid test(char* t, int n) {\n    char s[16];\n    copy(s, t, n);\n}\n\nint main() {\n    char t[BUFFER_LEN] = {\n        'w', 'o', 'l', 'd',\n        'a', 'b', 'a', 'b', 'a', 'b',\n        'a', 'b', 'a', 'b', 'a', 'b',\n    };\n    int n = BUFFER_LEN - 8;\n    int i = 0;\n    for (; i &lt; 8; i++) {\n        t[n+i] = (char)((((long)(&amp;bad)) &gt;&gt; (i*8)) &amp; 0xff);\n    }\n\n    test(t, BUFFER_LEN);\n    printf(&quot;hello\\n&quot;);\n}\n</code></pre><p>你可以用gcc编译器来编译上面这个程序：</p><pre><code>gcc -O1 -o bad bad.c -g -fno-stack-protector\n</code></pre><p>执行它，你可以看到，虽然在main函数里我们并没有调用bad函数，但它却执行了。最后运行结果是“Haha, I am hacked”。</p><p><strong>我们首先来分析一下，这个程序为什么会有这样的运行结果。</strong></p><p>当我们在调用test函数的时候，会把返回地址，也就是rip寄存器中的值，放到栈上，然后就进入了test的栈帧，CPU接着就开始执行test函数了。</p><p>test函数在执行时，会先在自己的栈帧里创建数组s，数组s的长度是16。此时，栈上的布局是这样的：</p><p><img src=\"https://static001.geekbang.org/resource/image/e3/97/e394bda6b98b8306c15e5a39c6d72897.jpg?wh=2284x985\" alt=\"\"></p><p>通过计算，我们可以知道返回地址是变量s的地址 + 16的地方，<strong>这就是我们要攻击的目标</strong>。我们只要在这个地方把原来的地址替换为函数bad的入口地址（第26至34行所做的事情），就可以改变程序的执行顺序，实现了一次<strong>缓冲区溢出</strong>。</p><p>简单地说，数组s的长度是16，理论上我们只能修改以s的地址开始、长度为16的数据。但是我们现在通过copy函数操作了大于16的数据，从而破坏了栈上的关键数据。也就是说我们针对函数调用的返回地址发起了一次攻击。所以，test函数的实现是不安全的。</p><p>其实这种缓冲区溢出，就是指通过一定的手段，来达成修改不属于本函数栈帧的变量的目的，而这种手段多是通过往字符串变量或者数组中写入错误的值而造成的。</p><p><strong>有两种常见的手段可以对这一类攻击进行防御。</strong></p><p>第一，对入参进行检查，尽量使用strncpy来代替strcpy。因为strcpy不对参数长度做限制，而strncpy则会做检查。比如上述例子中，如果我们对参数n做检查，要求它的值必须大于0且小于缓冲区长度，就可以阻击缓冲区溢出攻击了。</p><p>第二，可以使用gcc自带的栈保护机制，这就是-fstack-protector选项。你查gcc手册（在Linux系统使用“man gcc”就能查到）可以看到它的一些相关信息。</p><p>当-fstack-protector启用时，当其检测到缓冲区溢出时(例如，缓冲区溢出攻击）时会立即终止正在执行的程序，并提示其检测到缓冲区存在的溢出的问题。这种机制是通过在函数中的易被受到攻击的目标上下文添加保护变量来完成的。这些函数包括使用了alloca函数以及缓冲区大小超过8bytes的函数。这些保护变量在进入函数的时候进行初始化，当函数退出时进行检测，如果某些变量检测失败，那么会打印出错误提示信息并且终止当前的进程。</p><p>从4.8版本开始，gcc中的这个选项是默认打开的。如果我们在编译时，不加-fno-stack-protector，gcc就会给可执行程序增加栈保护的功能。这样的话，运行结果就会出现Segment Fault，导致进程崩溃。不过，你要知道，在遇到攻击时自己崩溃，相比起去执行攻击者的恶意代码，影响可就小多了。</p><p>这里，我们为了演示的方便，使用-fno-stack-protector关闭了这个选项。不过，在日常开发中，这个选项虽然使得栈的安全大大加强了，但它也有巨大的性能损耗。在一个实际的线上例子中，关闭这个选项可以提升8%至10%的性能。</p><p>当然这个选项也不是万能的，攻击者依然能通过精心构造数据来达成它的目标。所以在写代码的时候，你还是应该对缓冲区安全多加注意。</p><h2>总结</h2><p>这节课，我们一起学习了栈帧的作用，并通过汉诺塔程序的求解过程，来分析了栈帧的创建和销毁的过程，以此揭示了函数和栈帧的关系。栈帧就是函数的活动记录，当函数被调用时，栈帧创建，当函数调用结束后，栈帧消失。</p><p>在程序的执行过程中，尤其是递归程序的执行过程中，你可以清楚地观察到栈帧的创建销毁，满足后入先出的规律。这也是人们把管理函数的活跃记录的区域称为栈的原因。</p><p>除了用人的直观思维来理解栈帧之外，我还带你看了在汇编代码级别，栈帧是怎么真实地被创建和销毁的，或者说栈是怎么增长和收缩的。这会进一步加深你对栈的理解。</p><p>这节课的最后，我也通过一个缓冲区溢出的例子说明了，在栈空间内使用缓冲区的时候，你必须要十分小心，要避免恶意的输入对缓冲区进行越界读写，破坏栈的结构，从而导致关键数据被修改。我们演示了一个破坏了调用者返回地址的例子，以此来说明当返回地址被破坏以后，攻击者可以让程序的控制流转向我们不希望的地方。</p><p>很多人以为安全和攻击是做安全的同事才应该关心的问题，这个想法是不对的。要想提高软件的整体水平，每一个程序员都应该写出健壮而安全的代码。只有每一块砖都足够坚固，我们才有可能建成一个安全可靠的建筑物。</p><h2>思考题</h2><p>我们这节课前面讲的swap函数的例子，是很多新手会犯的错误。在C语言中，为了使swap可以交换main函数里的a/b两个变量的值，我们可以使用指针：</p><pre><code>#include &lt;stdio.h&gt;\n\nvoid swap(int* a, int* b) {\n    int t = *a;\n    *a = *b;\n    *b = t;\n}\n\nvoid main() {\n    int a = 2;\n    int b = 3;\n    swap(&amp;a, &amp;b);\n    printf(&quot;a is %d, b is %d\\n&quot;, a, b);\n}\n</code></pre><p>或者在C++中，直接使用引用，<strong>引用可以看成是一个能自动解引用的指针</strong>：</p><pre><code>#include &lt;stdio.h&gt;\n\nvoid swap(int&amp; a, int&amp; b) {\n    int t = a;\n    a = b;\n    b = t;\n}\n\nvoid main() {\n    int a = 2;\n    int b = 3;\n    swap(a, b);\n    printf(&quot;a is %d, b is %d\\n&quot;, a, b);\n}\n</code></pre><p>那么，我想请你从汇编代码层面思考，这是怎么做到的？（提示：要从“传入栈帧的参数到底是什么”这个角度去思考）另外，如果你对Java程序比较熟悉，你也可以思考一下Java能不能实现类似的功能？欢迎你在留言区和我交流你的想法，我在留言区等你。</p><p><img src=\"https://static001.geekbang.org/resource/image/15/c4/153e87bf17b61efb61609a264192c1c4.jpg?wh=2284x3407\" alt=\"\"></p><p>好，这节课到这就结束啦。欢迎你把这节课分享给更多对计算机内存感兴趣的朋友。我是海纳，我们下节课再见！</p>","float_qrcode":"https://static001.geekbang.org/resource/image/83/1c/83e244537b714e899fd2e7896e22371c.png","article_cover_hidden":false,"is_required":true,"score":"31635696000","like_count":6,"article_subtitle":"","audio_download_url":"https://static001.geekbang.org/resource/audio/7e/a7/7e86885044d086e26fb6e758ba3726a7.mp3","had_viewed":false,"article_title":"04 | 深入理解栈：从CPU和函数的视角看栈的管理","column_bgcolor":"#F6F7FB","offline_package":"{}","audio_title":"10.27 海纳_04_01.MP3","audio_time_arr":{"m":"20","s":"03","h":"00"},"text_read_percent":0,"cid":450,"article_cshort":"<p>你好，我是海纳。</p><p>上节课，我们讲到，栈被操作系统安排在进程的高地址处，它是向下增长的。但这只是对栈相关知识的“浅尝辄止”。那我们今天这节课，就会跟着前面的脉络，让你可以更深刻地理解栈的运行原理。</p><p>栈是每一个程序员都很熟悉的话题，但你敢说你真的完全了解它吗？我相信，你在工作中肯定遇到过栈溢出（StackOverflow）的错误，比如在写递归函数的时候，当漏掉退出条件，或者退出条件不小心写错了，就会出现栈溢出错误。我们也经常听说缓冲区溢出带来的严重的安全问题，这在日常的工作中都是要避免的。</p><p>所以，今天这节课，我们继续深入探讨一下栈这个话题，我会带你基于“符合人的直观思维”，也就是函数的层面和CPU的机器指令层面，多角度来理解栈相关的概念。这样，你以后遇到与栈相关的问题的时候，才知道如何着手进行排查。最后，我们还会通过一个缓冲区溢出攻击栈的案例，看看我们在日常工作中如何提升代码的健壮度和安全性。</p><h2>函数与栈帧</h2><p>当我们在调用一个函数的时候，CPU会在栈空间（这当然是线性空间的一部分）里开辟一小块区域，这个函数的局部变量都在这一小块区域里存活。当函数调用结束的时候，这一小块区域里的局部变量就会被回收。</p><p>这一小块区域很像一个框子，所以大家就命名它为stack frame。frame本意是框子的意思，在翻译的时候被译为帧，现在它的中文名字就是栈帧了。</p>","video_width":0,"column_could_sub":true,"video_id":"","sku":"100094901","video_cover":"","author_name":"海纳","column_is_onboard":true,"audio_url":"https://res001.geekbang.org/media/audio/7e/a7/7e86885044d086e26fb6e758ba3726a7/ld/ld.m3u8","chapter_id":"2314","column_had_sub":true,"column_cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg","neighbors":{"left":{"article_title":"03 | 内存布局：应用程序是如何安排数据的？","id":431904},"right":{"article_title":"05 | 栈的魔法：从栈切换的角度理解进程和协程","id":435493}},"rate_percent":0,"footer_cover_data":{"img_url":"https://static001.geekbang.org/resource/image/f0/fe/f0c47daf3262a8df6cf352d89896abfe.png","link_url":"https://time.geekbang.org/article/427012","mp_url":""},"float_app_qrcode":"","column_is_experience":false,"rate":{"1":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"2":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"3":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0}},"product_id":100094901,"had_liked":false,"id":433530,"free_get":false,"is_video_preview":false,"article_summary":"栈帧本质上是一个函数的活动记录。","column_sale_type":0,"float_qrcode_jump":"https://time.geekbang.org/serv/v4/misc/jump?uri=https%3A%2F%2Ftime.geekbang.org%2Fhybrid%2Fmp%2Fjump%3Furl%3Dhttps%253A%252F%252Fstatic001.geekbang.org%252Fresource%252Fimage%252Fee%252Fb2%252Feedd4090668aabfac91b2b445d27cbb2.png","column_id":450,"article_sharetitle":"04 | 深入理解栈：从CPU和函数的视角看栈的管理","article_poster_wxlite":"https://static001.geekbang.org/render/screen/d7/5c/d728620edf55ea8dc45140cd34d9c25c.jpeg","article_features":0,"comment_count":17,"audio_md5":"7e86885044d086e26fb6e758ba3726a7","offline":{"size":20369358,"file_name":"0a9dce60ad110646452691d62b5dcd41","download_url":"https://static-acl-001.geekbang.org/resource/zip/article/433530/0a9dce60ad110646452691d62b5dcd41.zip?auth_key=1641482109-e8ee4f9e08344219b61a6ed9d1bd8fab-0-a3683f4d2f2953b649a66c7c8b3b10e7"},"video_size":0,"hls_videos":[],"video_time":"","article_could_preview":false,"article_ctime":1635696000,"share":{"content":"学好内存，掌握系统开发诀窍","title":"海纳 · 编程高手必学的内存知识","poster":"https://static001.geekbang.org/resource/image/bc/c5/bcaf3ff69e538f8749970edf2301bbc5.jpg","cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg"}},"435493":{"text_read_version":0,"audio_size":19612374,"article_cover":"https://static001.geekbang.org/resource/image/30/36/30951b79d5ce3e01fdc0fb455736c336.jpg","subtitles":[],"product_type":"c1","audio_dubber":"海纳","is_finished":false,"like":{"had_done":false,"count":7},"audio_time":"00:20:25","video_height":0,"article_content":"<p>你好，我是海纳。</p><p>上一节课，我们了解到函数在执行的时候，就会在栈上创建栈帧，那么函数执行的上下文都将保存在栈帧里。今天，我们就再来进一步分析，栈切换在计算机系统设计中所发挥的重要作用。</p><p>几乎所有的程序员都会遇到并发程序。因为多进程或者多线程程序可以并发执行，充分利用多CPU多核的计算资源来完成任务，会大大提升应用程序的性能。</p><p>所以，我相信你在工作中也遇到过多线程程序，但不知道你是否考虑过进程和线程是如何切换的呢？很多文章都介绍了，操作系统为了避免频繁进入内核态，会把很多工作都尽量放在用户态。那么你有没有仔细思考过内核态、用户态到底意味着什么呢？</p><p>要回答上面的问题，我们就要理解这些概念背后最重要的一个步骤：对执行单元的上下文环境进行切换。它就是由栈这个核心数据结构支撑的，这也是我们今天学习的重点内容。</p><p>通过今天的学习，你将掌握协程的基本知识，这样，你在C++中使用各种协程库，或者在Lua、Go等语言中使用原生协程的时候，就能理解它们背后发生了什么，也可以帮你写出正确的IO程序。你还将深入理解操作系统用户态和内核态，这样，你在做架构的时候，就能正确评估操作系统进入内核态的开销是多少。</p><p>在讲解执行单元的切换与栈的关系之前，我们先来给出它的准确定义。</p><!-- [[[read_end]]] --><h3>什么是执行单元</h3><p>执行单元是指CPU调度和分派的基本单位，它是一个CPU能正常运行的基本单元。执行单元是可以停下来的，只要能把CPU状态（其实就是寄存器的值）全部保存起来，等到这个执行单元再被调度的时候，就把状态恢复过来就行了。我们把这种保存状态，挂起，恢复执行，恢复状态的完整过程，称为执行单元的调度(Scheduling)。</p><p>具体来说，常见的执行单元有进程，线程和协程三种，接下来，我们详细说明这三种执行单元的区别和联系。我们先来比较进程和线程。</p><h4>理解进程和线程</h4><p>当运行一个可执行程序的时候，操作系统就会启动一个进程。进程会被操作系统管理和调度，被调度到的进程就可以独占CPU了。</p><p>CPU就像是一个可以轮流使用的工作台，多个进程可以在工作台上工作，时间到了就会带着自己的工作离开工作台，换下一个进程上来工作。</p><p>进程有自己独立的内存空间和页表，以及文件表等等各种私有资源，如果使用多进程系统，让多个任务并发执行，那么它所占用的资源就会比较多。线程的出现解决了这个问题。</p><p>同一个进程中的线程则共享该进程的内存空间，文件表，文件描述符等资源，它与同一个进程的其他线程共享资源分配。除了共享的资源，每个线程也有自己的私有空间，这就是线程的栈。线程在执行函数调用的时候，会在自己的线程栈里创建函数栈帧。</p><p>根据上面所说的特点，人们常把进程看做是资源分配的单位，把线程才看成一个具体的执行实体。</p><p>由于线程的切换过程和进程的切换过程十分相似，我们这节课就只以进程的切换为重点进行讲解，请你一定要自己查找相关资料，对照进程切换的过程，去理解线程的切换过程。</p><h4>理解协程</h4><p>协程是比线程更轻量的执行单元。进程和线程的调度是由操作系统负责的，而协程则是由执行单元相互协商进行调度的，所以它的切换发生在用户态。只有前一个协程主动地执行yield函数，让出CPU的使用权，下一个协程才能得到调度。</p><p>因为程序自己负责协程的调度，所以大多数时候，我们可以让不那么忙的协程少参与调度，从而提升整个程序的吞吐量，而不是像进程那样，没有繁重任务的进程，也有可能被换进来执行。</p><p>协程的切换和调度所耗费的资源是最少的，Go语言把协程和IO多路复用结合在一起，提供了非常便捷的IO接口，使得协程的概念深入人心。</p><p>从操作系统和Web Server演进的历史来看，先是多进程系统的出现，然后出现了多线程系统，最后才是协程被大规模使用，这个演进历程背后的逻辑就是执行单元需要越来越轻量，以支持更大的并发总数。</p><p>但我们这节课却要先讲协程，这是因为从实现层面来说，协程是最简单的，当你理解了协程的实现原理，再回头学习进程就比较容易了，所以我们先来学习协程的原理。</p><h3>协程是怎么调度和切换的？</h3><p>在讲解协程的理论之前，我们先通过一个最简单的协程的例子，来观察协程的运作机制：</p><pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\n#define STACK_SIZE 1024\n\ntypedef void(*coro_start)();\n\nclass coroutine {\npublic:\n    long* stack_pointer;\n    char* stack;\n\n    coroutine(coro_start entry) {\n        if (entry == NULL) {\n            stack = NULL;\n            stack_pointer = NULL;\n            return;\n        }\n\n        stack = (char*)malloc(STACK_SIZE);\n        char* base = stack + STACK_SIZE;\n        stack_pointer = (long*) base;\n        stack_pointer -= 1;\n        *stack_pointer = (long) entry;\n        stack_pointer -= 1;\n        *stack_pointer = (long) base;\n    }\n\n    ~coroutine() {\n        if (!stack)\n            return;\n        free(stack);\n        stack = NULL;\n    }\n};\n\ncoroutine* co_a, * co_b;\n\nvoid yield_to(coroutine* old_co, coroutine* co) {\n    __asm__ (\n        &quot;movq %%rsp, %0\\n\\t&quot;\n        &quot;movq %%rax, %%rsp\\n\\t&quot;\n        :&quot;=m&quot;(old_co-&gt;stack_pointer):&quot;a&quot;(co-&gt;stack_pointer):);\n}\n\nvoid start_b() {\n    printf(&quot;B&quot;);\n    yield_to(co_b, co_a);\n    printf(&quot;D&quot;);\n    yield_to(co_b, co_a);\n}\n\nint main() {\n    printf(&quot;A&quot;);\n    co_b = new coroutine(start_b);\n    co_a = new coroutine(NULL);\n    yield_to(co_a, co_b);\n    printf(&quot;C&quot;);\n    yield_to(co_a, co_b);\n    printf(&quot;E\\n&quot;);\n    delete co_a;\n    delete co_b;\n    return 0;\n}\n</code></pre><p>我们使用g++对这段代码进行编译，注意要使用O0进行编译，不能使用更高的优化级别，这是因为更高级别的优化会内联yield_to方法，这就使得栈的布局和程序中期望的不相符了。我们先来看看这段代码的运行的结果，如下所示：</p><pre><code># g++ -g -o co -O0 coroutine.cpp\n# ./co\nABCDE\n</code></pre><p>这段代码的神奇之处在于，main函数在执行到一半的时候，可以停下来去执行start_b函数，这和我们通常遇到的函数调用是很不一样的。而这种效果是通过协程达到的。</p><p>你可以看到，在main函数的执行过程中（即代码的57行），CPU通过执行yield_to方法转到另外一个协程。新的协程的入口函数是start_b，所以，CPU就转而去执行start_b，在start_b执行到48行的时候，还能再通过yield_to，再回到main函数中继续执行。</p><p>下面我们来看协程是怎么实现这一点的。</p><p>我们调用构造函数coroutine创建了两个协程co_a和co_b（即代码的55、56行）。其中，co_b的入口地址是函数start_b，co_a没有入口地址。</p><p>我们具体来看在coroutine里发生了什么。其实在创建这两个协程之前，coroutine已经申请了一段大小为1K的内存作为协程栈，然后让栈底指针base指向栈的底部（第21行）。因为栈是由上向下增长的，所以，我们又在协程栈上放入了base地址和起始地址（第23~27行），此时，协程栈内的数据是这样的，如图1所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/b1/55/b12a4941be1f61d5132500c3d1b8ec55.jpg?wh=2284x1151\" alt=\"\"></p><p>在准备好协程栈以后，就可以调用yield_to方法进行协程的切换。在上一节中，我们提到过协程要主动调用yield方法将CPU的占有权让出来，后面的协程才能执行。所以，协程切换的关键机制就肯定隐藏在yield_to方法里。</p><p>yield_to方法具体做了什么事情呢？我们需要通过机器码来进行说明。这里我们使用\"objdump -d\"命令查看yield_to方法经过编译以后的机器码：</p><pre><code>000000000040076d &lt;_Z8yield_toP9coroutineS0_&gt;:\n  40076d:       55                      push   %rbp\n  40076e:       48 89 e5                mov    %rsp,%rbp\n  400771:       48 89 7d f8             mov    %rdi,-0x8(%rbp)\n  400775:       48 89 75 f0             mov    %rsi,-0x10(%rbp)\n  400779:       48 8b 45 f0             mov    -0x10(%rbp),%rax\n  40077d:       48 8b 00                mov    (%rax),%rax\n  400780:       48 8b 55 f8             mov    -0x8(%rbp),%rdx\n  400784:       48 89 22                mov    %rsp,(%rdx)\n  400787:       48 89 c4                mov    %rax,%rsp\n  40078a:       5d                      pop    %rbp\n  40078b:       c3                      retq\n</code></pre><p>yield_to中，参数old_co指向老协程，co则指向新的协程，也就是我们要切换过去执行的目标协程。</p><p>这段代码的作用是，首先，把当前rsp寄存器的值存储到old_co的stack_pointer属性（第9行），并且把新的协程的stack_pointer属性更新到rsp寄存器（第10行），然后，retq指令将会从栈上取出调用者的地址，并跳转回调用者继续执行（第12行，这是上一节课的内容，如果你不熟悉，可以再自行复习一下）。</p><p>结合以上分析，我们可以想象在协程示例代码的第57行，当调用这一次yield_to时，rsp寄存器刚好就会指向新的协程co的栈，接着就会执行\"pop rbp\"和\"retq\"这两条指令。这里你需要注意一下，栈的切换，并没有改变指令的执行顺序，因为栈指针存储在rsp寄存器中，当前执行到的指令存储在IP寄存器中，rsp的切换并不会导致IP寄存器发生变化。</p><p>而显然，如图1所示，我们刚才精心准备的base地址正好就是为了\"pop rbp\"准备的，而start_b则是为了retq准备的。执行这次retq，CPU就会跳转到start_b函数中去运行了。</p><p>经过这种切换，系统中会出现两个栈，如图2所示：<br>\n<img src=\"https://static001.geekbang.org/resource/image/63/31/63ffc7601254bb2460d65c43f57a7931.jpg?wh=2284x1249\" alt=\"\"></p><p>当程序继续执行时，在start_b中调用了yield_to，CPU又会转移回协程a的栈上，这样在执行retq时，就会返回到main函数里继续运行了。</p><p>在这个过程中，我们并没有使用任何操作系统的系统调用，就实现了控制流的转移。也就是说，在同一个线程中，我们真正实现了两个执行单元。这两个执行单元并不像线程那样是抢占式地运行，而是相互主动协作式执行，所以，这样的执行单元就是协程。我们可以看到，协程的切换全靠本执行单元主动调用yield_to来把执行权让渡给其他协程。</p><p>每个协程都拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方（上述例子中，保存在coroutine对象中），在切回来的时候，恢复先前保存的寄存器上下文和栈。</p><p>分析到这里，这个程序对我们而言，已经没有太多秘密了，它所有看上去神奇的地方，不过就是切换了程序运行的栈指针而已。</p><p>分析到这里，我们就可以准确地定义协程了。协程是一种轻量级的，用户态的执行单元。相比线程，它占用的内存非常少，在很多实现中（比如Go语言）甚至可以做到按需分配栈空间。</p><p>它主要有三个特点：</p><ol>\n<li>占用的资源更少;</li>\n<li>所有的切换和调度都发生在用户态。</li>\n<li>它的调度是协商式的，而不是抢占式的。</li>\n</ol><p>前两个特点容易理解，我来给你重点解释一下第三个特点。</p><p>目前主流语言基本上都选择了多线程作为并发设施，与线程相关的概念是抢占式多任务（Preemptive multitasking），而与协程相关的是协作式多任务。不管是进程还是线程，每次阻塞、切换都需要陷入系统调用(system call)，先让CPU执行操作系统的调度程序，然后再由调度程序决定该哪一个进程(线程)继续执行。</p><p>由于抢占式调度执行顺序无法确定，我们使用线程时需要非常小心地处理同步问题，而协程完全不存在这个问题。因为协作式的任务调度，是要用户自己来负责任务的让出的。如果一个任务不主动让出，其他任务就不会得到调度。这是协程的一个弱点，但是如果使用得当，这其实是一个可以变得很强大的优点。</p><p>你可以尝试将编译优化等级设为O1，观察yield_to函数的机器码的变化，然后就可以理解当栈基址寄存器的保存和恢复如果被优化掉以后，我们准备的那个数据就不再起作用了。也请你尝试对上述代码进行修改，以适应O1优化。</p><p>在理解了协程以后，我们再回过头来看进程。</p><h3>进程是怎么调度和切换的？</h3><p>进程切换的原理其实与协程切换的原理大致相同，都是将上下文保存在特定的位置，切换到新的进程去执行。所不同的是，操作系统为我们提供了进程的创建、销毁、信号通信等基础设施，这使得程序员可以很方便地创建进程。如果一个进程a创建了另外一个进程b，则称a为父进程，b为子进程。</p><p>我先带你通过下面这个例子，直观地感受多进程运行的情况：</p><pre><code>#include &lt;unistd.h&gt;\n#include &lt;stdio.h&gt;\n\nint main() {\n    pid_t pid;\n    if (!(pid = fork())) {\n        printf(&quot;I am child process\\n&quot;);\n        exit(0);\n    }\n    else {\n        printf(&quot;I am father process\\n&quot;);\n        wait(pid);\n    }\n\n    return 0;\n}\n</code></pre><p>编译执行这段代码的结果如下所示：</p><pre><code># gcc -o p process.c\n# ./p\nI am father process\nI am child process\n</code></pre><p>在这个结果里，我们可以看到，在if分支和else分支中的代码都被运行了。曾经有个笑话说，这个世界上最远的距离，不是你在天涯，我在海角，而是你在if里，我在else里。由此可见，这个笑话也并不正确，还是要看if条件里填的是什么。</p><p>在上面的代码中，fork是一个系统调用，用于创建进程，如果其返回值为0，则代表当前进程是子进程，如果其返回值不为0，则代表当前进程是父进程，而这个返回值就是子进程的进程ID。</p><p>我们看到，子进程在打印完一行语句后就调用exit退出执行了。父进程在打印完以后，并没有立即退出，而是调用wait函数等待子进程退出。由于进程的调度执行是操作系统负责的，具有很大的随机性，所以父进程和子进程谁先退出，我们并不能确定。为了避免子进程变成孤儿进程，我们采用了让父进程等待子进程退出的办法，就是对两个进程进行同步。</p><p>其实，这段程序最难理解的是第6行，为什么一次fork后，会有两种不同的返回值？这是因为fork方法本质上在系统里创建了两个栈，这两个栈一个是父进程的，一个是子进程的。创建的时候，子进程完全“继承”了父进程的所有数据，包括栈上的数据。父子进程栈的情况如图3所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/05/bb/05f522cfcaa8ef2d79fa7221cc1bb8bb.jpg?wh=2284x1445\" alt=\"\"></p><p>在图3里，只要有一个进程对栈进行修改，栈就会复制一份，然后父子进程各自持有一份。图中的黄色部分也是进程共用的，如果有一个进程修改它，也会复制一份副本，这种机制叫做写时复制。</p><p>接着，操作系统就会接管两个进程的调度。当父进程得到调度时，父进程的栈上是fork函数的frame，当CPU执行fork的ret语句时，返回值就是子进程的ID。</p><p>而当子进程得到调度时，rsp这个栈指针就将会指向子进程的栈，子进程的栈上也同样是fork函数的frame，它也会执行一次fork的ret语句，其返回值是0。</p><p>所以第6行虽然是同一个变量pid，但实际上，它在子进程的main函数的栈帧里有一个副本，在父进程的栈帧里也有一个副本。从fork开始，父进程和子进程就已经分道扬镳了。你可以将进程栈的切换与协程栈的切换对比着进行学习。</p><p>我们通过一个例子展示了进程是如何创建的，并且分析了进程创建背后栈的变化过程。你可以看到，进程做为一种执行单元，它的切换还是要依赖于栈切换这个核心机制。</p><p>关于fork的更多的细节，我们将在第10课再加以分析。在这节课，将进程的栈类比于协程栈已经足够了。</p><h3>用户态和内核态是怎么切换的？</h3><p>在第二节课里，我们讲解了中断描述符表，并且用系统调用write这个例子，来展示如何通过软中断进入内核态。实际上，内核态和用户态的切换也依赖栈的切换。因为在第二节课里，我们还没有讲到栈，所以在讲到用户态切换内核态的时候，并没有涉及到栈的切换，现在，我们补上用户态和内核态切换的最后一块拼图。</p><p>操作系统内核在运行的时候，肯定也是需要栈的，这个栈称为内核栈，它与用户应用程序使用的用户态栈是不同的。只有高权限的内核代码才能访问它。而内核态与用户态的相互切换，其中最重要的一个步骤就是两个栈的切换。</p><p>中断发生时，CPU根据需要跳转的特权级，去一个特定的结构中（不同的CPU会有所不同，比如i386就存在TSS中，但不管是什么CPU，一定会有一个类似的结构），取得目标特权级所对应的stack段选择子和栈顶指针，并分别送入ss寄存器和rsp寄存器，这就完成了一次栈的切换。</p><p>然后，IP寄存器跳入中断服务程序开始执行，中断服务程序会把当前CPU中的所有寄存器，也就是程序的上下文都保存到栈上，这就意味着用户态的CPU状态其实是由中断服务程序在系统栈上进行维护的。如图4所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/e0/65/e015b6a3b4d93431194614e065078e65.jpg?wh=2284x2101\" alt=\"\"></p><p>一般来说，当程序因为call指令或者int指令进行跳转的时候，只需要把下一条指令的地址放到栈上，供被调用者执行ret指令使用，这样可以便于返回到调用函数中继续执行。但图4中的内核态栈里有一点特殊之处，就是CPU自动地将用户态栈的段选择子ss3，和栈顶指针rsp3都放到内核态栈里了。这里的数字3代表了CPU特权级，内核态是0，用户态是3。</p><p>当中断结束时，中断服务程序会从内核栈里将CPU寄存器的值全部恢复，最后再执行\"iret\"指令（注意不是ret，而是iret，这表示是从中断服务程序中返回）。而iret指令就会将ss3/rsp3都弹出栈，并且将这个值分别送到ss和rsp寄存器中。这样就完成了从内核栈到用户栈的一次切换。同时，内核栈的ss0和rsp0也会被保存到前文所说的一个特定的结构中，以供下次切换时使用。</p><h3>总结</h3><p>这节课我们举例说明了进程，线程和协程的基本概念，并对它们的调度做了简单的说明。然后介绍了服务端编程模型从多进程向协程演进的历程。</p><p>接着，我们重点介绍了栈切换的整个过程。<strong>栈切换的核心就是栈指针rsp寄存器的切换，只要我们想办法把rsp切换了就相当于换了执行单元的上下文环境</strong>。这一节课所有的讲解都可以归到这条线索上。</p><p>我们又用了协程切换，进程栈的写时复制和切换，以及用户态和内核态的切换这三个例子来说明举例说明栈的切换所发挥的重要作用。</p><p>通过两节课的学习，我们对进程中的栈空间相关的知识进行一次比较深入的梳理。从中我们可以得到一个结论：栈往往和执行单元是一对一的关系，栈的活跃就代表着它所对应的执行单元的活跃。栈上的数据非常敏感，一旦被攻击，往往会造成巨大的破坏。</p><p>在第三节课里，我们学习了堆空间的管理方式，这两节课又学习了栈空间的运行机制，这两部分内容都是程序运行时所要操作的内存。在这之后，我们将目光转移到程序的汇编代码，研究一下程序的静态数据是如何组织和划分的。</p><h3>思考题</h3><p>我们这节课讲到了协程和进程的栈切换，但没有讲线程的栈切换。请你思考，线程的栈切换是更类似协程那种提前创建好的方式，还是更类似于进程那种按需写时复制？为什么？欢迎你在留言区和我交流你的想法，我在留言区等你。</p><p><img src=\"https://static001.geekbang.org/resource/image/c4/e9/c481a2a217c906ed118770a91f17c4e9.jpg?wh=2284x1355\" alt=\"\"></p><p>欢迎你在留言区分享你的想法和收获，我在留言区等你。如果这节课帮到了你，也欢迎你把这节课分享给自己的朋友。我们下一讲再见！</p>","float_qrcode":"https://static001.geekbang.org/resource/image/83/1c/83e244537b714e899fd2e7896e22371c.png","article_cover_hidden":false,"is_required":true,"score":"31635868800","like_count":21,"article_subtitle":"","audio_download_url":"https://static001.geekbang.org/resource/audio/23/6d/23c2cc1e6c92b16eabcb5720f9baa76d.mp3","had_viewed":false,"article_title":"05 | 栈的魔法：从栈切换的角度理解进程和协程","column_bgcolor":"#F6F7FB","offline_package":"{}","audio_title":"11.1 海纳 05_stack_switch_01.MP3","audio_time_arr":{"m":"20","s":"25","h":"00"},"text_read_percent":0,"cid":450,"article_cshort":"<p>你好，我是海纳。</p><p>上一节课，我们了解到函数在执行的时候，就会在栈上创建栈帧，那么函数执行的上下文都将保存在栈帧里。今天，我们就再来进一步分析，栈切换在计算机系统设计中所发挥的重要作用。</p><p>几乎所有的程序员都会遇到并发程序。因为多进程或者多线程程序可以并发执行，充分利用多CPU多核的计算资源来完成任务，会大大提升应用程序的性能。</p><p>所以，我相信你在工作中也遇到过多线程程序，但不知道你是否考虑过进程和线程是如何切换的呢？很多文章都介绍了，操作系统为了避免频繁进入内核态，会把很多工作都尽量放在用户态。那么你有没有仔细思考过内核态、用户态到底意味着什么呢？</p><p>要回答上面的问题，我们就要理解这些概念背后最重要的一个步骤：对执行单元的上下文环境进行切换。它就是由栈这个核心数据结构支撑的，这也是我们今天学习的重点内容。</p><p>通过今天的学习，你将掌握协程的基本知识，这样，你在C++中使用各种协程库，或者在Lua、Go等语言中使用原生协程的时候，就能理解它们背后发生了什么，也可以帮你写出正确的IO程序。你还将深入理解操作系统用户态和内核态，这样，你在做架构的时候，就能正确评估操作系统进入内核态的开销是多少。</p><p>在讲解执行单元的切换与栈的关系之前，我们先来给出它的准确定义。</p>","video_width":0,"column_could_sub":true,"video_id":"","sku":"100094901","video_cover":"","author_name":"海纳","column_is_onboard":true,"audio_url":"https://res001.geekbang.org/media/audio/23/6d/23c2cc1e6c92b16eabcb5720f9baa76d/ld/ld.m3u8","chapter_id":"2314","column_had_sub":true,"column_cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg","neighbors":{"left":{"article_title":"04 | 深入理解栈：从CPU和函数的视角看栈的管理","id":433530},"right":{"article_title":"06 | 静态链接：变量与内存地址是如何映射的？","id":436308}},"rate_percent":0,"footer_cover_data":{"img_url":"https://static001.geekbang.org/resource/image/f0/fe/f0c47daf3262a8df6cf352d89896abfe.png","link_url":"https://time.geekbang.org/article/427012","mp_url":""},"float_app_qrcode":"","column_is_experience":false,"rate":{"1":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"2":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"3":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0}},"product_id":100094901,"had_liked":false,"id":435493,"free_get":false,"is_video_preview":false,"article_summary":"栈切换的核心就是栈指针rsp寄存器的切换，只要我们想办法把rsp切换了就相当于换了执行单元的上下文环境。","column_sale_type":0,"float_qrcode_jump":"https://time.geekbang.org/serv/v4/misc/jump?uri=https%3A%2F%2Ftime.geekbang.org%2Fhybrid%2Fmp%2Fjump%3Furl%3Dhttps%253A%252F%252Fstatic001.geekbang.org%252Fresource%252Fimage%252Fee%252Fb2%252Feedd4090668aabfac91b2b445d27cbb2.png","column_id":450,"article_sharetitle":"05 | 栈的魔法：从栈切换的角度理解进程和协程","article_poster_wxlite":"https://static001.geekbang.org/render/screen/66/ce/661554f4d8a114a9f07b1a91295adece.jpeg","article_features":0,"comment_count":18,"audio_md5":"23c2cc1e6c92b16eabcb5720f9baa76d","offline":{"size":20865341,"file_name":"62e92de9115aa7a1df1edefa9c53ac50","download_url":"https://static-acl-001.geekbang.org/resource/zip/article/435493/62e92de9115aa7a1df1edefa9c53ac50.zip?auth_key=1641482125-e7da9b3d72f54d928e833206a3b8f7fd-0-42a74a2b2dd4d6188db41ee330933db0"},"video_size":0,"hls_videos":[],"video_time":"","article_could_preview":true,"article_ctime":1635868800,"share":{"content":"学好内存，掌握系统开发诀窍","title":"海纳 · 编程高手必学的内存知识","poster":"https://static001.geekbang.org/resource/image/bc/c5/bcaf3ff69e538f8749970edf2301bbc5.jpg","cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg"}},"436308":{"text_read_version":0,"audio_size":23424171,"article_cover":"https://static001.geekbang.org/resource/image/2d/c5/2da73edb126075656e7db09e470e1dc5.jpg","subtitles":[],"product_type":"c1","audio_dubber":"海纳","is_finished":false,"like":{"had_done":false,"count":6},"audio_time":"00:24:23","video_height":0,"article_content":"<p>你好，我是海纳。</p><p>在第3节课里，我们看到进程的内存空间包含代码段、数据段、bss段、堆和栈等区域。在第4节和第5节课里，我们对栈的相关知识进行了深入学习。今天我们来看看内存中的另一个重要部分：代码段和数据段的组织方式。</p><p>我们知道，编程的代码无非是由函数和各种变量，以及对这些变量的读、写所组成，而不管是变量还是函数，它们最终都要存储在内存里。为每个变量和函数正确地分配内存空间，记录它们的地址，并把这个地址复写回调用或引用它们的地方，这是一个十分重要且困难的任务。</p><p>在我们使用gcc时，往往执行一个命令后，就能得到可执行程序，所以你可能会误以为是编译器负责为变量分配内存地址，但是实际上，这个工作是由链接器来完成的。每个变量和函数都有自己的名称，通常我们把这些名称叫做符号。简单来讲，链接器的作用就是为符号转换成地址，一般来说可以分为三种情况：</p><ol>\n<li>生成二进制可执行文件的过程中。这种情况称为静态链接；</li>\n<li>在二进制文件被加载进内存时。这种情况是在二进制文件保留符号，在加载时再把符号解析成真实的内存地址，这种被称为动态链接；</li>\n<li>在运行期间解析符号。这种情况会把符号的解析延迟到最后不得不做时才去做符号的解析，这也是动态链接的一种。</li>\n</ol><!-- [[[read_end]]] --><p>相信你在工作中，尤其是在编译各种开源项目时，肯定遇到过“找不到符号”，或者“undefined reference to X”这样的报错信息，其实这些错误都和编译链接的过程有关系。所以，接下来的3节课，我们就重点来分析一下链接器是怎么完成内存地址的映射工作的，了解了这个原理后，再遇到类似的问题，你就知道如何着手去分析了。</p><p>今天这节课我们先来探讨静态链接的过程。</p><h3>关于链接的小例子</h3><p>我们先用一个具体的例子展示一遍编译和链接的全部过程，然后再分析每一步的原理。这个例子包含两个文件，第一个文件是example.c：</p><pre><code>// example.c\nextern int extern_var;\nint global_var = 1;\nstatic int static_var = 2;\n \nextern int extern_func();\nint global_func() {\n    return 10;\n}\n \nstatic int static_func() {\n    return 20;\n}\n \nint main() {\n    int var0 = extern_var;\n    int var1 = global_var;\n    int var2 = static_var;\n    int var3 = extern_func();\n    int var4 = global_func();\n    int var5 = static_func();\n    return var0 + var1 + var2 + var3 + var4 + var5;\n}\n</code></pre><p>第二个文件是external.c：</p><pre><code>// external.c\nint extern_var = 3;\nint extern_func() {\n    return 30;\n}\n</code></pre><p>我们先使用gcc将两个c文件分别编译成.o目标文件，这个过程称为编译，命令如下：</p><pre><code># gcc example.c -c -o example.o -fno-PIC -g\n# gcc external.c -c -o external.o -fno-PIC -g\n</code></pre><p>这里我给你解释一下命令中的几个选项：</p><ul>\n<li>-c意思是告诉gcc不要进行链接，只要编译到.o就可以了；</li>\n<li>-o指定了输出文件名;</li>\n<li>-fno-PIC是告诉编译器不要生成PIC的代码。因为我使用的是gcc4.8版本，在编译的过程中默认的模式是PIC模式，由于我们今天讨论的内容主要是静态链接的部分，所以需要打开-fno-PIC选项。这个选项对动态链接的意义比较大，在下节课讲动态链接时，我会对这个选项给你做详细解释。</li>\n<li>-g选项是打开调试信息，让我们在分析过程中能够对源码有更完整的对应关系。</li>\n</ul><p>然后，我们将两个.o文件链接生成可执行文件，由目标文件生成可执行文件的过程就是链接。命令如下：</p><pre><code># gcc external.o example.o -o a.out -no-pie\n</code></pre><p>在这个命令中，-no-pie表示关闭pie的模式。gcc会默认打开pie模式，也就意味着系统loader对加载可执行文件时的起始地址，会随机加载。关闭pie之后，在Linux 64位的系统下，默认的加载起始地址是0x400000。关于这个选项，我们将在下节课详细讲解。</p><p>这样，我们就得到了可执行二进制文件a.out，以上内容就是编译和链接的全过程了。接下来我们详细看一看链接器在这个过程中发挥的作用。</p><h3>链接器的作用</h3><p>我们继续结合上面的例子来说明，这个例子其实涵盖了程序员在开发过程中，最常用的几种变量类型以及函数类型，分别是：</p><ol>\n<li>\n<p>全局变量：global_var。</p>\n</li>\n<li>\n<p>静态变量：static_var。</p>\n</li>\n<li>\n<p>外部变量：extern_var，在example.c中使用extern关键字进行声明，定义在external.c里。</p>\n</li>\n<li>\n<p>局部变量：var0 … var5。</p>\n</li>\n<li>\n<p>全局函数：global_func。</p>\n</li>\n<li>\n<p>静态函数：static_func。</p>\n</li>\n<li>\n<p>外部函数：extern_func，在example.c中使用extern关键字进行声明，定义在external.c里。</p>\n</li>\n</ol><p>程序员在开发代码的过程中，也是直接跟这些符号打交道的。如果想获取某个变量的值，就直接从变量符号里读取内容；如果想调用某个函数，也是直接写一个函数符号的调用语句。</p><p>但是，我们知道，CPU在执行程序代码的时候，并不理解符号的概念，它所能理解的只有内存地址的概念。不管是读数据，调用函数还是读指令，对于CPU而言都是一个个的内存地址。因此，<strong>这里就需要一个连接CPU与程序员之间的桥梁，把程序中的符号转换成CPU执行时的内存地址</strong>。<strong>这个桥梁就是链接器，它负责将符号转换为地址。</strong></p><p><strong>链接器的第一个作用就是把多个中间文件合并成一个可执行文件</strong>。我们在第3节课分析过，每个中间文件都有自己的代码段和数据段等多个section，在合并成一个可执行程序时，多个中间文件的代码段会被合并到可执行文件的代码段，它们数据段也会被合并为可执行文件的数据段。具体的过程可以参考下面这个图：</p><p><img src=\"https://static001.geekbang.org/resource/image/e2/82/e22cc1820a1e9b5e3fc44b844dba7182.jpg?wh=2284x1301\" alt=\"\"></p><p>但是链接器在合并多个目标文件的时候并不是简单地将各个section合并就可以了，它还需要考虑每个目标中的符号的地址。这就引出了<strong>链接器的第二个任务：重定位</strong>。所谓重定位，就是当被调用者的地址变化了，要让调用者知道新的地址是什么。</p><h4>两步链接</h4><p>根据上边的分析，链接器的工作流程也主要分为两步：</p><p><strong>第一步是，链接器需要对编译器生成的多个目标（.o)文件进行合并，一般采取的策略是相似段的合并，最终生成共享文件(.so)或者可执行文件</strong>。这个阶段中，链接器对输入的各个目标文件进行扫描，获取各个段的大小，并且同时会收集所有的符号定义以及引用信息，构建一个全局的符号表。当链接器构造好了最终的文件布局以及虚拟内存布局后，我们根据符号表，也就能确定了每个符号的虚拟地址了。</p><p><strong>第二步是，链接器会对整个文件再进行第二遍扫描，这一阶段，会利用第一遍扫描得到的符号表信息，依次对文件中每个符号引用的地方进行地址替换。也就是对符号的解析以及重定位过程</strong>。</p><p>这就是链接器常用的两步链接(Two-pass linking)的步骤。简单来讲就是进行两遍扫描：第一遍扫描完成文件合并、虚拟内存布局的分配以及符号信息收集；第二遍扫描则是完成了符号的重定位过程。</p><p>重定位是符号解析的重要步骤，是我们理解静态链接和动态链接的基础原理。在JVM或者V8虚拟机中，对符号的解析的原理与链接器的重定位过程是十分相似的，可见重定位应用得非常广泛，所以接下来我们要重点了解一下重定位的原理。</p><h3>深入分析重定位过程</h3><p>工欲善其事，必先利其器，在GNU/linux下，GNU的binutils提供了一系列编程语言的工具程序，用来查看不同格式下的目标文件。今天我要给你重点介绍两个工具：readelf和objdump，这两个工具可以用来解析和读取上一节编译阶段生成的目标文件信息。</p><p>一般情况下，我在对二进制文件进行反汇编时会使用objdump工具，因为readelf工具没有提供反汇编的能力，它更多是用来解析二进制文件信息。</p><p>在前面的例子中，我们已经编译出两个.o目标文件，以及最终链接后的a.out可执行文件，接下来我们通过对比.o文件以及a.out文件中符号的差异来分析重定位的过程。</p><p>首先，我们通过objdump看一下此时目标文件里的反汇编是什么样子的。</p><pre><code># objdump -S example.o\n0000000000000000 &lt;global_func&gt;:\n   0:   55                      push   %rbp\n   1:   48 89 e5                mov    %rsp,%rbp\n   4:   b8 0a 00 00 00          mov    $0xa,%eax\n   9:   5d                      pop    %rbp\n   a:   c3                      retq\n \n000000000000000b &lt;static_func&gt;:\n   b:   55                      push   %rbp\n   c:   48 89 e5                mov    %rsp,%rbp\n   f:   b8 14 00 00 00          mov    $0x14,%eax\n  14:   5d                      pop    %rbp\n  15:   c3                      retq\n \n0000000000000016 &lt;main&gt;:\nint main() {\n  16:   55                      push   %rbp\n  17:   48 89 e5                mov    %rsp,%rbp\n  1a:   48 83 ec 20             sub    $0x20,%rsp\n    int var0 = extern_var;\n  1e:   8b 05 00 00 00 00       mov    0x0(%rip),%eax        # 24 &lt;main+0xe&gt;\n  24:   89 45 e8                mov    %eax,-0x18(%rbp)\n    int var1 = global_var;\n  27:   8b 05 00 00 00 00       mov    0x0(%rip),%eax        # 2d &lt;main+0x17&gt;\n  2d:   89 45 ec                mov    %eax,-0x14(%rbp)\n    int var2 = static_var;\n  30:   8b 05 00 00 00 00       mov    0x0(%rip),%eax        # 36 &lt;main+0x20&gt;\n  36:   89 45 f0                mov    %eax,-0x10(%rbp)\n    int var3 = extern_func();\n  39:   b8 00 00 00 00          mov    $0x0,%eax\n  3e:   e8 00 00 00 00          callq  43 &lt;main+0x2d&gt;\n  43:   89 45 f4                mov    %eax,-0xc(%rbp)\n    int var4 = global_func();\n  46:   b8 00 00 00 00          mov    $0x0,%eax\n  4b:   e8 00 00 00 00          callq  50 &lt;main+0x3a&gt;\n  50:   89 45 f8                mov    %eax,-0x8(%rbp)\n    int var5 = static_func();\n  53:   b8 00 00 00 00          mov    $0x0,%eax\n  58:   e8 ae ff ff ff          callq  b &lt;static_func&gt;\n  5d:   89 45 fc                mov    %eax,-0x4(%rbp)\n  …\n} \n</code></pre><p>由于空间的限制，我只保留了main函数中源码与汇编码对应的部分内容。你需要注意的是，上边源码与汇编的对应，需要在编译.o文件时打开-g选项，否则就只有汇编代码。</p><p>下面我们来分类讨论各种符号的处理方式。</p><h4>各种符号的处理方式</h4><p><strong>首先，我来看看局部变量的处理过程</strong>。从反汇编的结果里，我们可以看到，局部变量在程序中的地址，都是基于%rbp的偏移这种形式，rbp寄存器存放的是当前函数栈帧的基地址。这些局部变量的内存分配与释放，都是在运行时通过%rbp的改变来进行的，因此，局部变量的内存地址不需要链接器来操心。</p><p><strong>然后，再来看看比较简单的static_func，它是唯一不需要重定位的类型</strong>。对static_func的调用，所生成的指令的二进制是 e8 ae ff ff ff。其中，e8是callq指令的编码，后边4个字节就对应被调函数的地址。注意，这里生成的ae ff ff ff，如果采用小端的字节序数值来表示，应该是0xffffffae，也就是对应十进制的-82。</p><p>此时，当CPU执行到callq这条指令时，rip寄存器的值指向的是下一条指令的内存地址，也就是5d这条指令的内存地址，通过计算0x5d – 82可以得到0xb。从反汇编中可以得到，0xb刚好是static_func的地址。static_func的链接原理，你可以参考下面这幅图：</p><p><img src=\"https://static001.geekbang.org/resource/image/ae/3f/ae5b19734d342453826b41538d2d603f.jpg?wh=2284x1304\" alt=\"\"></p><p>从上图中可以看出，同一个编译单元内部，static_func与main函数的相对位置是固定不变的，即便链接的过程中会对不同.o文件中的代码段进行合并，但是同一个.o文件内部不同函数之间的位置也会保持不变，因此，我们在编译的时候，就能确定对静态函数调用的偏移。也就是说，静态函数的调用地址在编译阶段就可以确定下来。</p><p>我们可以在最终生成的可执行文件的main函数中，查看对应位置代码的反汇编。可以验证的是，这里确实没有进行重定位的修正：</p><pre><code>0000000004004ad &lt;main&gt;:\n  ...\n  4004ef:       e8 ae ff ff ff          callq  4004a2 &lt;static_func&gt;\n  ... \n</code></pre><p><strong>接下来，我们再看第三类符号，也就是外部变量、全局变量以及静态变量的处理过程</strong>。你可以从反汇编结果中看到，前三条语句对extern_var、global_var和static_var的访问，都生成了一条 mov 0x0(%rip)，%eax的指令。这是因为在这个时候，编译器还无法确定这三个变量的地址，因此，这里先通过0来进行占位，以后链接器会将真正的地址回填在这里。</p><p><strong>最后，我们来看对于extern_func和global_func的调用</strong>，call指令同样是通过0来进行占位，这和全局变量的处理方式一样。</p><h4>处理占位符</h4><p>我们前面说到，在无法确定变量的真实地址时，先通过0来进行占位。所以，我们这里继续观察链接器对extern_var，static_var，global_var，global_func以及extern_func的重定位过程，看看它们的占位符是如何处理的。</p><p>这里我们需要通过readelf工具来查看一下目标文件里有哪些信息：</p><pre><code># readelf -S example.o\nThere are 12 section headers, starting at offset 0x478:\n \nSection Headers:\n  [Nr] Name              Type             Address           Offset\n       Size              EntSize          Flags  Link  Info  Align\n  [ 0]                   NULL             0000000000000000  00000000\n       0000000000000000  0000000000000000           0     0     0\n  [ 1] .text             PROGBITS         0000000000000000  00000040\n       000000000000007e  0000000000000000  AX       0     0     1\n  [ 2] .rela.text        RELA             0000000000000000  00000358\n       0000000000000078  0000000000000018   I       9     1     8\n  [ 3] .data             PROGBITS         0000000000000000  000000c0\n       0000000000000004  0000000000000000  WA       0     0     4\n  [ 4] .bss              NOBITS           0000000000000000  000000c4\n       0000000000000004  0000000000000000  WA       0     0     4\n  [ 5] .comment          PROGBITS         0000000000000000  000000c4\n       000000000000002a  0000000000000001  MS       0     0     1\n  [ 6] .note.GNU-stack   PROGBITS         0000000000000000  000000ee\n       0000000000000000  0000000000000000           0     0     1\n  [ 7] .eh_frame         PROGBITS         0000000000000000  000000f0\n       0000000000000078  0000000000000000   A       0     0     8\n  [ 8] .rela.eh_frame    RELA             0000000000000000  000003d0\n       0000000000000048  0000000000000018   I       9     7     8\n  [ 9] .symtab           SYMTAB           0000000000000000  00000168\n       0000000000000180  0000000000000018          10    10     8\n  [10] .strtab           STRTAB           0000000000000000  000002e8\n       000000000000006b  0000000000000000           0     0     1\n  [11] .shstrtab         STRTAB           0000000000000000  00000418\n       0000000000000059  0000000000000000           0     0     1\n</code></pre><p>其中的readelf -S选项是打印出二进制文件中所有section-header的信息。我们可以看到example.o里总共包含了12个section，其中，.text段、.data段和.bss段我在前面的课程里都提到过，这里我们重点看下.rela.text段。</p><p>从section-header的信息里可以看到，.rela.text段的类型是RELA类型，也就是重定位表。我们在前面讲到，链接器在处理目标文件的时候，需要对目标文件里代码段和数据段引用到的符号进行重定位，而这些重定位的信息都记录在对应的重定位表里。</p><p>一般来说，重定位表的名字都是以.rela开头，比如.rela.text就是对.text段的重定位表，.rela.data是对.data段的重定位表。因为我们的例子中并没有涉及.data段的重定位，所以，在上面打印的信息中没有出现.rela.data段。</p><p>好了，接下来我们具体看一下.rela.text重定位表里的内容。你可以通过readelf -r选项来打印二进制文件中的重定位表信息，输出如下：</p><pre><code>Relocation section '.rela.text' at offset 0x330 contains 5 entries:\n  Offset          Info           Type           Sym. Value    Sym. Name + Addend\n000000000020  000d00000002 R_X86_64_PC32     0000000000000000 extern_var - 4\n000000000029  000a00000002 R_X86_64_PC32     0000000000000000 global_var - 4\n000000000032  000300000002 R_X86_64_PC32     0000000000000000 .data + 0\n00000000003f  000e00000002 R_X86_64_PC32     0000000000000000 extern_func - 4\n00000000004c  000b00000002 R_X86_64_PC32     0000000000000000 global_func - 4\n</code></pre><p>.rela.text的重定位表里存放了text段中需要进行重定位的每一处信息。所以，每个重定位项都会包含需要重定位的偏移、重定位类型和重定位符号。重定位表的数据结构是这样的：</p><pre><code>  typedef struct {\n    Elf64_Addr›   r_offset; /* 重定位表项的偏移地址 */\n    Elf64_Xword›  r_info;   /* 重定位的类型以及重定位符号的索引 */\n    Elf64_Sxword› r_addend; /* 重定位过程中需要的辅助信息 */\n  } Elf64_Rela;\n</code></pre><p>其中，r_info的高32bit存放的是重定位符号在符号表的索引，r_info的低32bit存放的是重定位的类型的索引。符号表就是.symtab段，可以把它看成是一个字典，这个字典以整数为key，以符号名为value。</p><p>在我们的例子中，根据上文的汇编代码来看，.rela.text段中的重定位表总共有5项，分别对应到.text的0x20, 0x29, 0x32, 0x3f, 0x4c偏移处。我们以0x20为例，它对应的汇编指令是0x1e位置的8b 05 00 00 00 00。0x20指向的是这条指令的操作数，在没有重定位之前，它是一个四字节填充的0，对应的是对变量extern_var的访问。</p><p>同样地，其余的几处偏移位置分别是访问global_var、static_var、global_func和extern_func这四个符号（函数和变量都可统一看成是符号）的地方。</p><p>接下来，我们着重分析这四个符号的重定位过程。我们可以看到重定位表中的这四项，<strong>它们的类型都是R_X86_64_PC32。这种类型的重定位计算方式为：S + A – P</strong>。</p><ul>\n<li>\n<p>这里的S表示完成链接后该符号的实际地址。在链接器将多个中间文件的段合并以后，每个符号就按先后顺序依次都会分配到一个地址，这就是它的最终地址S。</p>\n</li>\n<li>\n<p>A表示Addend的值，它代表了占位符的长度。它的具体用法我们下文还会详细分析。</p>\n</li>\n<li>\n<p>P表示要进行重定位位置的地址或偏移，可以通过r_offset的值获取到，这是引用符号的地方，也就是我们要回填地址的地方，简单说，它就是我们上文提到的用0填充的占位符的地址。</p>\n</li>\n</ul><p>这里S与P所表示的地址都是文件合并之后最终的虚拟地址，由于我们无法获取链接器中间过程的文件，所以，我们需要通过查看链接完成后的可执行文件，来寻找这两个地址。</p><p>我们以extern_var的变量为例，具体跟踪一遍重定位的过程。</p><pre><code>00000000004004ad &lt;main&gt;:\n  4004ad:       55                      push   %rbp\n  4004ae:       48 89 e5                mov    %rsp,%rbp\n  4004b1:       48 83 ec 20             sub    $0x20,%rsp\n  4004b5:       8b 05 75 0b 20 00       mov    0x200b75(%rip),%eax        # 601030 &lt;extern_var&gt;\n  4004bb:       89 45 e8                mov    %eax,-0x18(%rbp)\n</code></pre><p>上边输出部分是对生成可执行文件的反汇编。根据S、A、P的定义，我们知道对于extern_var来讲：</p><ul>\n<li>S是其最终符号的真实地址，如上汇编里边的注释所示  也就是上面注释的0x601030这个地址；</li>\n<li>A是Addend的值，可以从重定位表里查到是-4，对于A的具体含义我还会进一步解释；</li>\n<li>P是重定位offset的地址，这里是0x4004b7。</li>\n</ul><p>根据公式，我们算出重定位处需要填写的值应该是 0x601030 + (-4) – 0x4004b7 = 0x200b75，也就是最终可执行文件中这条mov指令里的值。</p><p>到目前为止，我们从链接器的视角推出了最终重定位位置的值，你可能会比较迷糊：系统为什么搞这么一套复杂的公式来计算出这么一个值呢？这个值的真正含义是什么？</p><p>针对这个问题，我们再从CPU的角度来看下这里的取值关系。从上面main函数的反编译的结果可以看到，我们最终对extern_var的访问生成的汇编是：</p><pre><code>mov    0x200b75(%rip), %eax\n</code></pre><p>这是一条PC相对偏移的寻址方式。当CPU执行到这条指令的时候，%rip的值存放的是下一条指令的地址，也就是0x4004bb。这时候可以算出0x4004bb + 0x200b75 = 0x601030，刚好是extern_var的实际地址。</p><p>经过正面分析这个重定位的值的作用后，这里我们再来理解一下S+A-P这个公式的作用。链接器有了整体的虚拟内存布局后，知道的信息是：需要重定位符号的地址S的值是(0x601030)，以及需要重定位的位置地址P的值是(0x4004b7)。</p><p>这时候，链接器需要在指令中占位符的位置填一个值，让程序运行的时候能够找到S。但程序运行到这条指令的时候，能够拿到的地址就只有PC的值，也就是下一条指令的地址(0x4004bb)。你会发现，重定位地址的值跟下一条pc的值，相差的就是这个Addend(-4)，这个Addend实际上就是用来调整P的值和执行时PC的值之间的差异的，所以它刚好就是占位符的宽度。</p><h4>静态变量</h4><p>除了上述所讲的四个符号之外，还有一个比较特殊的是static_var变量。我们可以从Sym. Name里找到其余变量的符号，但static_var的符号没有出现，只有一个.data的符号。</p><p>这是因为static_var变量本身是一个静态变量，只在本编译单元内可见，不会对外进行暴露，所以它是根据本编译单元的.data段的地址来进行重定位。也就是说，<strong>static_var的最终地址就是本编译单元的.data段的最终地址</strong>。所以，它的重定义方法与extern_var等符号的重定位方法是一样的，区别仅仅在于它的符号被隐藏了。如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/2e/be/2e29c9c26a715015ce08d3946dee6fbe.jpg?wh=2284x1032\" alt=\"\"></p><p>你可能会有疑问，既然静态函数可以在编译的时候确定相对偏移，那为什么静态变量做不到这一点呢？</p><p>这是因为静态变量的位置是在data段，而对静态函数的访问是在text段。对应text段内部的偏移可以保证在链接的过程中不发生改变，但由于text段和data段分属不同的段，在链接的时候大概率会进行重新排布，所以它和引用它的地方之间的相对位置就发生变化了。所以静态变量的地址就需要链接器来进行重定位了。</p><p>好，到这里我们就对整个重定位的过程有了清晰的了解。</p><h3>总结</h3><p>我们今天讲解了在静态链接过程中，变量与内存地址是如何对应起来的。其中，链接器的重定位操作是这个过程中的核心步骤。</p><p>我们说，从源文件生成二进制可执行文件，这一过程主要包含了编译和链接两个步骤。其中，编译的作用是生成性能优越的机器码。对于编译单元内部的静态函数，可以在编译时通过相对地址的办法，生成call指令，因为无论将来调用者和被调用者被安置到什么地方，它们之间的相对距离不会发生变化。</p><p>而其他类型的变量和函数在编译时，编译器并不知道它们的最终地址，所以只能使用占位符（比如0）来临时代替目标地址。</p><p>而链接器的任务是为所有变量和函数分配地址，并把被分配到的地址回写到调用者处。链接的过程主要分为两步，第一步是多文件合并，同时为符号分配地址，第二步则是将符号的地址回写到引用它的地方。其中，地址回写有一个专门的名字叫做重定位。重定位的过程依赖目标文件中的重定位表。</p><p>到这里，我们已经对例子中的几种不同类型符号的静态链接有了一个清晰的认识。下面的课程我会继续讲解loader和动态链接的过程。</p><h3>思考题</h3><p>经过了这节课的学习，我们深刻地理解了全局变量，静态变量的用法。请你思考一下，全局变量和静态变量的初值是在哪个阶段决定的？更具体地说，是编译、链接、加载和运行这四个阶段中的哪一个阶段决定的？或者你也可以进行这样的思考，我们在目标文件、可执行文件和运行时内存里，能不能观察到全局变量和静态变量的初始值？欢迎你在留言区和我交流你的想法，我在留言区等你。</p><p><img src=\"https://static001.geekbang.org/resource/image/78/3a/78039e07de9e869b6e966da9a61e963a.jpg?wh=2284x1523\" alt=\"\"><br>\n好啦，这节课到这就结束啦。欢迎你把这节课分享给更多对计算机内存感兴趣的朋友。我是海纳，我们下节课再见！</p>","float_qrcode":"https://static001.geekbang.org/resource/image/83/1c/83e244537b714e899fd2e7896e22371c.png","article_cover_hidden":false,"is_required":true,"score":"31636041600","like_count":18,"article_subtitle":"","audio_download_url":"https://static001.geekbang.org/resource/audio/85/02/85cc477f07cdbc89c00561d98a64cc02.mp3","had_viewed":false,"article_title":"06 | 静态链接：变量与内存地址是如何映射的？","column_bgcolor":"#F6F7FB","offline_package":"{}","audio_title":"11.1 海纳 06_static_linker_01.MP3","audio_time_arr":{"m":"24","s":"23","h":"00"},"text_read_percent":0,"cid":450,"article_cshort":"<p>你好，我是海纳。</p><p>在第3节课里，我们看到进程的内存空间包含代码段、数据段、bss段、堆和栈等区域。在第4节和第5节课里，我们对栈的相关知识进行了深入学习。今天我们来看看内存中的另一个重要部分：代码段和数据段的组织方式。</p><p>我们知道，编程的代码无非是由函数和各种变量，以及对这些变量的读、写所组成，而不管是变量还是函数，它们最终都要存储在内存里。为每个变量和函数正确地分配内存空间，记录它们的地址，并把这个地址复写回调用或引用它们的地方，这是一个十分重要且困难的任务。</p><p>在我们使用gcc时，往往执行一个命令后，就能得到可执行程序，所以你可能会误以为是编译器负责为变量分配内存地址，但是实际上，这个工作是由链接器来完成的。每个变量和函数都有自己的名称，通常我们把这些名称叫做符号。简单来讲，链接器的作用就是为符号转换成地址，一般来说可以分为三种情况：</p><ol>\n<li>生成二进制可执行文件的过程中。这种情况称为静态链接；</li>\n<li>在二进制文件被加载进内存时。这种情况是在二进制文件保留符号，在加载时再把符号解析成真实的内存地址，这种被称为动态链接；</li>\n<li>在运行期间解析符号。这种情况会把符号的解析延迟到最后不得不做时才去做符号的解析，这也是动态链接的一种。</li>\n</ol>","video_width":0,"column_could_sub":true,"video_id":"","sku":"100094901","video_cover":"","author_name":"海纳","column_is_onboard":true,"audio_url":"https://res001.geekbang.org/media/audio/85/02/85cc477f07cdbc89c00561d98a64cc02/ld/ld.m3u8","chapter_id":"2314","column_had_sub":true,"column_cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg","neighbors":{"left":{"article_title":"05 | 栈的魔法：从栈切换的角度理解进程和协程","id":435493},"right":{"article_title":"07 | 动态链接（上）：地址无关代码是如何生成的？","id":437653}},"rate_percent":0,"footer_cover_data":{"img_url":"https://static001.geekbang.org/resource/image/f0/fe/f0c47daf3262a8df6cf352d89896abfe.png","link_url":"https://time.geekbang.org/article/427012","mp_url":""},"float_app_qrcode":"","column_is_experience":false,"rate":{"1":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"2":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"3":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0}},"product_id":100094901,"had_liked":false,"id":436308,"free_get":false,"is_video_preview":false,"article_summary":"链接器需要对编译器生成的多个目标（.o)文件进行合并，一般采取的策略是相似段的合并，最终生成共享文件(.so)或者可执行文件。","column_sale_type":0,"float_qrcode_jump":"https://time.geekbang.org/serv/v4/misc/jump?uri=https%3A%2F%2Ftime.geekbang.org%2Fhybrid%2Fmp%2Fjump%3Furl%3Dhttps%253A%252F%252Fstatic001.geekbang.org%252Fresource%252Fimage%252Fee%252Fb2%252Feedd4090668aabfac91b2b445d27cbb2.png","column_id":450,"article_sharetitle":"06 | 链接：变量是如何与内存地址对应起来的？","article_poster_wxlite":"https://static001.geekbang.org/render/screen/e8/cd/e8b94cd7ce10934c2849a08cc5de95cd.jpeg","article_features":0,"comment_count":19,"audio_md5":"85cc477f07cdbc89c00561d98a64cc02","offline":{"size":24540240,"file_name":"68fd3e9ca6078d084de228164862830b","download_url":"https://static-acl-001.geekbang.org/resource/zip/article/436308/68fd3e9ca6078d084de228164862830b.zip?auth_key=1641482141-829d965b50e84177888619d0d4fdbf89-0-6cebaeea688f09b207812bbf24f7b915"},"video_size":0,"hls_videos":[],"video_time":"","article_could_preview":true,"article_ctime":1636041600,"share":{"content":"学好内存，掌握系统开发诀窍","title":"海纳 · 编程高手必学的内存知识","poster":"https://static001.geekbang.org/resource/image/bc/c5/bcaf3ff69e538f8749970edf2301bbc5.jpg","cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg"}},"437653":{"text_read_version":0,"audio_size":22198776,"article_cover":"https://static001.geekbang.org/resource/image/2b/41/2b9e0809255a839cb228869368766241.jpg","subtitles":[],"product_type":"c1","audio_dubber":"海纳","is_finished":false,"like":{"had_done":false,"count":4},"audio_time":"00:23:07","video_height":0,"article_content":"<p>你好，我是海纳。</p><p>通过上节课的学习，我们了解到，链接器可以将不同的编译单元所生成的中间文件组合在一起，并且可以为各个编译单元中的变量和函数分配地址，然后将分配好的地址传给引用者。这个过程就是静态链接。</p><p>静态链接可以让开发者进行模块化的开发，大大的促进了程序开发的效率。但同时静态链接仍然存在一个比较大的问题，就是无法共享。例如程序A与程序B都需要调用函数foo，在采用静态链接的情况下，只能分别将foo函数链接到A的二进制文件和B的二进制文件中，这样导致系统同时运行A和B两个进程的时候，内存中会装载两份foo的代码。那么如何消除这种浪费呢，这就是我们接下来两节课的主题：动态链接。</p><p>动态链接的重定位发生在加载期间或者运行期间，这节课我们将重点分析加载期间的重定位，它的实现依赖于地址无关代码。我们知道，深入地掌握动态链接库是开发底层基础设施必备的技能之一，如果你想要透彻地理解动态链接机制，就必须掌握地址无关代码技术。</p><p>在你掌握了地址无关代码技术后，你还将对程序员眼中的“风骚”操作，比如，如何通过重载动态库对系统进行热更新，如何对动态库里的函数进行hook操作，以便于调试和追踪问题等等，都会有更深入的理解。</p><!-- [[[read_end]]] --><p>我们先来一起看一下，动态链接是怎样解决静态链接不能充分共享代码这个问题的。</p><h2>什么是动态链接</h2><p>要想解决静态链接的问题，可以把共享的部分抽离出来，组成新的模块。为了让一些公共的库函数能够被多个程序，在运行的过程中进行共享，我们可以让程序在链接和运行过程中，也拆分成不同的模块，即共享模块和私有模块。共享模块用来存放供所有进程公共使用的库函数，私有模块存放本进程独享的函数与数据。</p><p>分析到这里，动态链接的基本思路就呼之欲出了。目前解决共享问题，采用的通用的思路是，将常用的公共的函数都放到一个文件中，在整个系统里只会被加载到内存中一次，无论有多少个进程使用它，这个文件在内存中只有一个副本，这种文件就是动态链接库文件。</p><p>它在Linux里是共享目标文件(share object, so)，在windows下是动态链接库文件(dynamic linking library, dll)。当然，以上只是一个最基本的想法，要想真正实现动态链接的技术还有很多问题需要考虑。接下来，我们来看最主要的两个问题。</p><p><strong>第一个问题是</strong>，由于公共库函数的代码要在多个不同的进程中进行共享，也就是说，不同的进程运行的库的代码是同一份，这就要求共享模块的代码必须是地址无关的，因为每个进程都有自己独立的内存空间，系统loader无法保证共享模块加载的内存地址，对于每个进程而言都是相同的地址。</p><p>例如进程A加载的libfoo.so的起始地址可能是0x1000，而进程B加载的libfoo.so的起始地址可能是0x3000，如果libfoo.so里代码访问的函数或者数据是绝对地址的话，那必然会造成进程A与B的冲突。</p><p><strong>第二个问题是</strong>，我们知道，虽然在开发的过程中，开发者可以将程序模块化处理，但还是需要静态链接来将不同模块链接到一起，对符号进行重定位，这样运行时CPU才能知道各个函数、变量的真正地址是什么。</p><p>同样的，要想让程序在运行过程中也进行模块化，那就意味着，不同模块之间符号的链接过程，需要推迟到加载时进行了，这也是动态链接(Dynamic Linking)技术名字的由来。</p><p>在讲解动态链接的具体实现之前，我们还是先来看下动态链接的小例子，来对动态链接有一个初步的印象。</p><h2>如何生成和使用动态链接库</h2><p>我们通过运行一个例子来展示动态链接和加载的完整过程：</p><pre><code>// foo.h\n#ifndef _FOO_H_\n#define _FOO_H_\n \nvoid foo();\n \n#endif\n \n// foo.c\n#include &lt;stdio.h&gt;\n#include &quot;foo.h&quot;\nvoid foo() {\n    printf(&quot;Hello foo\\n&quot;);\n}\n \n// main_a.c\n#include &lt;stdio.h&gt;\n#include &quot;foo.h&quot;\n \nint main() {\n    printf(&quot;A.exe: &quot;);\n    foo();\n    while(1) {\n    }\n}\n \n// main_b.c\n#include &lt;stdio.h&gt;\n#include &quot;foo.h&quot;\n \nint main() {\n    printf(&quot;B.exe: &quot;);\n    foo();\n    while(1) {\n    }\n}\n</code></pre><p>以上例子分了三个模块，分别是共享模块的foo.c，两个主程序main_a.c和main_b.c，主程序都调用了foo.c中的foo方法，最后放一个死循环用来保证程序不退出，以便于查看进程的相关信息。</p><p>我们先把foo.c编译成libfoo.so：</p><pre><code>$ gcc foo.c -fPIC -shared -o libfoo.so\n</code></pre><p>其中-fPIC目的是开启地址无关代码，一会儿我会给你详细解释；-shared意思是告诉链接器生成的目标文件是共享目标文件。</p><p>然后我们分别编译main_a.c 和main_b.c，来生成可执行文件A.exe和B.exe：</p><pre><code>$ gcc main_a.c -L. -lfoo -no-pie -o A.exe\n$ gcc main_b.c -L. -lfoo -no-pie -o B.exe\n</code></pre><p>我先来解释下，这块代码中几个选项的意思。</p><ul>\n<li>-L指定了查找链接库的路径(或者可以通过设置环境变量LIBRARY_PATH来追加路径)。-L. 就是告诉链接器需要到当前目录下查找共享文件。</li>\n<li>-l则指定了具体链接库的名称，需要注意的是，gcc在处理链接库名称时，会自动加上lib的前缀和.so的后缀，所以，gcc命令选项写的-lfoo，就是告诉链接器查找libfoo.so这个共享目标文件。</li>\n<li>-no-pie是禁止生成地址无关的可执行文件，方便我们查看进程的内存布局。</li>\n</ul><p>此时我们执行“ldd A.exe”或者“ldd B.exe”的时候，就可以看到两个可执行文件依赖的so中多了一个libfoo.so:</p><pre><code>$ ldd A.exe\n        linux-vdso.so.1 (0x00007ffebc5ed000)\n        libfoo.so =&gt; not found\n        libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f07e5ffe000)\n        /lib64/ld-linux-x86-64.so.2 (0x00007f07e65f1000)\n</code></pre><p>我要提醒你的是，上面的命令在输出过程中，libfoo.so的指向是not found。这是因为libfoo.so所在的路径是当前路径，运行时查找共享库的时候默认并不会来找寻当前路径，因此libfoo.so的指向目前是无法确认的。如果此时执行./A.exe同样也会报错，解决方法就是将当前路径设置到LD_LIBRARY_PATH的环境变量中。</p><pre><code>$ export LD_LIBRARY_PATH=.:$LD_LIBRARY_PATH\n</code></pre><p>此时再执行<code>ldd A.exe</code>就能找到libfoo.so的位置了。运行结果如下所示：</p><pre><code>$ ldd A.exe\n        linux-vdso.so.1 (0x00007ffef1cba000)\n        libfoo.so =&gt; ./libfoo.so (0x00007fe9d6998000)\n        libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007fe9d65a7000)\n        /lib64/ld-linux-x86-64.so.2 (0x00007fe9d6d9c000)\n</code></pre><p>在上面的例子中，我们提到了两个环境变量LIBRARY_PATH和LD_LIBRARY_PATH，你可能对这两个环境变量的作用不是很清楚，或者容易混淆，在这里我们再对这两个变量的作用进行一下对比区分。这两个环境变量都是用来设置库文件的查找路径的，只不过使用的时机不一样。</p><p>其中LIBRARY_PATH是由链接器来使用的，一般系统默认是gnu ld。对于大部分开发者来讲，如果LIBRARY_PATH没有设置好，在使用gcc或者clang这些编译器（其实它们都调用了ld这个链接器，真正做事情的是ld）的时候，会碰到类似<code>/usr/bin/ld: cannot find -lfoo</code>的错误。LIBRARY_PATH的一个等价的选项就是上文讲的-L指定路径的选项。</p><p>而另一个LD_LIBRARY_PATH的环境变量是由动态链接器来使用的，即我们通过ldd看到的ld-linux-x86-64.so.2这个库。动态链接器的知识我们会在下一节课中详细展开。目前这里，我们只需要知道这个动态链接器是在程序加载运行时执行的就可以了。</p><p>因此，如果LD_LIBRARY_PATH没有设置好的话，会碰到类似<code>./A.exe: error while loading shared libraries: libfoo.so: cannot open shared object file: No such file or directory</code>的问题。</p><p>总结下来，LIBRARY_PATH的使用时机是链接器在做链接的时候，LD_LIBRARY_PATH的使用时机是在程序运行时。</p><p>接下来我们再看一下可执行程序运行起来以后，它的内存布局是什么样子的，这样我们就能清楚动态链接技术是怎么节省内存的。</p><h2>动态链接库内存布局</h2><p>我们通过执行两个进程，一起看下它们的内存布局：</p><pre><code>$ ./A.exe &amp;\n$ ./B.exe &amp;\n$ cat /proc/`pidof A.exe`/maps\n00400000-00401000 r-xp 00000000 08:10 747270         ./A.exe\n00600000-00601000 r--p 00000000 08:10 747270         ./A.exe\n00601000-00602000 rw-p 00001000 08:10 747270         ./A.exe\n01e58000-01e79000 rw-p 00000000 00:00 0              [heap]\n…\n7fb25b13d000-7fb25b141000 rw-p 00000000 00:00 0\n7fb25b141000-7fb25b142000 r-xp 00000000 08:10 747268 ./libfoo.so\n7fb25b142000-7fb25b341000 ---p 00001000 08:10 747268 ./libfoo.so\n7fb25b341000-7fb25b342000 r--p 00000000 08:10 747268 ./libfoo.so\n7fb25b342000-7fb25b343000 rw-p 00001000 08:10 747268 ./libfoo.so\n…\n7ffed501b000-7ffed503c000 rw-p 00000000 00:00 0      [stack]\n7ffed51bc000-7ffed51c0000 r--p 00000000 00:00 0      [vvar]\n7ffed51c0000-7ffed51c1000 r-xp 00000000 00:00 0      [vdso]\n \n$ cat /proc/`pidof B.exe`/maps\n00400000-00401000 r-xp 00000000 08:10 747269         ./B.exe\n00600000-00601000 r--p 00000000 08:10 747269         ./B.exe\n00601000-00602000 rw-p 00001000 08:10 747269         ./B.exe\n01597000-015b8000 rw-p 00000000 00:00 0              [heap]\n…\n7f2991e85000-7f2991e89000 rw-p 00000000 00:00 0\n7f2991e89000-7f2991e8a000 r-xp 00000000 08:10 747268 ./libfoo.so\n7f2991e8a000-7f2992089000 ---p 00001000 08:10 747268 ./libfoo.so\n7f2992089000-7f299208a000 r--p 00000000 08:10 747268 ./libfoo.so\n7f299208a000-7f299208b000 rw-p 00001000 08:10 747268 ./libfoo.so\n…\n7f29922b6000-7f29922b7000 rw-p 00000000 00:00 0\n7fff73f9e000-7fff73fbf000 rw-p 00000000 00:00 0      [stack]\n7fff73fde000-7fff73fe2000 r--p 00000000 00:00 0      [vvar]\n7fff73fe2000-7fff73fe3000 r-xp 00000000 00:00 0      [vdso]\n</code></pre><p>从上面的命令运行结果中，我们可以观察到这样两个特点：</p><p>第一个特点是，动态库的数据段和代码段是靠在一起的，它并没有和可执行程序的数据段，代码段分别合并，这是与静态链接不同的地方。</p><p>第二个特点是，同一个动态库文件在两个进程中的虚拟地址并不相同，A.exe跟B.exe同时加载了libfoo.so，但所处的位置分别是0x7fb25b141000与0x7f2991e89000，并不相同。</p><p>然后我们再看一下两个进程中libfoo.so代码段的物理内存占用情况，先看A进程的：</p><pre><code>$ cat /proc/`pidof A.exe`/smaps\n7fb25b141000-7fb25b142000 r-xp 00000000 08:10 747268 ./libfoo.so\nSize:                  4 kB\nKernelPageSize:        4 kB\nMMUPageSize:           4 kB\nRss:                   4 kB\nPss:                   2 kB\n......\n</code></pre><p>再看看B进程的：</p><pre><code>$ cat /proc/`pidof B.exe`/smaps\n7f2991e89000-7f2991e8a000 r-xp 00000000 08:10 747268 ./libfoo.so\nSize:                  4 kB\nKernelPageSize:        4 kB\nMMUPageSize:           4 kB\nRss:                   4 kB\nPss:                   2 kB\n.......\n</code></pre><p>我们通过smap的结果来考察物理内存的实际占用情况。在<a href=\"https://time.geekbang.org/column/article/430073\">第1节课</a>的练习中，我曾经让你自己动手研究smap各字段的含义，今天我们来重点分析一下Rss与Pss。Rss的含义是当前段实际加载到物理内存中的大小，Pss指的是进程按比例分配当前段所占物理内存的大小。</p><p>在这个例子中，因为libfoo.so本身代码段不足4K，但是物理页的单位是4K，所以这里libfoo.so代码段本身需要占据一个物理页，也就是4K的大小，即Rss值为4K。</p><p>由于多个进程共享了动态库，所以Pss的计算方式应该是Rss值除以共享进程数。从上面例子可以看到，A.exe与B.exe共享了libfoo.so的代码段，按比例分配的话应该分别占用2K，即Pss的值都是2K。如果此时我们把B.exe进程终止掉，你会发现A.exe这里的Pss值就会变成4K。命令的输出还包含其他字段，如果你感兴趣的话，可以通过<code>man proc</code>命令来查询proc的详细信息。</p><p>通过这个小例子，我们看到了动态链接技术确实是将共享部分的内存省了下来，但是你也会发现，库文件在不同进程的映射中，虚拟内存地址可以不同。这就要求编译器在生成代码时能适应这个需求，那么地址无关代码技术就诞生了。</p><h2>为什么会有地址无关代码？</h2><p>首先，我们思考一下，动态库文件被加载到内存中并且被多个进程共享时，它的内存是什么样子的。</p><p>在<a href=\"https://time.geekbang.org/column/article/431904\">第3节课</a>我们已经看到了，可执行文件或者动态库文件被加载进内存的时候，文件中不同的section会被加载进内存中不同segment，比如.data和.bss段被加载进数据段(data segment)，而.code, .rodata被加载进代码段(code segment)。</p><p>在多进程共享动态库的时候，因为代码段是不可写的，所以进程间共享不存在问题，而数据段可写，系统必须保证一个进程写了共享库的数据段，另外一个进程看不到。这时的内存映射情况如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/f3/b4/f372ba2fdae24f462c45fc4bfd8a3db4.jpg?wh=2284x1385\" alt=\"\"></p><p>上面这幅图与<a href=\"https://time.geekbang.org/column/article/430073\">第1节课</a>中的页面映射的图几乎如出一辙。正是虚拟地址技术让我们在进程间共享动态库变得容易，我们只需要在虚拟空间里设置一下到物理地址的映射即可完成共享。</p><p>虽然libc.so在物理内存中只有一份，但它可以被多个进程进行映射。<strong>而且进程1映射libc.so代码段的虚拟地址与进程2映射libc.so代码段的虚拟地址可以不相等</strong>。正如本节课开头所分析的，这样做可以使得多个进程共享一份代码，大大节约了内存。</p><p>到目前为止，动态库技术看上去都非常好。但不知道你有没有发现一个问题？如果共享的动态库超过了两个，并且这些动态库之间还有相互引用的时候，情况就变得复杂了。我们还是用图来说明：</p><p><img src=\"https://static001.geekbang.org/resource/image/ba/63/baa40c177b411856f1c2e1918bc2ef63.jpg?wh=2284x1261\" alt=\"\"></p><p>如上图所示，如果两个进程共享了libc.so和libd.so两个动态库，而且libc中会调用libd中定义的foo方法。</p><p>进程1将foo方法映射到自己的虚拟地址0x1000处，而调用foo方法的指令被映射到0x2000处，那么call指令如果采用依赖rip寄存器的相对寻址的办法，这个偏移量应该填-0x1000。进程2将foo方法映射到自己虚拟地址0x2000处，调用foo方法的指令被映射到0x5000处，那么call指令的参数就应该填-0x3000。这就产生了冲突。</p><p>显然，我们第6节课所讲的通过rip寄存器进行相对寻址的办法在这里行不通了，相对寻址要求目标地址和本条指令的地址之间的相对值是固定的，这种代码就是地址有关的代码。当目标地址和调用者的地址之间的相对值不固定时，就需要地址无关代码技术了。</p><h2>地址无关代码的核心结构</h2><p>在计算机科学领域，有一句名言：“计算机领域的所有问题都可以使用新加一层抽象来解决”。这句话的应用在计算机领域随处可见。同样地，要实现代码段的地址无关代码，思路也是通过添加一个中间层，使得对全局符号的访问由直接访问变成间接访问。</p><p>我们可以引入一个固定地址，让引用者与这个固定地址之间的相对偏移是固定的，然后这个地址处再填入foo函数真正的地址。当然，这个地方必然位于数据段中，是每个进程私有的，这样才能做到在不同的进程里，可以访问不同的虚拟地址。这个新引入的固定地址就是<strong>全局偏移表(Global Offset Table, GOT)</strong>。GOT的工作原理如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/e7/88/e790179942821d8c5acb02de69fb1b88.jpg?wh=2284x1307\" alt=\"\"></p><p>在上图中，call指令处被填入了0x3000，这是因为进程1的GOT与call指令之间的偏移是0x5000-0x2000=0x3000，同时进程2的GOT与call指令之间的偏移是0x8000-0x5000=0x3000。所以对于这一段共享代码，不管是进程1执行还是进程2执行，它们都能跳到自己的GOT表里。</p><p>然后，进程1通过访问自己的GOT表，查到foo函数的地址是0x1000，它就能真正地调用到foo函数了。进程2访问自己的GOT表，查到foo函数的地址是0x2000，它也能顺利地调用foo函数。这样我们就通过引入了GOT这个间接层，解决了call指令和foo函数定义之间的偏移不固定的问题。</p><p>这种技术就是地址无关代码(Position Independent Code, PIC)。接下来我们用一个实际例子让你加深对PIC技术的理解。</p><p>与第6节课讲解linker相似，我们继续用具体的例子来看一下PIC技术中对几种常见类型的地址访问是如何处理的。例子如下：</p><pre><code>// foo.c\n \nstatic int static_var;\nint global_var;\nextern int extern_var;\nextern int extern_func();\n \nstatic int static_func() {\n    return 10;\n}\n \nint global_func() {\n    return 20;\n}\n \nint demo() {\n    static_var = 1;\n    global_var = 2;\n    extern_var = 3;\n    int ret_var = static_var + global_var + extern_var;\n    ret_var += static_func();\n    ret_var += global_func();\n    ret_var += extern_func();\n    return ret_var;\n}\n</code></pre><p>例子中分别从指令、数据和它的作用域的角度区分了如下几种类型：</p><ol>\n<li>static_var，表示静态变量的访问；</li>\n<li>static_func，表示静态函数的访问；</li>\n<li>extern_var，表示外部变量的访问；</li>\n<li>extern_func，表示外部函数的访问；</li>\n<li>global_var，表示全局变量的访问；</li>\n<li>global_func，表示全局函数的访问；</li>\n</ol><p>demo()函数是用来对以上几种类型进行访问来查看代码的生成。</p><p>我们把上边的例子编译成so文件，然后反汇编看一下demo函数的汇编是怎样的。</p><pre><code>$ gcc foo.c -fPIC -shared -fno-plt -o libfoo.so\n$ objdump -S libfoo.so\n0000000000000680 &lt;demo&gt;:\n 680:   55                      push   %rbp\n 681:   48 89 e5                mov    %rsp,%rbp\n 684:   48 83 ec 10             sub    $0x10,%rsp\n 688:   c7 05 92 09 20 00 01    movl   $0x1,0x200992(%rip)        # 201024 &lt;static_var&gt;\n 68f:   00 00 00\n 692:   48 8b 05 27 09 20 00    mov    0x200927(%rip),%rax        # 200fc0 &lt;global_var-0x68&gt;\n 699:   c7 00 02 00 00 00       movl   $0x2,(%rax)\n 69f:   48 8b 05 4a 09 20 00    mov    0x20094a(%rip),%rax        # 200ff0 &lt;extern_var&gt;\n 6a6:   c7 00 03 00 00 00       movl   $0x3,(%rax)\n 6ac:   8b 15 72 09 20 00       mov    0x200972(%rip),%edx        # 201024 &lt;static_var&gt;\n 6b2:   48 8b 05 07 09 20 00    mov    0x200907(%rip),%rax        # 200fc0 &lt;global_var-0x68&gt;\n 6b9:   8b 00                   mov    (%rax),%eax\n 6bb:   01 c2                   add    %eax,%edx\n 6bd:   48 8b 05 2c 09 20 00    mov    0x20092c(%rip),%rax        # 200ff0 &lt;extern_var&gt;\n 6c4:   8b 00                   mov    (%rax),%eax\n 6c6:   01 d0                   add    %edx,%eax\n 6c8:   89 45 fc                mov    %eax,-0x4(%rbp)\n 6cb:   b8 00 00 00 00          mov    $0x0,%eax\n 6d0:   e8 95 ff ff ff          callq  66a &lt;static_func&gt;\n 6d5:   01 45 fc                add    %eax,-0x4(%rbp)\n 6d8:   b8 00 00 00 00          mov    $0x0,%eax\n 6dd:   ff 15 ed 08 20 00       callq  *0x2008ed(%rip)        # 200fd0 &lt;global_func+0x20095b&gt;\n 6e3:   01 45 fc                add    %eax,-0x4(%rbp)\n 6e6:   b8 00 00 00 00          mov    $0x0,%eax\n 6eb:   ff 15 ef 08 20 00       callq  *0x2008ef(%rip)        # 200fe0 &lt;extern_func&gt;\n 6f1:   01 45 fc                add    %eax,-0x4(%rbp)\n 6f4:   8b 45 fc                mov    -0x4(%rbp),%eax\n 6f7:   c9                      leaveq\n 6f8:   c3                      retq\n</code></pre><p><strong>1. 静态变量访问方式</strong></p><p>这里先来看一下static_var。从demo的汇编里来看，在0x688的位置（第7行），我们可以看到这里对static_var变量的访问采用的是基于%rip的偏移。其中指令后边的注释标明了当前指令访问的虚拟地址0x201024，通过<code>objdump -d libfoo.so</code>查看0x201024位置存放的符号是static_var，在.bss段中。因此可以看出，在同一个共享文件里边，对static变量的访问可以通过%rip偏移的方式来确定数据的位置。</p><p>目前我们的讲解都是基于64位的系统，但这里值得一提的是，32位系统下由于没有相对PC偏移的寻址方式，编译器在生成32位PC偏移寻址时，是如下的一段汇编：</p><pre><code>000003c0 &lt;__x86.get_pc_thunk.bx&gt;:\n 3c0:   8b 1c 24                mov    (%esp),%ebx\n 3c3:   c3                      ret\n ...\n 000004e5 &lt;demo&gt;:\n ...\n 4ec:   e8 cf fe ff ff          call   3c0 &lt;__x86.get_pc_thunk.bx&gt;\n 4f1:   81 c3 0f 1b 00 00       add    $0x1b0f,%ebx\n 4f7:   c7 83 14 00 00 00 01    movl   $0x1,0x14(%ebx)\n ...\n</code></pre><p>这里可以看到，32位系统是通过一个call stub来获取的pc的值。因为call指令本身会做的一个操作是将return address压栈，而在__x86.get_pc_thunk.bx这个stub里边，则将当前栈顶的值(%esp)取出来放到%ebx寄存器中，那么此时%ebx里存放的就是ret之后的pc的值了。这个设计利用了call指令的会将下一条指令地址压栈的思路，非常巧妙的获取了pc的值，还是很有意思的。</p><ol start=\"2\">\n<li><strong>静态函数的访问方式</strong></li>\n</ol><p>静态函数和静态变量一样，都是不能被外部访问的，所以我们也可以推测它的寻址方式和静态变量一样，那么这里我就不再详细讲解验证过程了，请你自己动手验证。</p><ol start=\"3\">\n<li><strong>外部变量的访问</strong></li>\n</ol><p>接着来看对extern_var的访问。demo中对extern_var的访问是0x69f和0x6a6两条指令。0x69f先将extern_var的地址mov到rax寄存器中，然后0x6a6则将具体的数据0x3写到extern_var表示的内存地址中。</p><p>可以得到这条指令中使用的实际地址地址是0x6a6 + 0x20094a = 0x200ff0，继续通过objdump来查看对应位置的内容。</p><pre><code>$objdump -D libfoo.so\n Disassembly of section .got:\n \n0000000000200fc0 &lt;.got&gt;:\n        ...\n</code></pre><p>这里就是我们刚才讲过的GOT了。其中存放的是该模块需要访问的所有外部符号的地址。这样可以使得对外部符号的访问转换为对GOT表的访问。由于GOT表的相对偏移在同一个so中肯定是不变的，所以对GOT的访问可以使用相对寻址完成。</p><p>GOT中指向的是调用目标的在各自进程中的虚拟地址，我们是通过GOT表间接访问的方式，将对外部符号地址的直接依赖消除了。</p><p>每个进程都有自己的私有GOT段，GOT中记录了当前的so文件所引用的所有外部符号。这些外部符号都需要进行解析和重定位。这个工作由loader负责，其为符号分配并记录地址，然后将这些地址回写进GOT表。这个过程的原理和上节课所讲的两阶段重定位过程几乎一致，区别仅仅是linke操作的是文件中的地址，而loader操作的是内存地址。</p><ol start=\"4\">\n<li><strong>外部函数访问</strong></li>\n</ol><p>例子中0x6eb位置是对extern_func的调用处，同外部数据访问类似，这里也是采用了GOT表的间接访问的方式，GOT表0x200fe0的位置存放的是extern_func的运行时地址，也需要在启动时进行重定位。</p><ol start=\"5\">\n<li><strong>全局变量和全局函数的访问</strong></li>\n</ol><p>从例子中可以看到，对于全局变量和全局函数的访问的处理方式，与外部变量和外部函数的访问方式是保持一致的，都是采用GOT的方式，因此在这里我就不再详细解释了，你可以去上面的例子中看一下。</p><h2>总结</h2><p>为了节约内存，让进程间可以共享代码，人们把可以被共享的代码都抽出来，放到一个文件中，多个进程共享这个文件就可以了。这个可共享的文件就是动态库文件。动态库文件中的符号要在加载时才被解析，所以这种技术就叫动态链接技术。</p><p>动态库文件被加载进内存以后，在物理内存只有一份，多个进程都可以将它映射进自己的虚拟地址空间。各个进程在映射时可以将动态库的代码段映射到任意的位置。</p><p>如果两个共享库之间有引用关系的话，引用者和被引用者之间的相对位置就不能确定了，这时就需要引入地址无关代码技术。对于内部函数或数据访问，因为其相对偏移是固定的，所以可以通过相对偏移寻址的方式来生成代码；对于外部和全局函数或数据访问，则通过GOT表的方式，利用间接跳转将对绝对地址的访问转换为对GOT表的相对偏移寻址，由此得到了地址无关的代码。</p><p>地址无关的代码除了可以在so中使用，同样可以在可执行文件中使用，可以通过-pie选项使得gcc编译地址无关的可执行文件。地址文件的可执行文件可以被加载到内存的任意位置执行，这会使得缓冲区溢出的难度增加（你可以结合<a href=\"https://time.geekbang.org/column/article/433530\">第4节课</a>思考一下原因），但代价是通过GOT访问地址会多一次访存，性能会下降。</p><p>通过今天这节课，我们对动态链接和其中地址无关代码技术有了整体的认知，但在这里面仍然可以看到引入动态链接带来的一些问题。下节课，我们会进一步探讨动态链接的优化，以及动态链接器与loader的实现。</p><h2>思考题</h2><p>我们在<a href=\"https://time.geekbang.org/column/article/431904\">第3节课</a>讲过二进制文件的.text段会被加载进内存的代码段(code segment)，请你想一想，.got段加载进内存的什么位置是比较合理的？</p><p><img src=\"https://static001.geekbang.org/resource/image/57/6d/57bff71ccae778cd475df11f6bbc946d.jpg?wh=2284x1361\" alt=\"\"></p><p>好啦，这节课到这就结束啦。欢迎你把这节课分享给更多对计算机内存感兴趣的朋友。我是海纳，我们下节课再见！</p>","float_qrcode":"https://static001.geekbang.org/resource/image/83/1c/83e244537b714e899fd2e7896e22371c.png","article_cover_hidden":false,"is_required":true,"score":"31636300800","like_count":8,"article_subtitle":"","audio_download_url":"https://static001.geekbang.org/resource/audio/1b/5a/1bcab44c866695451edca8d9f247725a.mp3","had_viewed":false,"article_title":"07 | 动态链接（上）：地址无关代码是如何生成的？","column_bgcolor":"#F6F7FB","offline_package":"{}","audio_title":"11.6 海纳 07_dynamic_link_pic_R.mp3","audio_time_arr":{"m":"23","s":"07","h":"00"},"text_read_percent":0,"cid":450,"article_cshort":"<p>你好，我是海纳。</p><p>通过上节课的学习，我们了解到，链接器可以将不同的编译单元所生成的中间文件组合在一起，并且可以为各个编译单元中的变量和函数分配地址，然后将分配好的地址传给引用者。这个过程就是静态链接。</p><p>静态链接可以让开发者进行模块化的开发，大大的促进了程序开发的效率。但同时静态链接仍然存在一个比较大的问题，就是无法共享。例如程序A与程序B都需要调用函数foo，在采用静态链接的情况下，只能分别将foo函数链接到A的二进制文件和B的二进制文件中，这样导致系统同时运行A和B两个进程的时候，内存中会装载两份foo的代码。那么如何消除这种浪费呢，这就是我们接下来两节课的主题：动态链接。</p><p>动态链接的重定位发生在加载期间或者运行期间，这节课我们将重点分析加载期间的重定位，它的实现依赖于地址无关代码。我们知道，深入地掌握动态链接库是开发底层基础设施必备的技能之一，如果你想要透彻地理解动态链接机制，就必须掌握地址无关代码技术。</p><p>在你掌握了地址无关代码技术后，你还将对程序员眼中的“风骚”操作，比如，如何通过重载动态库对系统进行热更新，如何对动态库里的函数进行hook操作，以便于调试和追踪问题等等，都会有更深入的理解。</p>","video_width":0,"column_could_sub":true,"video_id":"","sku":"100094901","video_cover":"","author_name":"海纳","column_is_onboard":true,"audio_url":"https://res001.geekbang.org/media/audio/1b/5a/1bcab44c866695451edca8d9f247725a/ld/ld.m3u8","chapter_id":"2314","column_had_sub":true,"column_cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg","neighbors":{"left":{"article_title":"06 | 静态链接：变量与内存地址是如何映射的？","id":436308},"right":{"article_title":"08 | 动态链接（下）：延迟绑定与动态链接器是什么？","id":440471}},"rate_percent":0,"footer_cover_data":{"img_url":"https://static001.geekbang.org/resource/image/f0/fe/f0c47daf3262a8df6cf352d89896abfe.png","link_url":"https://time.geekbang.org/article/427012","mp_url":""},"float_app_qrcode":"","column_is_experience":false,"rate":{"1":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"2":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"3":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0}},"product_id":100094901,"had_liked":false,"id":437653,"free_get":false,"is_video_preview":false,"article_summary":"我们知道，深入地掌握动态链接库是开发底层基础设施必备的技能之一，如果你想要透彻地理解动态链接机制，就必须掌握地址无关代码技术。","column_sale_type":0,"float_qrcode_jump":"https://time.geekbang.org/serv/v4/misc/jump?uri=https%3A%2F%2Ftime.geekbang.org%2Fhybrid%2Fmp%2Fjump%3Furl%3Dhttps%253A%252F%252Fstatic001.geekbang.org%252Fresource%252Fimage%252Fee%252Fb2%252Feedd4090668aabfac91b2b445d27cbb2.png","column_id":450,"article_sharetitle":"07 | 动态链接（上）：地址无关代码是如何生成的？","article_poster_wxlite":"https://static001.geekbang.org/render/screen/dd/43/ddb9d3f5f39b1614bcc740d68ec80d43.jpeg","article_features":0,"comment_count":14,"audio_md5":"1bcab44c866695451edca8d9f247725a","offline":{"size":23374716,"file_name":"0d780c4496438ecd3c56bc8c1f7e7877","download_url":"https://static-acl-001.geekbang.org/resource/zip/article/437653/0d780c4496438ecd3c56bc8c1f7e7877.zip?auth_key=1641482157-98d74079382a4066a2c7b3d29dfcc1c6-0-3a684fa58df71d15330a92a152b01919"},"video_size":0,"hls_videos":[],"video_time":"","article_could_preview":false,"article_ctime":1636300800,"share":{"content":"学好内存，掌握系统开发诀窍","title":"海纳 · 编程高手必学的内存知识","poster":"https://static001.geekbang.org/resource/image/bc/c5/bcaf3ff69e538f8749970edf2301bbc5.jpg","cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg"}},"440452":{"text_read_version":0,"audio_size":19904415,"article_cover":"https://static001.geekbang.org/resource/image/51/cd/516cf5977fe12fcbe7b32b8a3a01c5cd.jpg","subtitles":[],"product_type":"c1","audio_dubber":"海纳","is_finished":false,"like":{"had_done":false,"count":6},"audio_time":"00:20:43","video_height":0,"article_content":"<p>你好，我是海纳。</p><p>在<a href=\"https://time.geekbang.org/column/article/431904\">第3节课</a>，我们讲到线性地址空间按照功能的不同，可以分为不同的区域。同时，我们还简单介绍了，如何使用sbrk和mmap这两个系统调用，向操作系统申请堆内存。</p><p>其实，堆内存是程序员打交道最多的一块区域，无论是哪种编程语言，正确合理并高效地使用堆内存，都是极具挑战的一件事情。对程序调优是系统程序员常见的工作任务，而堆内存的管理和分配恰恰是最容易出现性能瓶颈的模块。</p><p>不过，sbrk和mmap这两个系统调用分配内存效率比较低，我们在<a href=\"https://time.geekbang.org/column/article/435493\">第5节课</a>讲过，进程的内核态和用户态的区别，执行系统调用是要进入内核态的，运行态的切换会耗费不少时间。为了解决这个问题，人们倾向于使用系统调用来分配大块内存，然后再把这块内存分割成更小的块，以方便程序员使用，这样可以提升分配的效率。</p><p>在C语言的运行时库里，这个工作是由malloc函数负责的。但有时候C语言的原生malloc实现还是不能满足特定应用的性能要求，这就需要程序员来实现符合自己应用要求的内存池，以便自己进行内存的分配和释放。</p><p>这节课，我们就一起来学习，如何对通过系统调用申请来的大块内存进行更精细化的管理。通过这节课的学习，你将了解到堆内存管理的常用方法，以及内存泄露、double free等常见的内存问题产生的原因和排查方法，从而提高自己分析和解决内存问题的能力。</p><!-- [[[read_end]]] --><h2>malloc的基本功能</h2><p>正如这节课开头讲的，向操作系统申请来的大块内存，需要分割成合适的大小，才能让程序员正常使用，其实这个任务是由glibc承担的。</p><p>glibc是C语言的运行时库，C语言中常用的函数，例如printf、scanf、memcpy和strcat等等，它们的实现都在glibc.so中。通过后缀名，你可能也猜到了，glibc是一种动态链接库，它的工作原理与<a href=\"https://time.geekbang.org/column/article/437653\">第7节课</a>和<a href=\"https://time.geekbang.org/column/article/440471\">第8节课</a>里所讲的动态链接库是一样的，与普通的动态库相比，它并没有什么特别之处。</p><p>我们今天要讲的就是glibc中用于内存管理的两个重要函数：malloc和free。我们先展示一下malloc和free的用法，请看下面这段C代码：</p><pre><code>#include &lt;stdio.h&gt;\n#include &lt;malloc.h&gt;\n\nint main() {\n\tvoid *p = malloc(16)；\n\tprintf(&quot;%p\\n&quot;, p);\n\tfree(p);\n\treturn 0;\n}\n</code></pre><p>上述代码中应用程序通过malloc函数申请了一块内存，并把这块内存的起始地址打印了出来，然后再通过free函数释放这块内存。通过<a href=\"https://time.geekbang.org/column/article/430073\">第1节课</a>和<a href=\"https://time.geekbang.org/column/article/431904\">第3节课</a>的学习，我们已经知道，打印的结果实际上是申请的内存块的线性地址。更具体一点，这块空间位于堆中。</p><p>malloc 实现的基本原理是先向操作系统申请一块比较大的内存，然后再通过各种优化手段让内存分配的效率最大化。在glibc的实现里，malloc函数在向操作系统申请堆内存时，会使用mmap，以4K的整数倍一次申请多个页。这样的话，mmap的区域就会以页对齐，页与页之间的排列非常整齐，避免了出现内存碎片。</p><p>从这个角度看，glibc 中的 malloc 方法非常像批发商，它从供应商操作系统那里一次批发了很大的内存，然后以零销的方式一点点分配出去，而且它不光负责销售，还负责售后（分配到的内存可以使用 free 退货）。</p><p>要想又好又快地正确使用堆内存，一个很重要的方式就是对内存做精细化管理。</p><h2>malloc的实现原理</h2><p>内存的精细化管理，我们要考虑两个因素，<strong>一是分配和回收的效率，二是内存区域的有效利用率</strong>，内存区域的有效利用率又包含两个方面，一个方面是每一小块内存内部是否被合理利用，另一个方面是块与块之间是否存在无法利用的小块内存。</p><p>你可以用建筑规划来进行类比。我们有一块很平整的地，如果一个建筑设计得不合理，比如本来只有一千个学生，但却修了一个五千人的体育场，这就是内部空间的浪费。如果各个建筑都是奇形怪状的（不能对齐），那么建筑与建筑之间的不可用的地块就会增多，这就是外部空间的浪费，或者称为碎片。</p><p>对小块内存进行精细化管理，最常用的数据结构就是链表。为了能够方便地进行分配和回收，人们把空闲区域记录到链表里，这就是空闲链表(free list)。</p><h3>空闲链表</h3><p>空闲链表里的节点主要是为了记录内存的开始位置和长度，如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/f4/e1/f43468e79fd311ca81d3e181d8575ae1.jpg?wh=2284x1147\" alt=\"\"></p><p>图中展示了一个总长度为100的内存区域，已经分割成16、16、20、16、16、16六个小的内存块。其中着色部分，也就是第一、第三和第五块内存是已经分配出去的，正在使用的内存，而白色区域则是尚未分配的内存。图的上半部分代表空闲链表，每一块未分配的内存都会由一个空闲链表的节点进行管理。结点中记录了这块空闲内存区域的起始位置和长度。</p><p>当分配内存的请求到达以后，我们就通过遍历free list来查找可用的空闲内存区域，在找到合适的空闲区域以后，就将这一块区域从链表中摘下来。比如要请求的大小是m，就将这个结点从链表中取下，把起始位置向后移动m，大小也相应的减小m。将修改后的结点重新挂到链表上。</p><p>在释放的时候，将这块区域按照起始起址的排序放回到链表里，并且检查它的前后是否有空闲区域，如果有就合并成一个更大的空闲区。</p><p>这种算法所使用的数据结构比较简单，算法也很直接，我们把这种算法称为简单算法(Naive Algorithm)。我们举个例子说明简单算法的运行过程，假如在算法开始时，内存的情况如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/61/46/61ec3206f7369287939dbdae24804f46.jpg?wh=2284x1160\" alt=\"\"></p><p>假设对内存的操作序列是这样的：</p><pre><code>void test() {\n\tvoid* p1 = malloc(16);\n\tvoid* p2 = malloc(16);\n\tvoid* p3 = malloc(20);\n\n\tfree(p2);\n\n\tvoid* p4 = malloc(16);\n\tvoid* p5 = malloc(16);\n\n\tfree(p4);\n}\n</code></pre><p>执行完test函数以后，内存的划分就会和这节课的第一幅图一样了。做为练习，请你自己画出每一个步骤free list和内存的变化情况，这里不再给出。</p><p>如果此时，又到达了一个内存分配请求，要申请一个大小为20的内存区域，虽然所有空闲区域的大小之和是48，是超过20的，但是由于这三块空闲区域并不连续，所以，我们已经无法从这100字节的内存中再分配出一块20字节的内存区域了，相对于这次请求，这三块16字节的空闲区域就是<strong>内存碎片</strong>。这就是我们所介绍的简单算法的第一个缺陷：<strong>会产生内存碎片</strong>。</p><p>每一次分配内存时，我们都需要遍历free list，最差情况下的时间复杂度显然是O(n)。如果是多线程同时分配的话，free list会被多线程并发访问，为了保护它，就必须使用各种同步机制，比如锁或者无锁的concurrent linked list等。可见上述算法的第二个缺陷是<strong>分配效率一般，且多线程并发场景下性能还会恶化</strong>。</p><p>为了改进以上两个问题，人们想了很多办法，我们举几个历史上曾经出现的改进方案。</p><p>其中一种方案是直接对简单算法进行优化。简单算法中找到第一个可用的区域就返回，这个策略被称为 First Fit，优化的具体做法是把它改成最佳匹配(Best Fit)，改造后，它要找到能满足条件的最小的空闲区域才返回。</p><p>从直观上说，这种分配策略能尽可能地保留大块内存，避免它被快速地分割成小块内存，这就能更好地对抗内存碎片。严格的理论证明也证明了这一点。但是这种策略需要遍历整个链表，时间复杂度反而变差。</p><p>另一种方案是Knuth提出的Next Fit策略，即每次查找不必从头开始，而是从上一次查找的位置继续向后查找。实验也证明，这种策略会比从头开始的算法有更高的效率。但它依然不能解决内存碎片的问题。</p><p>还有一种改进方案，名字叫分桶式管理，<strong>这种改进是一种相对均衡的做法，在对抗内存碎片和分配释放的时间复杂度两个方向都有改善</strong>。这也是在现实中被使用的最广泛的一种方法。接下来，我们就重点分析分桶式管理算法。</p><h3>分桶式内存管理</h3><p>分桶式内存管理采用了多个链表，对于单个链表，它内部的所有结点所对应的内存区域的大小是相同的。换句话说，相同大小的区域会挂载到同一个链表上。</p><p>最常见的方式是以4字节为最小单位，把所有4字节的区域挂到同一个链表上，再把8字节的区域挂到一起，然后是16字节，32字节，这样以2次幂向上增长。如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/cd/c2/cd9deb75706b6e5267e8f7f308c205c2.jpg?wh=2284x1222\" alt=\"\"></p><p>采用了新的数据结构以后，分配和回收的算法也相应地发生了变化。</p><p>首先，分配的时候，我们要只要找到能满足这一次分配请求的最小区域，然后去相应的链表里把整块区域都取下来。比如，分配一个7字节的内存块时，我们就可以从8字节大小的空闲链表里直接取出链表头上的那块区域，分配给应用程序。由于从链表头上删除元素的时间复杂度是O(1)，所以我们分配内存的效率就大大提高了。</p><p>由于整个大块内存被提前分割成了整齐的小块（比如是以4字节对齐），所以整个区域里不存在块与块之间内存碎片。但是这种做法还是会产生区域内部的空间浪费，比如上面举的例子，当申请的内存大小是7时，按当前算法，只能分配给它大小为8的块，这就造成了一个字节的内部浪费，或者称之为内部碎片。</p><p>内部碎片带来的问题是内存使用率没有达到100%，在最差情况下，可能只有50%。但是内部碎片随着这一块区域的释放也就消失了，所以不会因为长时间运行而积累成严重的问题。</p><p>释放时，只需要把要释放的内存直接挂载到相应的链表里就可以了。 这个速度和分配是一样的，效率非常高。</p><p><strong>分桶式内存管理比简单算法无论是在算法效率方面，还是在碎片控制方面都有很大的提升</strong>。但它的缺陷也很明显：区域内部的使用率不够高和动态扩展能力不够好。例如，4字节的区域提前消耗完了，但8字节的空闲区域还有很多，此时就会面临两难选择，如果直接分配8字节的区域，则区域内部浪费就比较多，如果不分配，则明明还有空闲区域，却无法成功分配。</p><p>为了解决上述两个问题，人们在分桶的基础上继续改进，让内存可以根据需求动态地决定小的内存区域和大的内存区域的比例。这种设计的典型就是伙伴系统，我们一起来看下。</p><h3>伙伴系统</h3><p>正如上面的例子所讲的，当系统中还有很多8字节的空闲块，而4字节的空闲块却已经耗尽，这时再有一个4字节的请求，则会出现malloc失败的情况。为了避免分配失败，我们其实还可以考虑将大块的内存做一次拆分。</p><p>如下图所示。分配一块4字节大小的空间，在4字节的free list上找不到空闲区域，系统就会往上找，假如8字节和16字节的free list中也没有空闲区域，就会一直向上找到32字节的free list。</p><p><img src=\"https://static001.geekbang.org/resource/image/6f/26/6f9ef8402a8c0d1c9fdb61a3b4d24f26.jpg?wh=2284x1321\" alt=\"\"></p><p>伙伴系统不会直接把32的空闲区域分配出去，因为这样做的话，会带来巨大的浪费。它会先把32字节分成两个16字节，把后边一个挂入到16字节的free list 中。然后继续拆分前一半。前一半继续拆成两个8字节，再把后一半挂入到8字节的 free list，最后，把前一半8字节拿去分配，当然这里也要继续拆分成两个4字节的空闲区域，其中一个用于本次malloc分配，另一个则挂入到4字节的free list。分配后的内存的状态如下所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/68/fb/68e667bdf6yy507f21e0f2b775cd7dfb.jpg?wh=2284x1273\" alt=\"\"></p><p><strong>这种不断地把一块内存分割成更小的两块内存的做法，就是伙伴系统，这两块更小的内存就是伙伴</strong>。 它的好处是可以动态地根据分配请求将大的内存分割成小的内存。当释放内存时，如果系统发现与被释放的内存相邻的那个伙伴也是空闲的，就会把它们合并成一个更大的连续内存。通过这种拆分，系统就变得更加富有弹性。</p><p>malloc的实现，在历史上先后共有几十种策略，这些策略往往就是上述三种算法的组合。具体到glibc中的malloc实现，它就采用了分桶的策略，但是它的每个桶里的内存不是固定大小的，而是采用了将1 ~ 4字节的块挂到第一个链表里，将5 ~ 8字节的块挂到第二个链表里，将9~16字节的块挂到第三个链表里，依次类推。</p><p>在单个链表内部则采用naive的分配方式，比如要分配5个字节的内存块，我们会先在5 ~ 8这个链表里查找，如果查找到的内存大小是8字节的，那就会将这个区域分割成5字节和3字节两个部分，其中5字节用于分配，剩余的3字节的空闲区域则会挂载到1~4这个链表里。</p><p>可见malloc的实现策略是比较灵活的，针对不同的场景，不同的分配策略的性能表现也是不一样的。很多公司的基础平台都选择自己实现内存池来提供malloc接口，这样可以更好地服务本公司的业务。最著名的例子就是Google公司实现的Tcmalloc库。</p><p>Tcmalloc相比起其他的malloc实现，最大的改进是在多线程的情况下性能提升。我们知道，<strong>在多线程并发地分配内存时，每次分配都要对free list进行加锁以避免并发程序带来的问题，这就容易形成性能瓶颈</strong>。</p><p>为了解决这个问题，Tcmalloc引入了线程本地缓存(Thread Local Cache)，每个线程在分配内存的时候都先在自己的本地缓存中寻找，如果找到就结束，只有找不到的情况才会继续向全局管理器申请一块大的空闲区域，然后按照伙伴系统的方式继续添加到本地缓存中去。</p><p>在实际工作中，你可能会遇到这两个问题而束手无策：</p><ul>\n<li>第一个问题是，系统所提供的malloc，其性能不足以支撑自己的业务，或者自己的业务在分配内存时有其特殊的规律，需要为它做专门的订制和优化；</li>\n<li>第二个问题是，在malloc和free里做一些统计动作以排查问题，比如打印日志。</li>\n</ul><p>下面，我来带你自己动手实现内存管理库，让你更好解决内存问题的同时，还可以深入地理解内存管理的更多技术细节。</p><h2>自己动手实现内存管理库</h2><p>第1个问题的典型代表就是上面所提到的Tcmalloc。我们这里举一个第二个问题的例子。比如，我曾经遇到过一个double free的错误，在申请了一段内存以后，经过复杂的逻辑，有两个指针指向了同一块内存，当我对两个指针都调用free方法的时候，错误就发生了，我把这个错误示例进行了简化，并把它的代码放在下面：</p><pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nint main() {\n    int* p = (int*)malloc(sizeof(int));\n    char* q = (char*)p;\n\n    free(p);\n    free(q);\n}\n</code></pre><p>很明显，第8行第9行释放的是同一块内存，运行这个例子，我们会看到进程 crash了，并且系统提示为：</p><pre><code>*** Error in `./dfree': double free or corruption (fasttop): 0x000000000196a010 ***\nAborted\n</code></pre><p>这就是double free的错误，也就是说一块内存被释放了两次。这个例子比较简单，但是在复杂逻辑中，我们往往很难判断多个指针是否指向相同的地址。</p><p>如果我们在所有调用free的地方增加日志，把要释放的指针记录下来，就会比较有助于分析和定位问题。幸运的是，我们确实有这种手段，那就是Linux的preload机制。</p><p>在<a href=\"https://time.geekbang.org/column/article/440471\">第8节课</a>，我们深入地学习了Loader的原理，我们知道对于未定义的引用，动态链接器要先进行解析，它会先搜索LD_PRELOAD目录下的动态库，然后再搜索其他的库，所以我们就有办法对malloc和free函数进行替换。比如自己提供free的实现：</p><pre><code>#define _GNU_SOURCE\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;dlfcn.h&gt;\n\nvoid free(void *ptr) {\n    void(*freep)() = NULL;\n\n    printf(&quot;ready to do free: %p\\n&quot;, ptr);\n    freep = dlsym(RTLD_NEXT, &quot;free&quot;);\n    freep(ptr);\n    printf(&quot;free done: %p\\n&quot;, ptr);\n}\n</code></pre><p>我们先来分析一下上面的代码，我要提醒一下你，第1行的_GNU_SOURCE是一定要添加的，因为第10行的RTLD_NEXT宏依赖于这个宏。</p><p>接着，我们定义了一个函数指针（第7行），它可以指向真正的free的实现。然后分别在调用free前和调用free以后打印一次要释放的指针（第9行和第12行），再通过dlsym打开了glibc中的free方法（第10行），dlsym的作用是通过符号名称找到符号对应的地址。</p><p>你还要注意的是，第10行使用RTLD_NEXT就是告诉ld-linux.so不要在当前文件中找free这个符号，而是要按照动态库的搜索顺序找到下一个动态库，并在它里面寻找free函数，实际上，这里找到的就是glibc里的free函数了。</p><p>分析完这个代码以后，我们发现这里自己定义的free方法不过是glibc里的free方法的一个包装(wrapper)。接着，我们使用以下命令编译myfree库，并设置preload再执行dfree用例：</p><pre><code>$ gcc -shared -fpic -o myfree.so myfree.c -ldl\n$ LD_PRELOAD=&quot;./myfree.so&quot; ./dfree\nready to do free: 0x1b44010\nfree done: 0x1b44010\nready to do free: 0x1b44010\n*** Error in `./dfree': double free or corruption (fasttop): 0x0000000001b44010 ***\nAborted\n</code></pre><p>运行上面的代码，你可以看到，我们新添加的日志已经可以正确输出了。通过这种方式，我们就重载了free方法。如果你特别感兴趣的话，你可以自己重新实现一个malloc的例子，<a href=\"https://gitee.com/hinus/codelet\">我的代码仓</a>可以给你作为参考。</p><h2>总结</h2><p>我们这节课深入地介绍了malloc方法的实现原理，并以一个例子来说明如何设计自己的动态内存管理器。</p><p>通过这节课的学习，我们了解到sbrk和mmap是操作系统提供的系统调用，系统调用性能不够且不能对分配的内存进行有效管理，所以必须有人在用户态将大块的内存分割成小块，然后进行更精细的分配和回收，这个工作通常情况下是由glibc承担的。</p><p>glibc所提供的malloc在管理内存时，采用了空闲链表(free list)的方式。对free list的组织有简单算法、分桶管理和伙伴系统等不同的策略。</p><p>我们评价一个管理内存算法的因素主要有以下三个维度：</p><ol>\n<li><strong>数据结构如何设计，是否存在内存碎片</strong>；</li>\n<li><strong>分配的效率</strong>；</li>\n<li><strong>释放的效率</strong>。</li>\n</ol><p>我把三种算法按照上面三个维度总结成以下表格，你可以参考一下：</p><p><img src=\"https://static001.geekbang.org/resource/image/68/8c/6860afe12f23d59e9d70f8191aed3b8c.jpg?wh=2284x1387\" alt=\"\"></p><p>由于动态链接器只识别第一次遇到的符号，所以我们就有机会通过重写glibc中的方法来实现自己的内存管理库。这主要是为了提升性能，或者是为了排查错误。我们以double free为例介绍了典型的内存错误，并通过覆写free函数来解决它。这展示了一个完整的设计内存管理库的过程。</p><h2>思考题</h2><p>我们在实战环节介绍了如何在free方法里增加log以统计哪些内存曾经释放过，以便于排查哪些内存被重复释放了。</p><p>如果我们申请了一块内存，但是忘记释放了，这种情况就是内存泄露。请自己设计一个方案，统计内存泄露的情况。只要描述做法即可，如果你特别感兴趣的话，可以在我们这节课的代码里自己添加实现。欢迎你在留言区分享你的想法和收获，我在留言区等你。</p><p><img src=\"https://static001.geekbang.org/resource/image/bc/bf/bcf35ec9987038716b303079cfa982bf.jpg?wh=2284x1386\" alt=\"\"></p><p>好啦，这节课到这就结束啦。欢迎你把这节课分享给更多对计算机内存感兴趣的朋友。我是海纳，我们下节课再见！</p>","float_qrcode":"https://static001.geekbang.org/resource/image/83/1c/83e244537b714e899fd2e7896e22371c.png","article_cover_hidden":false,"is_required":true,"score":"31636646400","like_count":10,"article_subtitle":"","audio_download_url":"https://static001.geekbang.org/resource/audio/a5/37/a5c726fb86cdc710628165158202b237.mp3","had_viewed":false,"article_title":"09 | 深入理解堆：malloc和内存池是怎么回事？","column_bgcolor":"#F6F7FB","offline_package":"{}","audio_title":"11.8 海纳 09_malloc.MP3_R.mp3","audio_time_arr":{"m":"20","s":"43","h":"00"},"text_read_percent":0,"cid":450,"article_cshort":"<p>你好，我是海纳。</p><p>在<a href=\"https://time.geekbang.org/column/article/431904\">第3节课</a>，我们讲到线性地址空间按照功能的不同，可以分为不同的区域。同时，我们还简单介绍了，如何使用sbrk和mmap这两个系统调用，向操作系统申请堆内存。</p><p>其实，堆内存是程序员打交道最多的一块区域，无论是哪种编程语言，正确合理并高效地使用堆内存，都是极具挑战的一件事情。对程序调优是系统程序员常见的工作任务，而堆内存的管理和分配恰恰是最容易出现性能瓶颈的模块。</p><p>不过，sbrk和mmap这两个系统调用分配内存效率比较低，我们在<a href=\"https://time.geekbang.org/column/article/435493\">第5节课</a>讲过，进程的内核态和用户态的区别，执行系统调用是要进入内核态的，运行态的切换会耗费不少时间。为了解决这个问题，人们倾向于使用系统调用来分配大块内存，然后再把这块内存分割成更小的块，以方便程序员使用，这样可以提升分配的效率。</p><p>在C语言的运行时库里，这个工作是由malloc函数负责的。但有时候C语言的原生malloc实现还是不能满足特定应用的性能要求，这就需要程序员来实现符合自己应用要求的内存池，以便自己进行内存的分配和释放。</p><p>这节课，我们就一起来学习，如何对通过系统调用申请来的大块内存进行更精细化的管理。通过这节课的学习，你将了解到堆内存管理的常用方法，以及内存泄露、double free等常见的内存问题产生的原因和排查方法，从而提高自己分析和解决内存问题的能力。</p>","video_width":0,"column_could_sub":true,"video_id":"","sku":"100094901","video_cover":"","author_name":"海纳","column_is_onboard":true,"audio_url":"https://res001.geekbang.org/media/audio/a5/37/a5c726fb86cdc710628165158202b237/ld/ld.m3u8","chapter_id":"2314","column_had_sub":true,"column_cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg","neighbors":{"left":{"article_title":"08 | 动态链接（下）：延迟绑定与动态链接器是什么？","id":440471},"right":{"article_title":"10 | 页中断：fork、mmap背后的保护神","id":444178}},"rate_percent":0,"footer_cover_data":{"img_url":"https://static001.geekbang.org/resource/image/f0/fe/f0c47daf3262a8df6cf352d89896abfe.png","link_url":"https://time.geekbang.org/article/427012","mp_url":""},"float_app_qrcode":"","column_is_experience":false,"rate":{"1":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"2":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"3":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0}},"product_id":100094901,"had_liked":false,"id":440452,"free_get":false,"is_video_preview":false,"article_summary":"内存的精细化管理，我们要考虑两个因素，一是分配和回收的效率，二是内存区域的有效利用率。","column_sale_type":0,"float_qrcode_jump":"https://time.geekbang.org/serv/v4/misc/jump?uri=https%3A%2F%2Ftime.geekbang.org%2Fhybrid%2Fmp%2Fjump%3Furl%3Dhttps%253A%252F%252Fstatic001.geekbang.org%252Fresource%252Fimage%252Fee%252Fb2%252Feedd4090668aabfac91b2b445d27cbb2.png","column_id":450,"article_sharetitle":"09 | 深入理解堆：malloc和内存池是怎么回事？","article_poster_wxlite":"https://static001.geekbang.org/render/screen/d4/c5/d455e205bc1c61376860d74649fe71c5.jpeg","article_features":0,"comment_count":7,"audio_md5":"a5c726fb86cdc710628165158202b237","offline":{"size":20239788,"file_name":"a27d39d114eafac409e257b0e441f0ae","download_url":"https://static-acl-001.geekbang.org/resource/zip/article/440452/a27d39d114eafac409e257b0e441f0ae.zip?auth_key=1641482189-be8b48b3954147efa39425a2767a4372-0-40cf4746e2482dfba0e933fe50e2ee86"},"video_size":0,"hls_videos":[],"video_time":"","article_could_preview":false,"article_ctime":1636646400,"share":{"content":"学好内存，掌握系统开发诀窍","title":"海纳 · 编程高手必学的内存知识","poster":"https://static001.geekbang.org/resource/image/bc/c5/bcaf3ff69e538f8749970edf2301bbc5.jpg","cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg"}},"440471":{"text_read_version":0,"audio_size":22036563,"article_cover":"https://static001.geekbang.org/resource/image/83/31/8318a401be1c85d4d01d782926c0c431.jpg","subtitles":[],"product_type":"c1","audio_dubber":"海纳","is_finished":false,"like":{"had_done":false,"count":4},"audio_time":"00:22:56","video_height":0,"article_content":"<p>你好，我是海纳。</p><p>在上节课里，我们学习了动态链接过程的基本原理。动态链接通过GOT表加一层间接跳转的方式，解决了代码中call指令对绝对地址的依赖，从而实现了PIC的能力。我们同时也讲到了GOT表中的地址是由加载器在加载时填充的。</p><p>不过，细心的你也发现了，动态链接带来的代价是性能的牺牲。这里性能的牺牲主要来自于两个方面：</p><ol>\n<li>每次对全局符号的访问都要转换为对GOT表的访问，然后进行间接寻址，这必然要比直接的地址访问速度慢很多；</li>\n<li>动态链接和静态链接的区别是将链接中重定位的过程推迟到程序加载时进行。因此在程序启动的时候，动态链接器需要对整个进程中依赖的so进行加载和链接，也就是对进程中所有GOT表中的符号进行解析重定位。这样就导致了程序在启动过程中速度的减慢。</li>\n</ol><p>我们这节课来看看，如何通过延迟绑定技术，来解决性能下降的问题。延迟绑定不仅仅是用在动态链接中，还被广泛地应用在Hotspot，V8等带有即时编译功能的虚拟机中。另外，在游戏行业，修复服务器的错误的同时保证用户不掉线是硬需求，这种不停机进行代码修复的技术被称为热更新技术。学习完这节课后，你不仅能理解动态链接的基本原理，而且也能对热更新的基本原理有所感悟。</p><!-- [[[read_end]]] --><p>其实，不管是加载时重定位，还是延迟绑定技术，真正发挥作用的是动态链接器。所以这节课我也会给你简单介绍一下动态链接器的基本原理。</p><p>首先，我们从延迟绑定的最简单的形式，也就是Hotspot虚拟机中的运行时重定位技术patch code讲起。</p><h2>patch code技术</h2><p>我们知道，在Java语言中，类是按需加载的。也就是对于一个class文件，只有当hotspot第一次使用它的时候，它才会被加载进来。假如我们在即时编译A方法的时候要调用B方法，但这时B方法还没有被加载进来，该怎么办呢？</p><p>虚拟机会采用一种叫做patch code的技术，在运行时再进行加载。简单地说，就是在生成call指令时候，它的目标地址填成一个虚拟机内部的用于解析符号的方法。在CPU执行这条call语句的时候，就会调用符号解析函数。此时虚拟机就会加载B方法所在的类，然后就能确定B方法的地址了，这时再把B方法的地址写回到call指令里。这个过程如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/78/4c/78e36fae2f8c8239bae9fe422b1dce4c.jpg?wh=2284x1570\" alt=\"\"></p><p>这个过程很像是在给原始的代码打补丁，所以人们就把这种方式称为<strong>patch code技术</strong>。这就像是在原来的代码安装了一个机关，当CPU执行到这个机关时，就会触发一次符号的重定位，然后这个机关就被替换掉了。下一次CPU再执行到这个call指令的时候，就可以正常地调用到B方法了。</p><p>上节课，加载器在加载动态库时就把它的GOT中的所有符号都解析了，这种方法却把解析符号的过程又往后推到了执行代码时解析。</p><p>在Hotspot里的patch code技术，会直接修改指令参数。不过，运行时修改指令总是一件很危险的事情。所以，动态库真正使用的运行时解析符号技术是延迟绑定技术，它的关键步骤和patch code很相似，但却比patch code的安全性更好一些，我们一起来看一下。</p><h2>延迟绑定技术</h2><p>为了避免在加载时就把GOT表中的符号全部解析并重定位，就需要采用计算机领域非常重要的一个思想：Lazy。也就是说，把要做的事情推迟到必须做的时刻。</p><p>对于我们当前的问题来说，<strong>将函数地址的重定位工作一直推迟到第一次访问的时候再进行，这就是延迟绑定(Lazy binding)的技术</strong>。这样的话，对于整个程序运行过程中没有访问到的全局函数，可以完全避免对这类符号的重定位工作，也就提高了程序的性能。</p><p>patch code显然也是一种延迟绑定的技术，但是它要在运行时修改指令参数，这会带来风险。所以动态库的延迟绑定选择了继续使用GOT表来进行间接调用，然后patch的对象就不再是指令了，而是GOT中的一项。</p><p>理想情况下，我们把GOT中的待解析符号的地方都填成动态符号解析的函数就可以了，当CPU执行到这个函数的时候，就会跳转进去解析符号，然后把GOT表的这一项填成符号的真正的地址。如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/3b/e4/3b5b98531abd2b754e40d5af9cd32be4.jpg?wh=2284x1539\" alt=\"\"></p><p>但是动态解析符号的函数_dl_runtime_resolve依赖两个参数，一个是当前动态库的ID，另一个是要解析的符号在GOT表中的序号。动态库的ID存储在GOT的0x8偏移的位置，而要解析的符号序号却不容易得到。</p><p>为了解决传递参数的问题，动态链接又引入了过程链接表（Procedure Linkage Table， PLT)，将动态解析符号的过程做成了三级跳。如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/b6/02/b675cc80d4e8c10af17249d4e6461f02.jpg?wh=2284x1648\" alt=\"\"></p><p>在图中，我用序号①、②、③和它们旁边的箭头分别给你标注出了三级跳的路径。如果你仔细观察的话，你还会发现这张图与上一张图的主要变化就是引入了.plt段，在代码段里，main函数对B函数的调用转成了对\"B@plt\"的调用，\"B@plt\"函数只有三条指令。</p><p>它的第一条指令jmp *(GOT[3])是一个间接跳转，跳转的目标是GOT表偏移为0x18的位置，正常情况下，这个位置应该放的是B函数的真实地址。但现在填入的是指向了B@plt + 0x6的位置，这是为了传递参数给_dl_runtime_resolve函数。B@plt+0x6的位置其实就是B@plt函数的第二条指令，它的作用是将函数参数入栈，然后执行第三条指令jmp .plt再准备第二个参数。</p><p>我们再回到图中看看，在序号①箭头的位置，也就是第一级跳转，它的目的是把参数0入栈。由于GOT表的0x0，0x8，0x10的位置都被占用了，所以参数0代表的就是0x18位置，这就是B函数的真实地址应该存放的地方。</p><p>然后在序号②箭头的位置，发生了第二级跳转，这一次是为了把动态库的ID号压栈传参。</p><p>最后在序号③箭头的位置，继续进行第三级跳转，这一次跳转才真正地调用到了_dl_runtime_resolve。调用完这个方法以后，B函数的真实地址就会被填入GOT表中了。</p><p>上述过程由于传参的需要而变成了多级跳转，但如果抛开因为传参而产生的两级跳转，你会发现它的基本结构与patch code技术如出一辙。</p><p>这样的跳转虽然麻烦，但有一个非常重要的优点，就是运行期间不会修改代码段的指令，所有的修改只涉及了GOT这个位于数据段的表里。我们在<a href=\"https://time.geekbang.org/column/article/431904\">第3节课</a>就已经介绍过，.code和.plt会被加载到内存的代码段(code segment)，它的权限是可读可执行，但不可写；上节课也讲了.got会被加载进数据段，它的权限是可读可写。我们现在介绍的<strong>多级跳转的延迟绑定技术的整个重定位过程最终只会修改GOT的0x18这一个位置，其他位置都不必发生变化</strong>。</p><p>当执行完了重定位过程以后，CPU再一次运行到main里的call指令时，就能通过一次跳转就调用到真正的B函数了，这时的GOT已经与上节课所讲的加载时重定位后的GOT一模一样了。如图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/fb/11/fb2fb8ca742131711b040df30a452411.jpg?wh=2284x1742\" alt=\"\"></p><p>在这个图里，重定位完以后，只有红色字体的代码和数据是起作用的，.plt段里的其他代码就被“短路”掉了。这时，GOT表的结构就与上节课所讲的加载时重定位的情况完全一样了。<strong>只有用到的符号才会被重定位，这就是延迟绑定技术</strong>。未被用到的符号在加载时被重定位，这是一种浪费，而延迟绑定技术避免了这种浪费。为了加深理解，我结合一个具体例子向你展示延迟绑定是怎么实现的。</p><h2>延迟绑定技术的具体实现</h2><p>下面我们还是根据上节课的例子来看一下延迟绑定技术的具体实现。</p><pre><code>// foo.c\n\nstatic int static_var;\nint global_var;\nextern int extern_var;\nextern int extern_func();\n\nstatic int static_func() {\n    return 10;\n}\n\nint global_func() {\n    return 20;\n}\n\nint demo() {\n    static_var = 1;\n    global_var = 2;\n    extern_var = 3;\n    int ret_var = static_var + global_var + extern_var;\n    ret_var += static_func();\n    ret_var += global_func();\n    ret_var += extern_func();\n    return ret_var;\n}\n</code></pre><p>我们将这个例子编译成 libfoo.so ，编译命令是：</p><pre><code>$ gcc foo.c -fPIC -shared -o libfoo.so\n</code></pre><p>这里跟上节课编译的区别是，去掉了-fno-plt的编译选项，这样可以打开PLT表的生成。上一节课里，我们只需要关注PIC技术的实现，因此需要通过 -fno-plt 的选项来关闭PLT表的生成。</p><p>我们先通过反汇编先来看一下 demo 函数的汇编指令：</p><pre><code>00000000000006a0 &lt;demo&gt;:\n ...\n 6fd:   e8 7e fe ff ff          callq  580 &lt;global_func@plt&gt;\n 702:   01 45 fc                add    %eax,-0x4(%rbp)\n 705:   b8 00 00 00 00          mov    $0x0,%eax\n 70a:   e8 81 fe ff ff          callq  590 &lt;extern_func@plt&gt;\n 70f:   01 45 fc                add    %eax,-0x4(%rbp)\n 712:   8b 45 fc                mov    -0x4(%rbp),%eax\n 715:   c9                      leaveq\n 716:   c3                      retq \n</code></pre><p>从汇编中你可以看到，对函数global_func和extern_func的调用都变成了对global_func@plt和extern_func@plt的调用。继续查看这两个带@plt后缀的函数，其对应的VMA分别是0x580和0x590，所以接着看这两个位置的汇编代码。</p><pre><code>Disassembly of section .plt:\n\n0000000000000570 &lt;.plt&gt;:\n 570:   ff 35 92 0a 20 00       pushq  0x200a92(%rip)        # 201008 &lt;_GLOBAL_OFFSET_TABLE_+0x8&gt;\n 576:   ff 25 94 0a 20 00       jmpq   *0x200a94(%rip)        # 201010 &lt;_GLOBAL_OFFSET_TABLE_+0x10&gt;\n 57c:   0f 1f 40 00             nopl   0x0(%rax)\n\n0000000000000580 &lt;global_func@plt&gt;:\n 580:   ff 25 92 0a 20 00       jmpq   *0x200a92(%rip)        # 201018 &lt;global_func+0x200983&gt;\n 586:   68 00 00 00 00          pushq  $0x0\n 58b:   e9 e0 ff ff ff          jmpq   570 &lt;.plt&gt;\n\n0000000000000590 &lt;extern_func@plt&gt;:\n 590:   ff 25 8a 0a 20 00       jmpq   *0x200a8a(%rip)        # 201020 &lt;extern_func&gt;\n 596:   68 01 00 00 00          pushq  $0x1\n 59b:   e9 d0 ff ff ff          jmpq   570 &lt;.plt&gt;\n</code></pre><p>这段汇编是对libfoo.so中.plt段的反汇编。从这里我们可以看出来，PLT表的每一项其实都是一段相似的stub代码构成，这个stub共三条指令，这三条指令和我们上面的图中所画的是完全一样的。</p><p>从反汇编的结果来看，global_func@plt的第一行是一个间接跳转，跳转的目标地址存储在0x201018这个位置，通过objdump我们可以找到这个位置位于.got.plt段里。这个命令我们已经很熟悉了，你可以自己动手试一下。从名字中可以看出，.got.plt段跟.got段是一样的，存放的是GOT表，只不过.got.plt里边的GOT表是为PLT表准备的。</p><p>在这里 0x201018的位置存放的值是 0x586。这就跳回到global_func@plt里继续执行了，这是我们上面所分析的一级跳，是为了传递参数给符号解析函数的。最终经过传参，跳转，控制流才终于进入到dl_runtime_resolve中解析符号并做重定位。</p><p>最后，我们再总结一下GOT表中的各个表项的含义。</p><ol>\n<li>GOT.PLT[0]位置被加载器保留，它里面存放的是.dynamic段的地址，这里我们不用关心。</li>\n<li>GOT.PLT[1]位置存放的是当前so的ID，这个ID是加载器在加载当前动态库文件的时候分配的。</li>\n<li>GOT.PLT[2]位置存放的是动态链接函数的入口地址，一般是动态链接器中的_dl_runtime_resovle函数。这个函数的作用是找到需要查找的符号地址，并最终回填到GOT.PLT表的对应位置。</li>\n</ol><p>然后再回顾一下延迟绑定的整个过程。</p><ol>\n<li>当 demo 函数想要调用 global_func 的时候，程序调用先进入 global_func@plt 中；</li>\n<li>在 global_func@plt 中，会先执行 jmpq *GOT.PLT[3] ，此时 GOT.PLT[3] 里存放的是 global_func@plt 项中的第二条指令，因此控制流继续返回到 global_func@plt 中进行执行；</li>\n<li>接下会把数值0x0进行压栈，这个数值代表了 global_func 的ID。然后jmp到 PLT[0] 的表项中进行执行；</li>\n<li>在 PLT[0] 中，继续将 GOT.PLT[1] 的值也就是库文件的ID进行压栈，然后通过 GOT.PLT[2] 跳转到 _dl_runtime_resolve 函数中；</li>\n<li>_dl_runtime_resolve 则根据存在栈上的函数ID和so的ID进行全局搜索，找到对应的函数地址之后就可以将其重新填充到 GOT.PLT[3] 中，这个时候延迟加载的整个过程就完成了；</li>\n<li>当下一次调用 global_func 的时候，CPU就可以通过 global_func@plt 中第一条指令 jmpq *GOT.PLT[3] 直接跳转到 global_func 的真实地址中。</li>\n</ol><p>到这里，我们对动态链接中PIC技术和延迟加载技术进行了深入的分析。这个过程中我们几次提到动态链接器，但一直没有展开说，接下来我们就来揭开动态链接器的神秘面纱。</p><h2>Loader的加载机制</h2><p>虽然我们已经搞清楚了链接的全部流程。不过还缺了最后一环，就是可执行文件和共享库文件是如何被加载的？</p><p>在Linux下，编译一个最简单的可执行程序，通过 ldd a.out 命令你会发现有一个特殊的共享库文件：ld-linux-x86-64.so。从名字上可以看出，这个ld-linux.so跟链接器ld应该是存在某种联系的。</p><p>动态链接会把不同模块之间，符号重定位的操作，推迟到程序运行的时候，而ld-linux.so就负责这个工作。所以<strong>我们经常称ld.so为动态链接器，又因为它还负责加载动态库文件，所以我们有时也叫它loader，或者加载器。</strong></p><p>我们知道，一个完全静态链接的可执行文件则不需要动态链接器的辅助，所以内核加载完之后可以直接跳转到用户代码的入口中进行执行。内核加载的过程主要是打开文件，初始化进程空间，读磁盘加载文件数据等等，这部分工作不是我们关心的重点，所以就不再分析了。</p><p>而对于一个需要动态链接的可执行文件a.out，当我们在Linux的shell终端里边敲了./a.out的命令后，内核会先准备好可执行文件需要的环境，然后依次把a.out和ld-linux.so加载到内存中，下一步就是跳转到 ld-linux.so的入口函数中。</p><p>进入ld-linux.so以后，与上文所讲的内核的文件加载过程就有区别了。它已经不是内核态执行，而是用户态执行了。ld-linux.so的源码实际上是在glibc里边，主要实现都是在glibc的elf文件夹下。</p><p>ld-linux.so做的事情主要有这么几件：第一是启动动态链接器；第二是根据可执行文件的动态链接信息，寻找并加载可执行文件依赖的.so文件；第三步是跟静态链接器一样，对所有的符号进行解析和重定位；最后会根据so的情况来依次执行各个so的init函数。</p><ul>\n<li><strong>启动动态链接器</strong></li>\n</ul><p>在第一点中，你可能会问，加载跟启动动态链接器的事情不是已经在内核里边做过了么？这里启动动态链接器是在做什么呢？</p><p>我们知道，动态链接器的作用是用来对可执行文件中需要动态链接的这些全局符号进行重定位解析，填写GOT表等，这时候你会发现，ld-linux.so本身也是一个共享文件，那它自己的动态链接的过程是谁来进行呢？</p><p>答案就是自己。<strong>ld-linux.so在启动之后，首先需要完成自己的符号解析和重定位的过程，这个过程叫做动态链接器的自举(Bootstrap)</strong>。ld-linux.so中的整个自举过程的代码是需要非常小心翼翼的，因为此时ld-linux.so本身的GOT/PLT信息都未完成，所以在自举过程中的代码不能使用全局符号和外部符号，稍有不慎就会导致整个程序崩溃。你可以到elf/rtld.c中看一下这块代码，主要逻辑在_dl_start函数里。</p><ul>\n<li><strong>加载依赖共享文件</strong></li>\n</ul><p>完成自举后，ld-linux.so就可以放心的使用各种全局符号和外部符号了。接下来第二步是根据可执行文件的.dynamic段信息依次加载程序依赖的共享库文件。<strong>程序的共享库依赖关系往往是一个图的关系，所以这里在加载共享库的过程也相当于是图遍历的过程，这里往往采用的是广度优先搜索的算法来遍历</strong>。</p><p>在<a href=\"https://time.geekbang.org/column/article/436308\">第6节课</a>，我们讲过静态链接，在链接的过程中需要维护一个全局的符号表，遍历.o文件的时候不断收集文件中的符号并且合并到全局符号表中。</p><p>同样的，ld-linux.so在加载共享文件的过程中也会维护一个全局符号表，每次加载新的共享文件后，将共享文件中的符号信息合并到全局符号表中。这个时候，问题来了：如果两个不同的so，如libfoo1.so与libfoo2.so都定义了一个foo函数，那ld-linux.so加载这两个so的时候会发生什么？</p><p>在静态链接的过程中，如果不同的.o里边定义了相同的符号，这时链接器会报出redefine的错误。而ld-linux.so的执行策略则是不同的，ld-linux.so在碰到相同的符号时，只会将第一次碰到的符号添加到全局符号表中，而后续碰到重名的符号就被自动忽略。</p><p>这样导致的结果是，不同so的同名函数，在运行时能看到的只有加载顺序在前的函数定义。所以对于上面的问题而言，如果libfoo1.so依赖在前，那么最终运行时只能看到libfoo1.so的foo函数，即使是libfoo2.so里的函数调用foo，调用的也是libfoo1.so里的foo，而不是自己so的foo。由此我们在开发过程中一定需要注意不同so中符号重名的问题，否则就会碰到意想不到的问题。</p><ul>\n<li><strong>符号重定位与解析</strong></li>\n</ul><p>在完成了共享文件的加载之后，全局符号表的信息就收集完成了，这时ld-linux.so就可以根据全局符号表和重定位表的信息依次对各个so和可执行文件进行重定位修正了。<strong>这个过程跟静态链接中重定位的过程类似</strong>，你可以自己去分析一下。</p><ul>\n<li><strong>init函数调用</strong></li>\n</ul><p>最后，有的so文件还会有.init段，进行一些初始化函数的调用，例如so中全局变量的对象构造函数，或者用户自己生成在.init段的初始化函数等。这些都会由ld-linux.so在最后的阶段进行一次调用。当这些完成之后，ld-linux.so就会结束自己的使命，最终将程序的控制流转到可执行文件的入口函数中进行。</p><p>整个Loader加载动态链接的可执行文件流程如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/fb/ca/fb26425534d40bcec723f2716cb42eca.jpg?wh=2284x1280\" alt=\"\"></p><h2>总结</h2><p>我们通过三节课的学习，弄明白了“将符号转成地址”这个工作是由谁、在何时、如何完成的。</p><p>编译器在把源代码翻译成汇编指令的过程中，由于不知道其他编译单元的符号的真实地址，在引用这些符号的时候只能使用占位符（通常是0）来代替。这些占位符由链接器填充。当链接器把所有的符号的位置都确定好以后，再把真实地址回填到占位符里，这个过程就是重定位。</p><p>重定位的时机有三个，分别是编译期重定位（<a href=\"https://time.geekbang.org/column/article/436308\">第6节课</a>），加载期（<a href=\"https://time.geekbang.org/column/article/437653\">第7节课</a>）和这节课介绍的运行时重定位。</p><p>这节课我们先介绍了patch code技术，它被采用了即时编译的语言虚拟机广泛地使用。它可以做到运行时解析符号。它的主要原理是把call指令的目标地址填成用于解析符号的函数地址，当CPU执行到这个call指令时就会转去解析函数，然后把call指令的目标地址替换成符号的真实地址。</p><p>patch code技术有一个缺点，那就是在运行期要修改代码段的数据，这为系统带来了风险。动态链接库则引入了.plt和.got段，通过间接调用来解决这个问题。在运行时，符号解析函数只需要修改GOT的内容就可以了，代码段是不会发生任何变化的。</p><p>当然，因为要向符号解析函数传递参数，所以动态库的.plt设计成了三级跳转的结构，看上去虽然很复杂，但我们只需要牢牢记住.plt最终的目标还是调用到符号解析函数，然后重写GOT表的内容即可。</p><p>我们这两节课的内容都是动态链接，而真正负责动态链接的是ld-linux.so，它被称为动态链接器，但因为它还负责加载文件工作，所以也被人称为加载器或者loader。它的工作流程主要有启动，加载，重定位和init四个步骤。</p><p>链接与加载还有很多细节，但我已经带你建立起了基本的知识框架。如果对链接和加载还有更浓厚的兴趣，你可以参考<a href=\"https://book.douban.com/subject/3652388/\">《程序员的自我修养 》</a>，<a href=\"https://book.douban.com/subject/4083265/\">《链接器和加载器 》</a>等书，以便了解更多的相关结构和算法。</p><h2>思考题</h2><p>相信你已经完全理解了动态链接器的时机和原理了，那么请你思考一下：在生成一个动态库文件的时候，我们一定要加shared选项，但-fPIC选项是必然要加的吗？有没有不需要用这个选项的情况呢？如果没有，为什么？如果有的话，又是什么情况呢？欢迎你在留言区分享你的想法和收获，我在留言区等你。</p><p><img src=\"https://static001.geekbang.org/resource/image/9e/a2/9e53f01990e33e735ff21a751e23eaa2.jpg?wh=2284x1386\" alt=\"\"></p><p>好啦，这节课到这就结束啦。欢迎你把这节课分享给更多对计算机内存感兴趣的朋友。我是海纳，我们下节课再见！</p>","float_qrcode":"https://static001.geekbang.org/resource/image/83/1c/83e244537b714e899fd2e7896e22371c.png","article_cover_hidden":false,"is_required":true,"score":"31636473600","like_count":5,"article_subtitle":"","audio_download_url":"https://static001.geekbang.org/resource/audio/68/f8/68058725f99a5f67cbyy270d943765f8.mp3","had_viewed":false,"article_title":"08 | 动态链接（下）：延迟绑定与动态链接器是什么？","column_bgcolor":"#F6F7FB","offline_package":"{}","audio_title":"11.7 海纳 08_dynamic_link_plt_R.mp3","audio_time_arr":{"m":"22","s":"56","h":"00"},"text_read_percent":0,"cid":450,"article_cshort":"<p>你好，我是海纳。</p><p>在上节课里，我们学习了动态链接过程的基本原理。动态链接通过GOT表加一层间接跳转的方式，解决了代码中call指令对绝对地址的依赖，从而实现了PIC的能力。我们同时也讲到了GOT表中的地址是由加载器在加载时填充的。</p><p>不过，细心的你也发现了，动态链接带来的代价是性能的牺牲。这里性能的牺牲主要来自于两个方面：</p><ol>\n<li>每次对全局符号的访问都要转换为对GOT表的访问，然后进行间接寻址，这必然要比直接的地址访问速度慢很多；</li>\n<li>动态链接和静态链接的区别是将链接中重定位的过程推迟到程序加载时进行。因此在程序启动的时候，动态链接器需要对整个进程中依赖的so进行加载和链接，也就是对进程中所有GOT表中的符号进行解析重定位。这样就导致了程序在启动过程中速度的减慢。</li>\n</ol><p>我们这节课来看看，如何通过延迟绑定技术，来解决性能下降的问题。延迟绑定不仅仅是用在动态链接中，还被广泛地应用在Hotspot，V8等带有即时编译功能的虚拟机中。另外，在游戏行业，修复服务器的错误的同时保证用户不掉线是硬需求，这种不停机进行代码修复的技术被称为热更新技术。学习完这节课后，你不仅能理解动态链接的基本原理，而且也能对热更新的基本原理有所感悟。</p>","video_width":0,"column_could_sub":true,"video_id":"","sku":"100094901","video_cover":"","author_name":"海纳","column_is_onboard":true,"audio_url":"https://res001.geekbang.org/media/audio/68/f8/68058725f99a5f67cbyy270d943765f8/ld/ld.m3u8","chapter_id":"2314","column_had_sub":true,"column_cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg","neighbors":{"left":{"article_title":"07 | 动态链接（上）：地址无关代码是如何生成的？","id":437653},"right":{"article_title":"09 | 深入理解堆：malloc和内存池是怎么回事？","id":440452}},"rate_percent":0,"footer_cover_data":{"img_url":"https://static001.geekbang.org/resource/image/f0/fe/f0c47daf3262a8df6cf352d89896abfe.png","link_url":"https://time.geekbang.org/article/427012","mp_url":""},"float_app_qrcode":"","column_is_experience":false,"rate":{"1":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"2":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"3":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0}},"product_id":100094901,"had_liked":false,"id":440471,"free_get":false,"is_video_preview":false,"article_summary":"我们这节课来看看，如何通过延迟绑定技术，来解决性能下降的问题。","column_sale_type":0,"float_qrcode_jump":"https://time.geekbang.org/serv/v4/misc/jump?uri=https%3A%2F%2Ftime.geekbang.org%2Fhybrid%2Fmp%2Fjump%3Furl%3Dhttps%253A%252F%252Fstatic001.geekbang.org%252Fresource%252Fimage%252Fee%252Fb2%252Feedd4090668aabfac91b2b445d27cbb2.png","column_id":450,"article_sharetitle":"08 | 动态链接（下）：延迟绑定与动态链接器是什么？","article_poster_wxlite":"https://static001.geekbang.org/render/screen/ec/97/ecc3ca709d6c910cac2d3bfb35759c97.jpeg","article_features":0,"comment_count":7,"audio_md5":"68058725f99a5f67cbyy270d943765f8","offline":{"size":22461426,"file_name":"cb04e00b08d4d46cc133f162a34de6e1","download_url":"https://static-acl-001.geekbang.org/resource/zip/article/440471/cb04e00b08d4d46cc133f162a34de6e1.zip?auth_key=1641482173-9f2d54efe9bc4ee3b314beb83b1afab1-0-4621bf45e02f2bf0c6556ce18b58e3ab"},"video_size":0,"hls_videos":[],"video_time":"","article_could_preview":false,"article_ctime":1636473600,"share":{"content":"学好内存，掌握系统开发诀窍","title":"海纳 · 编程高手必学的内存知识","poster":"https://static001.geekbang.org/resource/image/bc/c5/bcaf3ff69e538f8749970edf2301bbc5.jpg","cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg"}},"444178":{"text_read_version":0,"audio_size":21948557,"article_cover":"https://static001.geekbang.org/resource/image/3c/c3/3c824532d44eff45f58ace3284d027c3.jpg","subtitles":[],"product_type":"c1","audio_dubber":"海纳","is_finished":false,"like":{"had_done":false,"count":2},"audio_time":"00:22:51","video_height":0,"article_content":"<p>你好，我是海纳。</p><p>这节课是对前面所有课程的一次总结和回顾。前面我们介绍了很多内存管理的相关机制，其实都是为了把这节课的故事讲完整。在前面的课程里，我们了解了进程内部的分布，但也留下了三个关键的问题没有讲清楚：</p><ol>\n<li>fork的工作方式非常奇怪，一方面父进程和子进程还可以访问共有的变量，另一方面，它们又可以各自修改这个变量，且这个修改对方都看不见，这是怎么做到的呢？</li>\n<li>我们在<a href=\"https://time.geekbang.org/column/article/430073\">第1节课</a>讲内存映射时，就讲过页表中未映射状态的页表项，并不存在一块具体的物理内存与之对应。但是当我们访问到这一页的时候，页表项可以自动变成已映射的正常状态。谁在背后做了什么事情呢？</li>\n<li>mmap的功能十分强大，这些强大的能力是怎么完成的呢？</li>\n</ol><p>这三个问题，虽然看上去相互之间关系不大，但实际上它们背后都依赖<strong>页中断机制</strong>。</p><p>页中断和普通的中断一样，它的中断服务程序入口也在IDT中（<a href=\"https://time.geekbang.org/column/article/431400\">第2节课</a>的内容），但它是由MMU产生的硬件中断。<strong>页中断有两类重要的类型：写保护中断和缺页中断。正是这两类中断在整个系统的后台默默地工作着，就像守护神一样支撑着内存系统正常工作</strong>。</p><p>大多数时候，我们即使不知道它们的存在，程序也能正常地运行。但是有时候，程序写得不好就有可能造成中断频繁发生，从而带来巨大的性能下降。面对这种情况，我们第一时间就应该想到统计页中断。因为除了页中断本身会带来性能下降之外，统计页中断也可以反推程序的运行特点，从而为进一步分析程序瓶颈点，提供数据和思路。</p><!-- [[[read_end]]] --><p>讲到这，我想你应该意识到掌握页中断的必要性了，其实这也是我们这节课的学习目标，同时我们还将借此解决上面提到的三个问题。好，不啰嗦了，我们先了解下页中断有哪些类型吧。</p><h2>页中断有哪些类型？</h2><p>在之前的课程里，我们介绍了页表映射的原理，也提到过页表项里定义了页的读写属性等等。如果物理页不在内存中，或者页表未映射，或者读写请求不满足页表项内的权限定义时，MMU单元就会产生一次中断。</p><p>我们在<a href=\"https://time.geekbang.org/column/article/431400\">第2节课</a>中详细介绍了中断机制和IDT的结构，并且在介绍中断向量时提到过页中断的向量是14。所以，操作系统在启动以后，它会把处理页中断的程序入口地址，设置到IDT的14号中断描述符里。在Linux系统上，页中断服务程序的名称是do_page_fault。</p><p>当中断发生以后，CPU会自动地在栈里存放一个错误码，来区分页中断的类型，还会把发生页中断的虚拟地址放到CR2寄存器，这样，中断服务程序就可以清楚地知道是什么原因导致的中断，然后才能做出相应的处理。</p><p>根据中断来源的不同，页中断大致可以分为以下几种类型：<br>\n<img src=\"https://static001.geekbang.org/resource/image/0f/fb/0f11c2f9f54988728aacc57c461f3ffb.jpg?wh=2284x1574\" alt=\"\"></p><p>从这个表格里，你会发现，页中断服务程序根据不同的情况，兢兢业业地为整个系统的内存管理，默默做着贡献。接下来，我们就带着这节课开头提出的三个问题，来看看页中断是怎么工作的。我们先从第一个问题，fork的原理是什么开始吧。</p><h2>fork原理：写保护中断与写时复制</h2><p>我们前面说，父进程和子进程不仅可以访问共有的变量，还可以各自修改这个变量，并且这个修改对方都看不见。这其实是fork的一种写时复制机制，这一点我们在<a href=\"https://time.geekbang.org/column/article/435493\">第5节课</a>中模糊提到过，而里面起关键作用的就是写保护中断。下面我们来看看这到底是怎么一回事。</p><p>实际上，操作系统为每个进程提供了一个进程管理的结构，在偏理论的书籍里一般会称它为进程控制块（Process Control Block，PCB)。具体到Linux系统上，PCB就是task_struct这个结构体。它里面记录了进程的页表基址，打开文件列表、信号、时间片、调度参数和线性空间已经分配的内存区域等等数据。</p><p>其中，<strong>描述线性空间已分配的内存区域的结构对于内存管理至关重要</strong>，我们先来看一下这个结构。在Linux源码中，负责这个功能的结构是vm_area_struct，后面简称vma。内核将每一段具有相同属性的内存区域当作一个单独的内存对象进行管理。vma中比较重要的属性我列在下面：</p><pre><code>struct vm_area_struct { \n\tunsigned long vm_start;      // 区间首地址\n\tunsigned long vm_end;        // 区间尾地址\n    pgprot_t      vm_page_prot;  // 访问控制权限\n    unsigned long vm_flags;      // 标志位\n    struct file * vm_file;       // 被映射的文件\n    unsigned long vm_pgoff;      // 文件中的偏移量\n\t...\n}\n</code></pre><p><strong>在操作系统内核里，fork的第一个动作是把PCB复制一份，但类似于物理页等进程资源不会被复制</strong>。这样的话，父进程与子进程的代码段、数据段、堆和栈都是相同的，这是因为它们拥有相同的页表，自然也有相同的虚拟空间布局和对物理内存的映射。如果父进程在fork子进程之前创建了一个变量，打开了一个文件，那么父子进程都能看到这个变量和文件。</p><p><strong>fork的第二个动作是复制页表和PCB中的vma数组，并把所有当前正常状态的数据段、堆和栈空间的虚拟内存页，设置为不可写，然后把已经映射的物理页面的引用计数加1</strong>。这一步只需要复制页表和修改PTE中的写权限位可以了，并不会真的为子进程的所有内存空间分配物理页面，修改映射，所以它的效率是非常高的。这时，父子进程的页表的情况如下图所示：<br>\n<img src=\"https://static001.geekbang.org/resource/image/b2/99/b251f885955dd063df54f5e452f61999.jpg?wh=2284x1230\" alt=\"\"></p><p>在上图中，物理页括号中的数字代表该页被多少个进程所引用。Linux中用于管理物理页面，和维护物理页的引用计数的结构是mem_map和page struct。</p><p>这两个动作执行完后，fork调用就结束了。此时，由于有父进程和子进程两个PCB，操作系统就会把两个进程都加入到调度队列中。当父进程得到执行，它的IP寄存器还是指向fork调用中，所以它会从这个调用中返回，只不过返回值是子进程的PID。当子进程得到执行时，它的IP寄存器也是停在fork调用中，它从这个调用中返回，其返回值是0。</p><p>接下来，就是写保护中断要发挥作用的地方了。不管是父进程还是子进程，它们接下来都有可能发生写操作，但我们知道在fork的第二步操作中，已经将所有原来可写的地方都变成不可写了，所以这时必然会发生写保护中断。</p><p>我们刚才说，Linux系统的页中断的入口地址是do_page_fault，在这个函数里，它会继续判断中断的类型。由于发生中断的虚拟地址在vma中是可写的，在PTE中却是只读的，可以断定这是一次写保护中断。这时候，内核就会转而调用do_wp_page来处理这次中断，wp是write protection的缩写。</p><p>在do_wp_page中，系统会首先判断发生中断的虚拟地址所对应的物理地址的引用计数，如果大于1，就说明现在存在多个进程共享这一块物理页面，那么它就需要为发生中断的进程再分配一个物理页面，把老的页面内容拷贝进这个新的物理页，最后把发生中断的虚拟地址映射到新的物理页。这就完成了一次写时复制(Copy On Write， COW）。具体过程如下图所示：<br>\n<img src=\"https://static001.geekbang.org/resource/image/73/57/739cbddb3d1e64ab203ecf55b9c9ce57.jpg?wh=2284x1175\" alt=\"\"></p><p>在上图中，当子进程发生写保护中断后，系统就会为它分配新的物理页，然后复制页面，再修改页表映射。这时老的物理页的引用计数就变为1，同时子进程中的PTE的权限也从只读变为读写。</p><p>当父进程再访问到这个地址时，也会触发一次写保护中断，这时系统发现物理页的引用计数为1，那就只要把父进程PTE中的权限，简单地从只读变为读写就可以了。这个过程比较简单，我就不画图了，你可以自己思考一下。</p><p>fork之后如果要执行新的程序，那么就需要执行execve这个系统调用。它的主要作用是加载可执行程序并运行。接下来我们就看看这个函数背后的故事。</p><h2>execve原理：缺页中断</h2><p>接着来说这节课开始时所提到的第二个问题，未映射页面是如何自动变成正常页面的？我们将通过execve的例子来进行分析。</p><p>execve的作用是使当前进程执行一个新的可执行程序，它的原型如下所示：</p><pre><code>#include &lt;unistd.h&gt;\n\nint execve(const char* filename, const char* argv[],\n          const char* envp[])\n</code></pre><p>其中execve的第一个参数是可执行程序的文件名，第二个参数用来传递命令行参数，第三个参数用来传递环境变量。</p><p>execve的执行步骤如下所示：</p><ol>\n<li>清空页表，这样整个进程中的页都变成不存在了，一旦访问这些页，就会发生页中断；</li>\n<li>打开待加载执行的文件，在内核中创建代表这个文件的struct file结构；</li>\n<li>加载和解析文件头，文件头里描述了这个可执行文件一共有多少section；</li>\n<li>创建相应的vma来描述代码段，数据段，并且将文件的各个section与这些内存区域建立映射关系；</li>\n<li>如果当前加载的文件还依赖其他共享库文件，则找到这个共享库文件，并跳转到第2步继续处理这个共享库文件；</li>\n<li>最后跳转到可执行程序的入口处执行。</li>\n</ol><p>我们在<a href=\"https://time.geekbang.org/column/article/431904\">第3节课</a>讲了section与内存中的segment的对应关系。<strong>execve的实现并不负责将文件内容加载到物理页中，它只建立了这种文件section，与内存区域的映射关系就结束了</strong>。真正负责加载文件内容的是缺页中断，接下来，我们就看看缺页中断是如何加载物理页的。</p><p>在execve的执行步骤中，我们讲了，内核为可执行程序创建一个vma结构体实例，然后将它的vm_file属性设成第2步所打开的文件，这就建立起了内存区域和文件的映射关系。这个内核区域的区间首地址、区间尾地址和控制权限，都是由第3步解析的信息决定的。例如.text段被加载到的内存首地址，也就是链接时所决定的起始地址，它就决定了内存代码段的起始地址。</p><p>由于第1步把页表都清空了，这就导致CPU在加载指令时会发现代码段是缺失的，此时就会产生缺页中断。</p><p>Linux内核用于处理缺页中断的函数是do_no_page，如果内核检查，当前出现缺页中断的虚拟地址所在的内存区域vma（虚拟地址落在该内存区域的vm_start和vm_end之间）存在文件映射(vm_file不为空），那就可以通过虚拟内存地址计算文件中的偏移，这就定位到了内存所缺的页对应到文件的哪一段。然后内核就启动磁盘IO，将对应的页从磁盘加载进内存。一次缺页中断就这样被解决了。</p><p>到这里，第二个问题的答案你就都搞清楚了。<strong>可执行程序的加载不是一次性完成的，而是由缺页中断根据需要，将文件的内容以页为单位加载进内存的，一次只会加载一页</strong>。</p><p>搞清楚了execve背后的原理，我们再来分析mmap的原理，你就很容易理解了，因为它背后的机制仍然是围绕着vm_area_struct这个核心结构，由页中断来完成各种功能。</p><h2>mmap强大的能力是怎么来的？</h2><p>在回答这节课开始提出的第三个问题，也就是mmap的功能十分强大，这些强大的能力是怎么完成的前，我们先回顾下<a href=\"https://time.geekbang.org/column/article/431904\">第3节课</a>的内容，mmap根据映射的类型，有四种最常用的组合：</p><ul>\n<li><strong>私有匿名映射，用于分配堆空间；</strong></li>\n<li><strong>共享匿名映射，用于父子进程之间通讯；</strong></li>\n<li><strong>私有文件映射，用于加载动态链接库；</strong></li>\n<li><strong>共享文件映射，用于多进程之间通讯。</strong></li>\n</ul><p>我们接下来针对这四种情况依次进行分析。</p><h3>私有匿名映射</h3><p>私有匿名映射是最简单的情况，<strong>在调用mmap时，只需要在文件映射区域分配一块内存，然后创建这块内存所对应的vma结构，这次调用就结束了</strong>。</p><p>当访问到这块虚拟内存时，由于这块虚拟内存都没有映射到物理内存上，就会发生缺页中断，但这一次的缺页中断与execve时的缺页中断不一样，这次是匿名映射，所以关联文件属性为空。此时，内核就会调用do_anonymous_page来分配一个物理内存，并将整个物理页全部初始化为0，然后在页表里建立起虚拟地址到物理地址的映射关系。</p><h3>私有文件映射</h3><p>在内核中，如果有一个进程打开了一个文件，PCB中就会有一个struct file结构与这个文件对应。struct file结构是与进程相关，假如进程A与进程B都打开了文件f，那么进程A中就会有一个struct file结构，进程B中也会有一个。</p><p>Linux的文件系统中有一个叫做inode的结构，这个结构与具体的磁盘上的文件是一一对应的，也就是说对于同一个文件，整个内核中只会有一个inode结构。所以进程A与进程B的file struct结构都有一个指针指向inode结构，这就将file struct与inode结构联系起来了。</p><p>在inode结构中，有一个哈希表，以文件的页号为key，以物理内存页为value。当进程A打开了文件f，然后读取了它的第4页，这时，内核就会把4和这个物理页，放入这个哈希表中。当进程B再打开文件f，要读取它的第4页时，因为f的第4页的内容已经被加载到物理页中了，所以就不用再加载一次了。只需要将B的虚拟地址与这个物理页建立映射就可以了，如下图所示：<br>\n<img src=\"https://static001.geekbang.org/resource/image/63/df/63c6871c463136c98561489a2dfb09df.jpg?wh=2284x1110\" alt=\"\"></p><p>我要提醒你的是，<strong>哈希表在现代的Linux内核中，已经被优化成了Radix tree和最小堆的一种优化的数据结构，它们比哈希表有更好的时间效率，所以你在阅读不同版本的Linux内核代码时要注意这个变化</strong>。</p><p>如果文件是只读的话，那这个文件在物理页的层面上其实是共享的。也就是进程A和进程B都有一页虚拟内存被映射到了相同的物理页上。但如果要写文件的时候，因为这一段内存区域的属性是私有的，所以内核就会做一次写时复制，为写文件的进程单独地创建一份副本。这样，一个进程在写文件时，并不会影响到其他进程的读。</p><p>对于共享库文件，代码段的私有属性其实并不影响它在所有进程间共享；但如果数据段在执行的过程发生变化，内核就可以通过写时复制机制为每个进程创建一个副本。这就是对于共享库文件要选择私有文件映射的根本原因。</p><p>这里我们就有这样一个结论：<strong>私有文件映射的只读页是多进程间共享的，可写页是每个进程都有一个独立的副本，创建副本的时机仍然是写时复制</strong>。</p><h3>共享文件映射</h3><p>在私有文件映射的基础上，共享文件映射就很简单了：<strong>对于可写的页面，在写的时候不进行复制就可以了</strong>。这样的话，无论何时，也无论是读还是写，多个进程在访问同一个文件的同一个页时，访问的都是相同的物理页面。</p><h3>共享匿名映射</h3><p>在这节课之前，你可能会觉得共享匿名映射在父子进程间通讯是最简单的，因为父子进程共享了相同的mmap的返回值，看上去最直观。但实际上，从内核的角度说，它却是最复杂的。</p><p>原因是<strong>mmap并不真正分配物理内存，它只是分配了一段虚拟内存，也就是说只在PCB中创建了一个vma结构而已。这就导致fork在复制页表的时候，页表中共享匿名映射区域都是未映射状态</strong>。</p><p>请你设想一下，如果内核不做特殊处理的话，在父进程因为访问共享内存区域而遇到缺页中断时，内核为它分配了物理页面，等子进程再访问共享内存区域时，内核也没有办法知道子进程的虚拟内存，应该映射到哪个物理页面上，因为缺页中断只能知道当前进程是谁，以及发生问题的虚拟地址是什么，这些信息不足够计算出，是否有其他进程已经把共享内存准备好了。</p><p>在内核中使用虚拟文件系统来解决这个问题之前，早期的Linux内核中并不支持共享匿名映射。虚拟文件并不是真实地在磁盘上存在的。它只是由内核模拟出来的，但是它也有自己的inode结构。这样一来，内核就能在创建共享匿名映射区域时，创建一个虚拟文件，并将这个文件与映射区域的vma关联起来。</p><p>当fork创建子进程时，子进程会复制父进程的全部vma信息。接下来发生的事情就和共享文件映射完全一样了，我们就不再重复了。</p><p>至此，我们才终于把mmap的核心原理分析清楚。第三个问题的答案也就很清楚了：<strong>mmap的功能之所以十分强大，主是因为操作系统综合使用写保护中断、缺页中断和文件机制来实现mmap的各种功能</strong>。</p><h2>总结</h2><p>这节课，我们先介绍了页中断产生的原因，大致可以分为缺页中断、写保护中断和非法访问造成的中断等等。</p><p>接下来，我们深入地分析了fork的原理。fork在执行时，子进程只会复制父进程的PCB和页表，并且把所有页表项都设为只读，这个过程并不会复制真正的物理页。只有当父子进程其中一个对页进行写操作的时候，才会复制一个副本出来。这种机制被称为写时复制。</p><p>execve是一种系统调用，用于加载并运行一个可执行文件。它会打开文件，并做好文件的section与内存segment的映射，这种映射关系维护在vm_area_struct中，然后就清空页表退出执行了。</p><p>当指令真正访问到内存的时候，由于页表被清空，这时会产生缺页中断，然后，内核就使用vma中的文件映射关系，去磁盘上读取相应的内容，将它放到物理页中，最后建立好虚拟地址到物理地址的映射。这是一种按需加载的机制。</p><p>我们分析了mmap背后的页中断原理，根据映射的类型，我们还介绍了它常用的4种组合和作用。其中：</p><ul>\n<li><strong>私有匿名映射</strong>，在缺页中断的处理过程，会通过do_anonymous_page函数申请一块全零的物理页，并建立虚拟地址到物理页的映射，以达成分配内存的目标；</li>\n<li><strong>私有文件映射</strong>，则借助文件的inode结构共享文件的物理缓存页，当发生写操作时，则会出现写时复制，从而保证每一个进程中都有自己的副本；</li>\n<li><strong>共享文件映射</strong><strong>，</strong>在私有文件映射的基础上，只取消了写时复制，这样一个进程就可以看到其他进程对这个页的修改了；</li>\n<li><strong>共享匿名映射</strong><strong>，</strong>借助了虚拟文件系统。内核在父子进程间，使用自己创建的虚拟文件和共享文件映射，来实现共享匿名映射。</li>\n</ul><p>最后，你要特别注意的一点是，Linux内核为了优化性能，还引入了大量的结构，这使得研究内存管理的源代码变得非常困难。我们这里主要介绍了设计思路，而不会涉及到具体的细节，如果想研究Linux内存管理的源码的话，你还可以继续参考<a href=\"https://book.douban.com/subject/1610233\">《Understanding the Linux Virtual Memory Manager 》</a>、<a href=\"https://book.douban.com/subject/1503819\">《Linux内核设计与实现》</a>和<a href=\"https://book.douban.com/subject/1767120\">《深入理解LINUX内核 》</a>等资料。</p><p>通过这节课的学习，我相信我已经帮你建立起了基本的骨架，填充更多细节的任务就交给你自己完成吧。</p><h2>思考题</h2><p>我们先将<a href=\"https://time.geekbang.org/column/article/431904\">第3节课</a>的mmap的例子，映射方式修改为MAP_PRIVATE。请你结合本节课所学的知识分析它的运行结果。欢迎你在留言区分享你的想法和收获，我在留言区等你。</p><pre><code>#include &lt;sys/mman.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;unistd.h&gt;\nint main() {\n    pid_t pid;\n    char* shm = (char*)mmap(0, 4096, PROT_READ | PROT_WRITE,\n        MAP_PRIVATE| MAP_ANONYMOUS, -1, 0);\n    if (!(pid = fork())){\n        sleep(1);\n        printf(&quot;child got a message: %s\\n&quot;, shm);\n        sprintf(shm, &quot;%s&quot;, &quot;hello, father.&quot;);\n        exit(0);\n    }\n    sprintf(shm, &quot;%s&quot;, &quot;hello, my child&quot;);\n    sleep(2);\n    printf(&quot;parent got a message: %s\\n&quot;, shm);\n    return 0;\n}\n</code></pre><p><img src=\"https://static001.geekbang.org/resource/image/32/f2/321edbbc5877bc44f4e5397c18847cf2.jpg?wh=2284x1370\" alt=\"\"></p><p>好啦，这节课到这就结束啦。欢迎你把这节课分享给更多对计算机内存感兴趣的朋友。我是海纳，我们下节课再见！</p>","float_qrcode":"https://static001.geekbang.org/resource/image/83/1c/83e244537b714e899fd2e7896e22371c.png","article_cover_hidden":false,"is_required":true,"score":"31636905600","like_count":9,"article_subtitle":"","audio_download_url":"https://static001.geekbang.org/resource/audio/19/70/1904c6dc60601f56c8b6f89fd9e86970.mp3","had_viewed":false,"article_title":"10 | 页中断：fork、mmap背后的保护神","column_bgcolor":"#F6F7FB","offline_package":"{}","audio_title":"11.13 海纳 10_page_fault.MP3_L.mp3","audio_time_arr":{"m":"22","s":"51","h":"00"},"text_read_percent":0,"cid":450,"article_cshort":"<p>你好，我是海纳。</p><p>这节课是对前面所有课程的一次总结和回顾。前面我们介绍了很多内存管理的相关机制，其实都是为了把这节课的故事讲完整。在前面的课程里，我们了解了进程内部的分布，但也留下了三个关键的问题没有讲清楚：</p><ol>\n<li>fork的工作方式非常奇怪，一方面父进程和子进程还可以访问共有的变量，另一方面，它们又可以各自修改这个变量，且这个修改对方都看不见，这是怎么做到的呢？</li>\n<li>我们在<a href=\"https://time.geekbang.org/column/article/430073\">第1节课</a>讲内存映射时，就讲过页表中未映射状态的页表项，并不存在一块具体的物理内存与之对应。但是当我们访问到这一页的时候，页表项可以自动变成已映射的正常状态。谁在背后做了什么事情呢？</li>\n<li>mmap的功能十分强大，这些强大的能力是怎么完成的呢？</li>\n</ol><p>这三个问题，虽然看上去相互之间关系不大，但实际上它们背后都依赖<strong>页中断机制</strong>。</p><p>页中断和普通的中断一样，它的中断服务程序入口也在IDT中（<a href=\"https://time.geekbang.org/column/article/431400\">第2节课</a>的内容），但它是由MMU产生的硬件中断。<strong>页中断有两类重要的类型：写保护中断和缺页中断。正是这两类中断在整个系统的后台默默地工作着，就像守护神一样支撑着内存系统正常工作</strong>。</p><p>大多数时候，我们即使不知道它们的存在，程序也能正常地运行。但是有时候，程序写得不好就有可能造成中断频繁发生，从而带来巨大的性能下降。面对这种情况，我们第一时间就应该想到统计页中断。因为除了页中断本身会带来性能下降之外，统计页中断也可以反推程序的运行特点，从而为进一步分析程序瓶颈点，提供数据和思路。</p>","video_width":0,"column_could_sub":true,"video_id":"","sku":"100094901","video_cover":"","author_name":"海纳","column_is_onboard":true,"audio_url":"https://res001.geekbang.org/media/audio/19/70/1904c6dc60601f56c8b6f89fd9e86970/ld/ld.m3u8","chapter_id":"2314","column_had_sub":true,"column_cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg","neighbors":{"left":{"article_title":"09 | 深入理解堆：malloc和内存池是怎么回事？","id":440452},"right":{"article_title":"11 | 即时编译：高性能JVM的核心秘密","id":445925}},"rate_percent":0,"footer_cover_data":{"img_url":"https://static001.geekbang.org/resource/image/f0/fe/f0c47daf3262a8df6cf352d89896abfe.png","link_url":"https://time.geekbang.org/article/427012","mp_url":""},"float_app_qrcode":"","column_is_experience":false,"rate":{"1":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"2":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"3":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0}},"product_id":100094901,"had_liked":false,"id":444178,"free_get":false,"is_video_preview":false,"article_summary":"页中断有两类重要的类型：写保护中断和缺页中断。正是这两类中断在整个系统的后台默默地工作着，就像守护神一样支撑着内存系统正常工作。","column_sale_type":0,"float_qrcode_jump":"https://time.geekbang.org/serv/v4/misc/jump?uri=https%3A%2F%2Ftime.geekbang.org%2Fhybrid%2Fmp%2Fjump%3Furl%3Dhttps%253A%252F%252Fstatic001.geekbang.org%252Fresource%252Fimage%252Fee%252Fb2%252Feedd4090668aabfac91b2b445d27cbb2.png","column_id":450,"article_sharetitle":"10 | 页中断：fork、mmap背后的保护神","article_poster_wxlite":"https://static001.geekbang.org/render/screen/e2/0b/e22651fa7c7860ae02c564e3ff39860b.jpeg","article_features":0,"comment_count":11,"audio_md5":"1904c6dc60601f56c8b6f89fd9e86970","offline":{"size":22104111,"file_name":"4498eb797260b8f5dfce8ded16132278","download_url":"https://static-acl-001.geekbang.org/resource/zip/article/444178/4498eb797260b8f5dfce8ded16132278.zip?auth_key=1641482205-4c96538f08aa4d5a81bb96e591ce9c81-0-8f52af8dad3f9ca1d3c64b8412ac6ed3"},"video_size":0,"hls_videos":[],"video_time":"","article_could_preview":false,"article_ctime":1636905600,"share":{"content":"学好内存，掌握系统开发诀窍","title":"海纳 · 编程高手必学的内存知识","poster":"https://static001.geekbang.org/resource/image/bc/c5/bcaf3ff69e538f8749970edf2301bbc5.jpg","cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg"}},"445925":{"text_read_version":0,"audio_size":19194540,"article_cover":"https://static001.geekbang.org/resource/image/52/ee/52752fea50f3487d850a285aebdf22ee.jpg","subtitles":[],"product_type":"c1","audio_dubber":"海纳","is_finished":false,"like":{"had_done":false,"count":2},"audio_time":"00:19:59","video_height":0,"article_content":"<p>你好，我是海纳。</p><p>在前面的课程里，我们讲解了进程内部的具体布局，以及每一个部分的功能和作用。你会发现，所有的例子都是用C/C++写的，我相信你在学习的过程中，心里可能会产生这样的疑问：<strong>那Java和Python语言是怎么运行起来的呢？</strong></p><p>有这个疑问非常合理。我曾经讲过C/C++编译的结果，它在linux上是ELF文件，在windows上是exe文件，这两种文件都可以直接被操作系统加载运行的二进制文件。另外，C/C++源代码也可以被编译成动态链接库文件。</p><p>而在Java语言里，程序员都知道Java源代码被javac编译以后，生成的是字节码文件，也就是class文件，而且不管编译所使用的操作系统是什么，相同的Java源码必然得到相同的class文件。class文件显然与上面C/C++编译的二进制文件都不相同，因为它与编译的平台无关。</p><p>这节课，我们就围绕着Java是怎么运行起来的这个问题逐层展开，在这个过程中，我会教你如何阅读和分析字节码，以及猜测它的JIT结果。所以通过这节课的学习，你不仅能了解到Java字节码的核心知识、JVM中的解释器和JIT编译器的原理，而且，还能进一步理解JVM虚拟机。在这个基础上，你就能写出更高效、对编译器更友好的程序，而且碰到桥接方法这一类Java中非常抽象和难以理解的概念时，也能着手分析。</p><!-- [[[read_end]]] --><p>既然Java源代码都被编译成了字节码文件，那我们就从Java字节码讲起吧。</p><h2>Java字节码</h2><p>我们先在某个目录下创建一个名为Main.java的Java源文件：</p><pre><code>import java.lang.Math;\n\npublic class Main {\n    public static double dist(double x1, double y1, double x2, double y2) {\n        double dist_x = x1 - x2;\n        double dist_y = y1 - y2;\n        return Math.sqrt(dist_x * dist_x + dist_y * dist_y);\n    }\n\n    public static void main(String[] args) {\n        System.out.println(dist(0.0f, 0.0f, 1.0f, 1.0f));\n    }\n}\n</code></pre><p>然后，在这个目录下执行：</p><pre><code>$ javac Main.java\n</code></pre><p>这时，我们会观察到目录下多了一个名为Main.class的文件，我们知道这个文件就是Java的字节码文件。只要选择的 javac 版本相同，无论这个实验是在windows系统上，还是在linux系统上，也无论它运行在x86平台，还是鲲鹏平台，得到的字节码文件都是相同的。</p><p>比如，我们可以使用java命令来执行这个字节码文件：</p><pre><code>$ java Main\n1.4142135623730951\n</code></pre><p>也可以使用javap命令来观察它的字节码：</p><pre><code>$ javap -c Main.class\n  public static double dist(double, double, double, double);\n    Code:\n       0: dload_0\n       1: dload         4\n       3: dsub\n       4: dstore        8\n       6: dload_2\n       7: dload         6\n       9: dsub\n      10: dstore        10\n      12: dload         8\n      14: dload         8\n      16: dmul\n      17: dload         10\n      19: dload         10\n      21: dmul\n      22: dadd\n      23: invokestatic  #2                  // Method java/lang/Math.sqrt:(D)D\n      26: dreturn\n</code></pre><p>这一段代码显示的是dist方法编译以后的字节码。字节码的格式是：</p><pre><code>op_code(1 byte), operand1(1 byte, optional)\n</code></pre><p>每条字节码都有自己的op_code，然后带有0个或者1个参数。每个op_code在class文件中都是一个无符号byte类型的整数，刚好占据一个字节，这也是Java虚拟机指令被称为字节码的原因。javap命令为了方便人的阅读，会将op_code翻译成文字助记符。</p><p>dist方法的字节码从第4行开始，第4行开头的数字0，代表的是这一条字节码的偏移量。dload_0具体是什么含义，我们这节课后面的内容中解释。这里我们只注意这条字节码占据的宽度是1，所以第二条字节码的偏移就是1。</p><p>第5行字节码开头的数字1，同样表示该字节码的偏移值，这一条字节码不同于上一条，它带有一个参数4。字节码的操作符和参数各占1个字节，所以这条字节码的长度就是2。可以推算下一条字节码的偏移值是当前字节码偏移值加当前字节码长度，也就是1+2=3。所以第6行的开头数字是3。</p><p>那么这些字节码究竟是怎么执行的呢？这就不得不提Java语言虚拟机了。<strong>通常，Java语言虚拟机包含两大核心模块：执行器和内存管理器，这里的执行器就是专门用来执行字节码的</strong>。目前使用最广泛的虚拟机是Hotspot，它的执行器包括了解释器和JIT编译器，接下来，我就以Hotspot为例，分别对它们加以介绍。</p><h2>简单的解释器实现</h2><p>通常来讲，解释器是对字节码进行解释执行的。而在Hotspot里，<strong>解释器主要分为两大类：cpp解释器和模板解释器</strong>。其中，cpp解释器结构最为简单，其他解释器都是在它的基础上做了不同的改进，但核心原理基本相同。因此，这里我就以Hotspot中的cpp解释器为例，带你来分析它的实现原理。</p><p>原理理解起来并不复杂，<strong>最简单的解释器就是按照规范，模拟一个栈，然后按照字节码的语义逐个执行</strong>。这种解释器的策略就是取一条字节码，然后按照这条字节码的语义对栈进行操作。我这里还是以上面的例子来进行说明。</p><p>每一个Java方法的栈里面都有一个模拟栈和一个变量表。在刚开始执行dist方法的时候，栈里的模拟栈是空的，变量表中的值只有4个入参，如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/33/a6/332497376e6a099a60625a6a5ceef5a6.jpg?wh=2284x1226\" alt=\"\"></p><p>因为参数类型都是双精度数，所以变量表里每个参数都占据两个存储位。第一条字节码是d_load0，这条字节码的语义是将变量表位置为0的那个值加载到栈上，执行完这条字节码以后的栈是这样的：</p><p><img src=\"https://static001.geekbang.org/resource/image/ed/59/ed068ef20aedd131086665a2bb17c659.jpg?wh=2284x1254\" alt=\"\"></p><p>接下来，解释器会执行下一条字节码“dload 4\"，这条字节码的语义是将位置为4的那个值加载到栈上，执行完以后的栈是这样的：</p><p><img src=\"https://static001.geekbang.org/resource/image/c5/f4/c51c60561b9c086809937c461c131ef4.jpg?wh=2284x1226\" alt=\"\"></p><p>再下一条字节码是dsub，这条字节码的语义是将栈顶元素1.0取出做为减数，然后再取出新的栈顶元素0.0做为被减数，执行减法，得到差-1.0，再把这个数放到栈顶，经过这步操作以后，栈里的情况是这样的：</p><p><img src=\"https://static001.geekbang.org/resource/image/bc/02/bc3f6b610929ee32d3d89dff99881502.jpg?wh=2284x1258\" alt=\"\"></p><p>接下来是dstore 8，这条字节码的语义是将栈顶元素取出，并存储到变量表里，位置为8的地方。所以，经过这一条字节码以后的栈的情况如下所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/43/3f/437639a82bd72d6a40dbd3ffe039ee3f.jpg?wh=2284x1265\" alt=\"\"></p><p>因为分析的情况类似，所以剩余的字节码我就不再分析了，你可以自己练习一下。</p><p>讲到这里，你就能理解，<strong>每一条字节码的语义都是由Java语言规范规定的，不管在什么平台上，模拟栈和变量表这两个数据结构都是相同的。本质上，字节码就是对模拟栈和变量表不断地进行操作。这种逐条取出字节码，逐条执行的方式被称为解释执行。对字节码进行解释执行的执行器叫做解释器。</strong></p><p>在理解了解释器以后，你肯定会想到，解释器的运行效率肯定很差吧。举个例子，对于加法操作，C++的加法语句会被翻译成加法指令，只需要一条就够了。但是Java的加法语句却要经历两次出栈操作、一次加法操作和一次入栈操作。</p><p>如果Java语言虚拟机只有解释器一种执行策略的话，性能肯定无法支撑Java获得今天的地位。实际上，Java语言的运行效率是非常高的，这是怎么做到的呢？它的核心秘密就在于即时编译(Just In Time, JIT)。我们接下来一起打开JIT编译器的神秘面纱吧。</p><h2>JIT编译器</h2><p>JVM在运行之初将class文件加载进内存，然后就开始解释执行。如果一个函数被执行多次，JVM就会认为这个函数是一个热点(hotspot)函数，然后就将它翻译成机器码执行。在<a href=\"https://time.geekbang.org/column/article/436308\">第6节课</a>至<a href=\"https://time.geekbang.org/column/article/440471\">第8节课</a>中所讲的C++静态编译相比，JIT最大的特点是在程序运行时进行编译。</p><p>这种编译方式相对解释器，性能得到了巨大的提升。它与静态编译相比又具有怎样的特点呢？我们接下来从原理入手，抽丝剥茧来回答这个问题。我们先来了解JIT编译器能成功运行所依赖的两大核心机制，分别是：</p><ol>\n<li><strong>申请可写可执行的内存区域，确保在运行器可以生成可执行的机器码；</strong></li>\n<li><strong>基于性能采样的编译优化(Profiling Guided Optimization, PGO)，可以使得JIT编译器获得超过静态编译器的运行性能。</strong></li>\n</ol><p>我们先来看看JIT编译的第一个核心机制：可写可执行的内存区域。</p><h3>可写可执行的内存区域</h3><p>具体来讲，这个机制是申请一块既有写权限又有执行权限的内存，然后把你要编译的Java方法，翻译成机器码，写入到这块内存里。当再需要调用原来的Java方法时，就转向调用这块内存。你可以看下面例子：</p><pre><code>#include&lt;stdio.h&gt;\n\nint inc(int a) {\n    return a + 1;\n}\n\nint main() {\n    printf(&quot;%d\\n&quot;, inc(3));\n    return 0;\n}\n</code></pre><p>上面这个例子很简单，就是把3加1，然后打印出来，我们通过以下命令，查看一下它的机器码：</p><pre><code>$ gcc -o inc inc.c\n$ objdump -d inc\n</code></pre><p>然后在这一堆输出中，可以找到 inc 方法最终被翻译成了这样的机器码：</p><pre><code>  40052d:\t55                   \tpush   %rbp\n  40052e:\t48 89 e5             \tmov    %rsp,%rbp\n  400531:\t89 7d fc             \tmov    %edi,-0x4(%rbp)\n  400534:\t8b 45 fc             \tmov    -0x4(%rbp),%eax\n  400537:\t83 c0 01             \tadd    $0x1,%eax\n  40053a:\t5d                   \tpop    %rbp\n  40053b:\tc3                   \tretq \n</code></pre><p>我们先来分析一个这块机器码，你可以看到，它首先会保存上一个栈帧的基址，并把当前的栈指针赋给栈基址寄存器（第1行），这是进入一个函数的常规操作。这个过程我们在<a href=\"https://time.geekbang.org/column/article/433530\">第4节课</a>有详细介绍，你可以去看一下。</p><p>然后把edi存到栈上（第3行）。在X86 64位Linux系统上，前6个参数都是使用寄存器传参的。第一个参数会使用rdi，第二个参数使用 rsi，等等。所以 edi 里存的其实就是第一个参数，也就是整数 3，为什么使用rdi的低32位，也就是 edi 呢？因为我们的入参 a 是 int 型，你可以试试换成long型，会有什么样的效果。</p><p>接着，把上一步存到栈上的那个整数再存进 eax 中（第4行）。最后，把 eax 加上 1， 然后就退栈，返回（第5行往后）。按照二进制接口(Application Binary Interface, ABI)的规定，返回值通过eax传递。</p><p>通过上面的分析，你会发现，其实上面编译代码的第3行和第4行根本没有存在的必要，gcc 默认情况下，生成的机器码有点傻，它总要把入参放到栈上，但其实，我们是可以直接把参数从 rdi 中放入到 rax 中的。</p><p>如果你还是觉得这段代码太复杂了，那我们可以自己改一下，让它更精简一点。怎么做呢？答案就是运行时修改 inc 的逻辑，修改后的代码如下所示：</p><pre><code>#include&lt;stdio.h&gt;\n#include&lt;memory.h&gt;\n#include&lt;sys/mman.h&gt;\n\ntypedef int (* inc_func)(int a); \n\nint main() {\n    char code[] = { \n        0x55,             // push rbp\n        0x48, 0x89, 0xe5, // mov rsp, rbp\n        0x89, 0xf8,       // mov edi, eax\n        0x83, 0xc0, 0x01, // add $1, eax\n        0x5d,             // pop rbp\n        0xc3              // ret\n    };  \n\n    void * temp = mmap(NULL, sizeof(code), PROT_WRITE | PROT_EXEC,\n            MAP_ANONYMOUS | MAP_PRIVATE, -1, 0); \n\n    memcpy(temp, code, sizeof(code));\n    inc_func p_inc = (inc_func)temp;\n    printf(&quot;%d\\n&quot;, p_inc(7));\n\n    return 0;\n}\n</code></pre><p>在这个例子中，我们使用了 mmap 来申请了一块有写权限和执行权限的内存，然后把我们手写的机器码拷进去，然后使用一个函数指针指向这块内存，并且调用它。通过这种方式我们就可以执行这一段手写的机器码了。我们来运行一下看看：</p><pre><code>$ gcc -o inc inc.c \n$ ./inc\n8\n</code></pre><p>为了生成更精简的机器码，我们可以引入编译器优化手段，例如全局值编码、死代码消除、标量展开、公共子表达式消除和常量传播等等。这样生成出来的机器码会更加优化。所以这种编译方式就被称为即时编译。</p><p>我们搞清楚了JIT编译的第一个核心机制后，再来看它的第二个核心机制：基于采样的编译优化。</p><h3>基于采样的编译优化和退优化</h3><p>我们知道，架构师的一个核心职责是对比各种技术方案的优劣，我至今还能听到有的架构师说Java性能不好，理由有很多，比如Java是解释执行、Java里所有函数都是虚函数、每次调用都需要查询虚表等等。</p><p>这些说法在老版本的JVM中可能是对的，但随着JVM中的JIT编译器的演进和优化，上面所讲的Java语言的性能缺陷都在逐渐被克服。</p><p>在<a href=\"https://time.geekbang.org/column/article/436308\">第6节课</a>我们已经学习过了编译器的作用就是把高级语言翻译成性能最好的机器码。不管是静态编译还是JIT编译，它们的功能都是一样的，但是JIT编译往往可以做得更好。我们通过一个编译优化的常量传播例子来说明。</p><p>这个例子的代码如下：</p><pre><code>public static int test() {\n    int b = 3;  \n    int c = 4;\n    return b + c;\n}\n</code></pre><p>在这块代码中，由于第2行对b赋值一个常量后，后面的语句没有再改过b的值，我们就可以把后面所有出现b的地方都改为3，同理所有出现c的地方都改为4。经过这种优化，代码的第4行就可以改写成：</p><pre><code> return 3 + 4;\n</code></pre><p>接着，编译器再做一轮分析，将运算符两边都是常量的情况，直接进行计算，也就是把上面的代码再优化成：</p><pre><code> return 7;\n</code></pre><p>这种优化被称为<strong>常量折叠</strong>。（这里也请你思考一下，如果这行代码是C语言的，在最优化的情况下，gcc会生成什么样的机器码？）</p><p>通过上面的例子，我们讲了编译器优化的一个简单思路。其实，编译器的优化还有很多其他的思路和方法，这里我就不再一一列举了，如果你对编译器设计特别感兴趣的话，我推荐你去看看<a href=\"https://book.douban.com/subject/3774682\">《编译原理 》</a>和<a href=\"https://book.douban.com/subject/1400374\">《高级编译器设计与实现 》</a>等书。</p><p>接下来，我们再看第二个例子，这是一个C语言编译器没有办法优化，但是JIT编译却能进一步优化的例子。在这个例子中，你会发现常量传播不再起作用了：</p><pre><code>public static int test(boolean flag) {\n    int b = 0;\n    if (flag) {\n        b = 3;\n    }\n    else {\n        b = 2;\n    }\n    return b + 4;\n}\n</code></pre><p>和第一个例子相比较，这个例子增加了第3行到第8行的条件判断。所以编译器无法知道在第9行b的真实取值是什么。只能严格按照这个函数的逻辑去生成比较，跳转，赋值等等，那么这个例子就比可以常量折叠那个例子复杂多了。</p><p>一般情况下，虽然这个例子中的test函数没有优化空间了，但是JVM的JIT技术还是在这里找到了最优化的机会。假如存在一种情况，每一次test方法被调用的时候，传的参数flag都是true或者都是false，也就是说，flag的取值固定。那么JIT编译器就可以认为另外一个分支是不存在的，可以不编译。</p><p>JIT编译器在开始之前，test方法是由解释器执行的。解释器一边执行，一边会统计flag的取值，这种统计就叫做性能采样（Profiling)。当JIT编译器发现，test方法被调用了500次（这个阈值可以以JVM参数指定），每一次flag的值都是true，那它就可以合理地猜测，下一次可能还是true，它就会把test方法优化成这个样子：</p><pre><code>public static int test(boolean flag) {\n    return 7;\n}\n</code></pre><p>但是，这种做法，相信你也都看出问题来了，如果恰好test方法的下一次调用就是false呢？所以JVM必须在test方法里留一个哨兵，当参数flag的值为false的时候，可以再退回到解释器执行。这个过程就是<strong>退优化（Deoptimization）</strong>。这个过程相当于，JVM的JIT编译器生成的机器码等效于以下代码：</p><pre><code>public static int test(boolean flag) {\n    if (!flag)\n        deoptimize()\n    return 7;\n}\n</code></pre><p>在这个代码中，deoptimize方法是JVM提供的内建方法，它的作用是由JIT编译器退回到解释器进行执行。这个过程涉及栈帧（<a href=\"https://time.geekbang.org/column/article/435493\">第5节课</a>的核心概念）的运行时切换，无疑是非常精巧和复杂的，但我们做为Java语言的使用者，并不需要完全理解JIT背后的每一个技术细节。但通过这个例子，我们可以掌握了如何写程序，才能让JIT编译器帮我们生成最高效的机器码。</p><p>让JIT编译器运行得好，我们只需要遵守一条原则：<strong>让程序行为可预测</strong>。因为JIT编译优化的基本假设是过去和未来，程序的运行规律基本一致，所以它基于过去的行为测试未来。如果它预测的未来和真实情况不一致，就会发生退优化。退优化的情况会对性能带来巨大的伤害，所以JIT有时也可能是一把双刃剑。</p><p>到此为止，我们就把JIT编译器的基本原理介绍完了。<strong>可写可执行内存和基于采样的编译优化这两大机制保障了JIT编译器的实现，而JIT编译器又是JVM高效的核心秘密</strong>。如果你还想了解更多关于JIT的知识，你可以思考一下，JIT还有哪些优化方式呢？这里我就不再展开了，你可以查阅相关资料来学习，欢迎你在留言区与我交流。</p><h2>总结</h2><p>在今天这节课里，我们学习了Java字节码的基本原理，了解到Java字节码是一种基于栈的中间格式。机器码是由CPU的设计风格和指令集决定的，所以在不同的架构上，机器码都是不同的，但是Java字节码则与机器码不同，它在所有的平台和操作系统上，都是一样的，这就屏蔽掉了平台差异。</p><p>字节码文件是由Java语言虚拟机加载执行的。只要遵守Java语言规范，任何人都可以实现自己的Java语言虚拟机，例如IBM的J9，开源的Hotspot虚拟机等等。虚拟机有自动内存管理模块和执行器两大核心组成。自动内存管理是我们这个专栏的核心内容，我们会在后面的内容中加以介绍。</p><p>虚拟机中用于执行字节码的模块是执行器，最简单的执行器是解释器，但是解释器的性能比较差。为了提高性能，JVM中引入了JIT编译器。JIT编译器依赖两个核心机制：</p><ol>\n<li><strong>申请可写可执行的内存区域，以便于运行时生成机器码；</strong></li>\n<li><strong>基于性能采样的编译优化(Profiling Guided Optimization, PGO)，可以使得JIT编译器获得最佳性能。</strong></li>\n</ol><p>当JIT编译器在做性能优化的时候，我们根据统计做了很多假设，比如某个分支语句，一直只选择其中一个分支，另一个分支就不会走到；再比如某个对象的类型永远都是父类，而不是子类等等。</p><p>这种根据过去的规律去预测未来的做法，并不能保证完全正确，所以JIT编译器往往又会配合退优化机制一起工作。当预测正确时，此时的性能是最好的，但是如果预测不正确，就可以退回到解释器进行执行，从JIT编译器退回解释器的过程就是退优化。</p><h2>思考题</h2><p>我们在<a href=\"https://time.geekbang.org/column/article/431904\">第3节课</a>讲过了进程的内存布局，但当时是以C/C++的进程来举例的。考虑以下Java程序：</p><pre><code>  public static int test() {\n        Random r = new Random();\n        int a = r.nextInt();\n        return a + 1;\n    }\n</code></pre><p>请你分析，变量r和a分别创建在进程的什么位置？欢迎你在留言区分享你的想法和收获，我在留言区等你。</p><p><img src=\"https://static001.geekbang.org/resource/image/de/29/deaede37ae353706c32d615f157dc729.jpg?wh=2284x1792\" alt=\"\"></p><p>好啦，这节课到这就结束啦。欢迎你把这节课分享给更多对计算机内存感兴趣的朋友。我是海纳，我们下节课再见！</p>","float_qrcode":"https://static001.geekbang.org/resource/image/83/1c/83e244537b714e899fd2e7896e22371c.png","article_cover_hidden":false,"is_required":true,"score":"31637078400","like_count":10,"article_subtitle":"","audio_download_url":"https://static001.geekbang.org/resource/audio/3b/ea/3b7deb06499f84fed9a0b799b3585eea.mp3","had_viewed":false,"article_title":"11 | 即时编译：高性能JVM的核心秘密","column_bgcolor":"#F6F7FB","offline_package":"{}","audio_title":"11.14 海纳 （有车辆鸣笛声）11_jit_compiler_01.MP3","audio_time_arr":{"m":"19","s":"59","h":"00"},"text_read_percent":0,"cid":450,"article_cshort":"<p>你好，我是海纳。</p><p>在前面的课程里，我们讲解了进程内部的具体布局，以及每一个部分的功能和作用。你会发现，所有的例子都是用C/C++写的，我相信你在学习的过程中，心里可能会产生这样的疑问：<strong>那Java和Python语言是怎么运行起来的呢？</strong></p><p>有这个疑问非常合理。我曾经讲过C/C++编译的结果，它在linux上是ELF文件，在windows上是exe文件，这两种文件都可以直接被操作系统加载运行的二进制文件。另外，C/C++源代码也可以被编译成动态链接库文件。</p><p>而在Java语言里，程序员都知道Java源代码被javac编译以后，生成的是字节码文件，也就是class文件，而且不管编译所使用的操作系统是什么，相同的Java源码必然得到相同的class文件。class文件显然与上面C/C++编译的二进制文件都不相同，因为它与编译的平台无关。</p><p>这节课，我们就围绕着Java是怎么运行起来的这个问题逐层展开，在这个过程中，我会教你如何阅读和分析字节码，以及猜测它的JIT结果。所以通过这节课的学习，你不仅能了解到Java字节码的核心知识、JVM中的解释器和JIT编译器的原理，而且，还能进一步理解JVM虚拟机。在这个基础上，你就能写出更高效、对编译器更友好的程序，而且碰到桥接方法这一类Java中非常抽象和难以理解的概念时，也能着手分析。</p>","video_width":0,"column_could_sub":true,"video_id":"","sku":"100094901","video_cover":"","author_name":"海纳","column_is_onboard":true,"audio_url":"https://res001.geekbang.org/media/audio/3b/ea/3b7deb06499f84fed9a0b799b3585eea/ld/ld.m3u8","chapter_id":"2314","column_had_sub":true,"column_cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg","neighbors":{"left":{"article_title":"10 | 页中断：fork、mmap背后的保护神","id":444178},"right":{"article_title":"12 | 内存虚拟化：云原生时代的奠基者","id":446677}},"rate_percent":0,"footer_cover_data":{"img_url":"https://static001.geekbang.org/resource/image/f0/fe/f0c47daf3262a8df6cf352d89896abfe.png","link_url":"https://time.geekbang.org/article/427012","mp_url":""},"float_app_qrcode":"","column_is_experience":false,"rate":{"1":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"2":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"3":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0}},"product_id":100094901,"had_liked":false,"id":445925,"free_get":false,"is_video_preview":false,"article_summary":"这节课，我们就围绕着Java是怎么运行起来的这个问题逐层展开，在这个过程中，我会教你如何阅读和分析字节码，以及猜测它的JIT结果。","column_sale_type":0,"float_qrcode_jump":"https://time.geekbang.org/serv/v4/misc/jump?uri=https%3A%2F%2Ftime.geekbang.org%2Fhybrid%2Fmp%2Fjump%3Furl%3Dhttps%253A%252F%252Fstatic001.geekbang.org%252Fresource%252Fimage%252Fee%252Fb2%252Feedd4090668aabfac91b2b445d27cbb2.png","column_id":450,"article_sharetitle":"11 | 即时编译：高性能JVM的核心秘密","article_poster_wxlite":"https://static001.geekbang.org/render/screen/15/80/15db4da5cc4cb93cafd773b7d9106780.jpeg","article_features":0,"comment_count":7,"audio_md5":"3b7deb06499f84fed9a0b799b3585eea","offline":{"size":21373393,"file_name":"c69e0377dc7ea1e0c42869d66c786291","download_url":"https://static-acl-001.geekbang.org/resource/zip/article/445925/c69e0377dc7ea1e0c42869d66c786291.zip?auth_key=1641482221-980a2a59dd2f48f3bf4ac401378f9c0e-0-b8b3a127e6b1b780f67482f91d251ede"},"video_size":0,"hls_videos":[],"video_time":"","article_could_preview":false,"article_ctime":1637078400,"share":{"content":"学好内存，掌握系统开发诀窍","title":"海纳 · 编程高手必学的内存知识","poster":"https://static001.geekbang.org/resource/image/bc/c5/bcaf3ff69e538f8749970edf2301bbc5.jpg","cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg"}},"446677":{"text_read_version":0,"audio_size":22623666,"article_cover":"https://static001.geekbang.org/resource/image/1f/9c/1f8ffcae8ecd9832eee760d5be55139c.jpg","subtitles":[],"product_type":"c1","audio_dubber":"海纳","is_finished":false,"like":{"had_done":false,"count":4},"audio_time":"00:23:33","video_height":0,"article_content":"<p>你好，我是海纳。</p><p>今天的这节课呢，是软件篇中的最后一节课了，在前面的课程里，我们整体介绍了单机系统上内存管理的基础知识。这节课，我们就结合前面学习的内容，一起来探讨下，虚拟化中的内存管理，因为我们前面讲过了内存知识，在这个基础上，你再来学习虚拟化中的内存管理，就会简单多了。</p><p>当前，云计算已经成为各种网络服务的主流形式，但云计算不是一蹴而就的，它的发展也经过了长期的探索和演变。在演变的过程中，扮演核心角色的就是主机虚拟化技术。它经历了虚拟机和容器两大阶段，其中<strong>虚拟机以VMWare和KVM等为代表，容器以Docker为代表</strong>。</p><p>虽然现在Docker技术非常火爆，甚至某种程度上，人们在讨论云化的时候往往就是指容器化，但是虚拟机技术在长期的发展中，也留下了非常宝贵的技术积累，这些积累在各种特定的场景里还在发挥着重要作用。</p><p>举一个我曾经遇到过的一个真实案例：在Windows上快速预览Android游戏。这个操作听起来很神奇是吧，那如何才能打破架构上的壁垒，达到快速执行的目的呢？这就需要对虚拟化的基本原理掌握得比较好，从而在虚拟机层面做很多优化。所以掌握虚拟化技术绝不仅仅只应用于云服务的场景，它可能会在各种意想不到的场景中发挥奇效。</p><!-- [[[read_end]]] --><p>这节课，我将带你从内存入手来学习虚拟化技术，当然，在学习内存虚拟化之前，我们还是需要先了解一下虚拟化技术中的一些关键技术点，以及几个核心的角色，这些背景是贯穿虚拟化技术的核心所在。</p><h2>虚拟化技术的基本概念和原则</h2><p>我们先来进行一下名词解释，<strong>在虚拟化技术中涉及的有三个核心角色，分别是宿主机，客户机和虚拟机监控器</strong>。宿主机，也被称为Host，一般指代物理主机。客户机，也被称为Guest，是指运行在宿主机上的虚拟机。而负责为客户机准备虚拟CPU，虚拟内存等虚拟资源，并同时对客户机进行管理的模块，就是虚拟机监控器(Virtual Machine Monitor , VMM)。</p><p>对于传统的物理主机而言，往往有很强大的计算资源，所以通常一台机器上会有多个用户共同使用，这样才能使得主机资源得到充分利用。但是在传统的单机系统里边，多个用户之间往往会相互影响。例如，通过ps命令可以看到系统上其他用户启动的进程，也可以使用kill命令杀死其他用户的进程，这就对隐私性和安全性带来了比较大的挑战。</p><p>针对上面的问题，<strong>虚拟化技术可以让用户相互隔离开</strong>。在不同的虚拟机实例中运行的用户，虽然运行在同一个物理主机上，但是相互无法看到对方，这样就很好的保证了虚拟机用户的隐私与安全。<strong>在所有的虚拟化实现方案中，内置于Linux内核的虚拟化技术，也就是基于内核的虚拟机(Kernel based Virtual Machine, KVM)是影响力比较大的一个</strong>。这也是我们这节课的重点。</p><p>第三个比较重要的组件是VMM，有些资料中也把它叫做Hypervisor。正如它的名字的含义，它是负责管理和调度虚拟机的，虚拟机在执行特权指令、处理中断和管理内存等特殊操作时，都需要通过VMM来完成相应功能。了解完虚拟化技术的三个核心角色后，接下来，我来介绍下虚拟化技术在实际工作中需要遵循的指导原则。</p><p>1974年，两位计算机科学家Gerald Popek 和 Robert Goldberg发表了一篇重要的论文 《虚拟化第三代体系结构的正式要求》，在这篇论文中提出了虚拟化的三个基本条件：</p><ol>\n<li><strong>等价性</strong>，即要求在虚拟机环境中运行的程序，应当与在物理机上运行的程序行为一致，且所有能在物理机上运行的程序都应该能够在虚拟机中运行；</li>\n<li><strong>资源限制</strong>，即要求虚拟机使用的资源需要被进行监督和限制，虚拟机不能越界使用到不属于它的资源；</li>\n<li><strong>高效性</strong>，即要求在虚拟机中运行的程序与在物理机中运行的程序相比，性能应该无明显的损耗。</li>\n</ol><p>这三个条件便为后续的虚拟化技术的发展提供了有效的指导原则，设计良好的虚拟化技术需要同时满足以上三个条件。</p><p>第一个等价性的条件自然不需要过多解释，如果Guest里运行的程序结果都不能保证，那么虚拟化的技术就没有任何意义了。因此，<strong>等价性是虚拟化技术中最基本的要求</strong>。</p><p>至于第二个资源限制的条件也比较容易理解，<strong>我们使用Guest的目的本身就是为了对资源使用进行限制与管理，防止不同用户之间对计算机资源的相互干扰，这也是我们考虑使用虚拟化技术时最重要的原因</strong>。</p><p>在前两个条件的约束下，我们可以很容易想到一个实现虚拟化技术的方案是：<strong>通过纯软件模拟CPU执行过程</strong>。也就是说，这里需要对完整的底层硬件进行模拟，包括处理器、物理内存和外部设备等等。这样的话，Guest的所有程序都相当于运行在Host的一个解释器里，来一条指令就解释一条指令，资源限制以及运行等价的要求都很容易满足。</p><p>不过这个方案的缺陷也非常明显，就是无法满足三个条件里面的高效性。因为你是用软件来对CPU的指令进行了翻译，通常一条指令最终会被翻译成非常多的指令，那效率自然也是非常低的。既然对指令进行翻译的效率是如此低下，那我们为什么不能让Guest程序的代码直接运行在Host的CPU上呢？</p><p>我们本来翻译指令的目的，是为了让VMM能够对Guest执行的指令进行监管，防止Guest对计算资源的滥用，那如果又让Guest的执行直接运行在CPU上，VMM又哪里有机会能够对Guest进行监管呢？</p><p>为了解决这个问题。人们提出一个重要的模型，这就是<strong>陷入模拟（Trap-and-Emulate）模型</strong>。接下来，我们就来了解一下吧。</p><h2>Trap-and-Emulate模型</h2><p>陷入模型的核心思想是：<strong>将Guest运行的指令进行分类，一类是安全的指令，也就是说这些指令可以让Host的CPU正常执行而不会产生任何副作用，例如普通的数学运算或者逻辑运算，或者普通的控制流跳转指令等；另一类则是一些“不安全”的指令，又称为“Trap”指令，也就是说，这些指令需要经过VMM进行模拟执行，例如中断、IO等特权指令等</strong>。</p><p>接下来，我们来看一下它的具体实现过程：对于“安全”的指令，Guest在执行时可以交由Host的CPU正常运行，这样可以保证大部分场景的性能。不过，当Guest执行一些特权指令时就需要发出Trap，通知VMM来接管Guest的控制流。VMM会对特权指令进行模拟(Emulate)，从而达到资源控制的效果。当然在进行模拟的过程中需要保证执行结果的等价性。</p><p>经过这样一个Trap-and-Emulate的过程，Guest就可以在保障等价性以及资源限制的前提下，尽可能地满足虚拟化的高效性的条件。</p><p>可能你对此感知不深，下面我给你举一个例子，你就能理解Trap-and-Emulate到底是怎么回事了。</p><p>以0x80号中断为例，在<a href=\"https://time.geekbang.org/column/article/431400\">第2节课</a>里，我们使用0x80号中断，调用了write这个系统调用，在控制台上打印文字。\"int 0x80\"这条指令就是一个特权指令，它会导致当前进程切入内核态执行。在虚拟化场景下遇到这种特权指令，我们不能直接交给宿主机的真实CPU去执行，因为宿主机CPU会使用宿主机的IDT来处理这次中断请求。</p><p>而我们真正希望的是，<strong>使用客户机的IDT去查找相应的中断服务程序</strong>。这就需要Guest退回到VMM，让VMM模拟CPU的动作去解析IDT中的中断描述符，找到Guest的中断服务程序并调用它。在这个例子中，Geust退回VMM的操作就是Trap，VMM模拟CPU的动作去调用Guest的中断服务程序就是Emulate。</p><p>现在，我们有了整体的方案，不过，这里仍然存在一个问题：当Guest的内核代码在Host的CPU上执行的时候，Guest没有办法区分“安全”指令和“非安全”指令，也就是说Guest不知道哪条指令应该触发Trap。幸好，现代的芯片对这种情况做了硬件上的支持。</p><p>现代的X86芯片提供了VMX指令来支持虚拟化，并且在CPU的执行模式上提供了两种模式：<strong>root mode和non-root mode，这两种模式都支持ring 0 ~ ring 3三种特权级别</strong>。VMM会运行在root mode下，而Guest操作系统则运行在non-root mode下。所以，对于Guest的系统来讲，它也和物理机一样，可以让kernel 运行在ring 0的内核态，让用户程序运行在ring 3的用户态， 只不过整个Guest都是运行在non-root 模式下。</p><p>有了VMX硬件的支持，Trap-and-Emulate就很好实现了。Guest可以在root模式下正常执行指令，就如同在执行物理机的指令一样。当遇到“不安全”指令时，例如I/O或者中断等操作，就会触发CPU的trap动作，使得CPU从non-root 模式退出到root模式，之后便交由VMM进行接管，负责对Guest请求的敏感指令进行模拟执行。这个过程称为<strong>VM Exit</strong>。</p><p>而处于root模式下的VMM，在一开始准备好Guest的相关环境，准备进入Guest时，或者在VM Exit之后执行完Trap指令的模拟准备，再次进入Guest的时候，可以继续通过VMX提供的相关指令VMLAUNCH以及VMResume，来切换到non-root 模式中由Guest继续执行。 这个过程也被称为<strong>VM Entry</strong>。</p><p>在理解了VMM的基本工作原理以后，我们就可以探讨虚拟化场景下的内存管理了。接下来，我们从虚拟机用户的视角出发，来看看VMM是如何支持Guest的内存管理的。</p><h2>Guest内存管理</h2><p>既然讲到内存管理，我们先来研究下物理机和虚拟机分别是怎么获取系统的内存信息的。</p><p>在x86架构的实模式下，系统启动时，BIOS ROM会被映射到内存的0xF0000 ~ 0xFFFFF的位置。CPU上电后会从0xFFFF0的位置开始执行，这里会跳转到BIOS的起始代码中。BIOS的代码会检查物理内存的信息，并记录下来。</p><p>之后，操作系统可以通过查询INT 15h的中断，来获取物理内存的信息，然后根据寄存器AX值的不同来返回不同的内存信息。当AX值设置为0xE820时，将返回所有已安装 RAM 以及 BIOS 保留的物理内存范围的内存映射。我们看到，在物理机下，是通过INT 15h这个中断来获取系统的内存信息的。</p><p>类比下来，如果虚拟机里边想要获取系统的内存信息的话，就需要VMM模拟物理机BIOS的行为。在系统启动时，VMM会将模拟BIOS的代码直接放到内存的0xF0000 ~ 0xFFFFF的位置。在构建中断向量表的时候，则将第0x15位置的中断函数地址设置为虚拟的内存查询函数地址。而调用INT 15h的中断时，中断服务程序返回的是用户配置的Guest的内存信息。这样的话，就可以使得Guest以为自己已经获取了实际物理机的内存信息。</p><p>我们知道，<strong>在x86的架构上，系统启动时需要先在实模式下完成系统的引导，然后才会进入保护模式</strong>。同样，<strong>Guest在启动过程中也需要先通过实模式进行引导，再切换到保护模式下</strong>。所以，我们学习Guest的访存机制也是需要分别考虑实模式跟保护模式下的不同处理方式。</p><h3>实模式Guest的访存</h3><p>正常情况下，当一个Host系统中启动运行Guest系统时，此时的Host是处于保护模式的，而Guest则因为刚启动，所以需要运行在实模式下。此时又碰到一个问题，Guest里实模式的代码又如何运行在Host处于保护模式下的CPU上呢？</p><p>这个问题同样需要硬件来支持。在x86体系的CPU中，可以支持一种虚拟8086的模式，这个模式又被称为虚拟-实模式，意思是可以让CPU在保护模式下来运行实模式的程序。当然这里虚拟8086模式下访问的地址，并不意味着程序跟实模式一样，就可以直接访问Host的真实物理地址了，只是说在该模式下，程序可以采用同实模式下一样的寻址方式，但访问的地址还是Host的虚拟地址，但在Guest自己看来，它认为自己访问的是Guest的物理地址(Guest Physical Address,GPA)。</p><p>这种情况下，Guest代码中的逻辑地址到Host的物理地址（Host Physical Address, HPA）的转换主要分为三个步骤：</p><ol>\n<li>我们知道实模式下程序访存时是通过段式寻址的方式，也就是说，Guest程序的逻辑地址可以通过分段机制转换为GPA，这个过程是由Guest模式下CPU自发地进行，需要CPU运行在上边提到的虚拟8086模式下；</li>\n<li>Guest的物理内存可以由VMM转换到Host的虚拟内存地址（Host Virtual Address，HVA）。这一步的转换过程可以由VMM内部维护的数据结构进行查表得出；</li>\n<li>最后一步的转换也是由VMM直接调用缺页中断服务函数(get_user_pages)将Host的虚拟内存映射到物理页。你要注意的是，这一步是VMM主动调用的，而不是由中断触发的。</li>\n</ol><p>我们知道，在物理机上进行虚拟地址与物理地址转换的话，需要cr3寄存器来存放页表。因此，在Guest的实模式下，为了能够获取到实际运行的物理地址，我们需要在VM Enter的过程中将cr3寄存器设置成VMM为Guest准备的页表。</p><p>在实模式下，因为Guest指令访问都是物理地址，所以cr3寄存器还需要放置负责映射GPA到HPA的页表基址。在初始状态下，VMM只需要准备一个根页面就可以了，等运行到缺页异常时，再通过缺页异常处理函数，来按需要完成页面的映射。</p><h3>保护模式下Guest的访存</h3><p>我们说，运行在实模式的Guest只需要一个页表就可以完成GPA到HPA的映射。但在保护模式下，我们知道每个进程都有自己的页表，维护着GVA到GPA的映射。所以保护模式下的内存转换方式要更加复杂。</p><p>在保护模式下的进程，当Guest准备访存时，cr3寄存器此时存放的是Guest的页表。如果将这个页表交给MMU去查询，得到的将是GPA的地址，而不是真正的HPA地址。这是因为从GVA到HPA之间存在三层映射关系，即：</p><ul>\n<li><strong>GVA到GPA的映射</strong>；</li>\n<li><strong>GPA到HVA的映射</strong>；</li>\n<li><strong>HVA到HPA的映射</strong>。</li>\n</ul><p>但MMU却只有一个。因此，要解决这个问题，我们需要将cr3寄存器中指向的Guest的页表，替换成为一张从GVA到HPA映射的页表。当Guest再进行访存时，则可以通过这个页表完成完成从GVA到HPA的转换过程。因为在这个过程中，<strong>新建的这张页表实际上会把Guest本身的页表给遮挡起来，所以我们称这个页表为影子页表(Shadow page table)</strong>。</p><p>我们说过，保护模式下每个进程都需要有自己的页表，同样的，VMM也需要为Guest的每个进程维护一个影子页表。在Guest的进程切换过程，要更新cr3寄存器指向的页表地址，VMM要把这个操作拦截下来，将Guest页表换成影子页表。影子页表的示意图如下所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/87/32/8778c4f0157bb3a57afef22dc14fa632.jpg?wh=2284x1177\" alt=\"\"></p><p>可以看到，影子页表将原来的三级映射压缩成了一级映射，CR3寄存器里只要存储影子页表的地址就可以了。这样MMU就可以自动完成从GVA到HPA的转换。</p><p>明白了影子页表的作用后，我们接下来看下在KVM里边影子页表是如何实现的。从影子页表的机制中我们可以看出，实现影子页表的过程中有两个关键点：</p><ol>\n<li><strong>cr3寄存器的切换；</strong></li>\n<li><strong>影子页表的构建。</strong></li>\n</ol><p>在第一点中，由于进程切换的时候都需要进行页表的切换，也就是对cr3寄存器的修改。因此，<strong>当Guest在进程切换准备把Guest的页表写入cr3寄存器时，需要VMM介入进来，记录下此时要写入的Guest的页表，同时把GVA到HPA映射的影子页表写入到cr3中，完成一次偷梁换柱</strong>。</p><p>在第二点中，影子页表的构建，主要是通过影子页表的缺页异常处理函数来完成的，它主要的流程是：<strong>当Guest执行访存指令，来进行访存的时候，会将GVA发送给MMU进行查找。由于此时cr3存放的是影子页表，因此MMU会通过影子页表来查找GVA对应的HPA。如果找到了，就可以直接从HPA中读取对应的数据，然后流程结束</strong>。</p><p>如果此时影子页表中还没有GVA到HPA的映射，就会触发VM Exit，并从Guest模式退出到Host模式，由影子页表的缺页处理函数进行处理。影子页表的缺页处理函数会通过上文保存的Guest的页表，来查找GVA对应的GPA。</p><p>如果Guest的页表中，GVA到GPA的映射还不存在，就会由VMM向Guest注入缺页异常，并交由Guest的缺页异常处理函数，完成GVA到GPA的映射过程。完成映射后，Guest会继续进行访存，由于此时影子页表中GVA到HPA的映射还未完成，CPU此时会继续进入影子页表的缺页异常处理函数中。</p><p>当GVA与GPA的映射已存在时，就只需要根据VMM所维护的映射关系计算出HVA。然后可以借助Host的内存管理机制，来分配空闲的物理页面，并且完成GVA到HPA的映射（与实模式相同，这一步也是由VMM主动调用get_user_pages完成的）。最后将映射关系填充到影子页表中。这就完成了影子页表的构建。</p><p>影子页表机制实际是一种纯软件实现的机制，我们可以看出，<strong>影子页表是通过软件方式实现了MMU的能力</strong>。而且在影子页表的使用过程中，会多次发生VM Entry和VM Exit，你可以想象的到，影子页表机制的效率非常慢。为了解决这个问题，<strong>硬件厂商通过新增一个页表转换单元来提升性能，这就是扩展页表(Extended Page Table, EPT)</strong>。</p><h2>硬件支持——EPT</h2><p>芯片厂商们为了提高虚拟化的效率，从硬件实现上支持了2层地址的翻译，其中AMD提出了嵌套页表（Nested Page Table，NPT）的机制，Intel提出的是扩展页表（Extended Page Table，EPT）机制。其实，这两者的实现原理类似，只是命名有所不同。因为Intel的服务器芯片更加常见，所以我们这节课主要还是以Intel的EPT为例来进行讲解。</p><p>在增加EPT机制后，相当于有两个地址转换器，其中，MMU负责GVA到GPA的地址转换， 而EPT则负责GPA到HPA的地址转换。两个转换器在硬件上相互配合，MMU根据Guest的页表翻译好GPA并传递给EPT，EPT通过EPT的页表翻译找到HPA的地址进行访存。同时Intel为了处理EPT的缺页，也引入了EPT的缺页异常机制，<strong>它的原理与MMU原理一致</strong>。</p><p>EPT的工作原理如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/70/10/70cd19356215ee078d8207c8960e7c10.jpg?wh=2284x1287\" alt=\"\"></p><p>上面这幅图与<a href=\"https://time.geekbang.org/column/article/431400\">第2节课</a>中的图非常相似，不同之处在CR3，PDE， PTE中记录都是GPA，而所有的物理地址都要经过EPT的翻译才能找到真正的HPA。</p><p>对Guest页表来说，因为每个进程需要一个Guest页表，所以会维护多个Guest的页表。但对于EPT的页表来说，其本质是GPA到HPA的映射页表，因此一个虚拟机只需要维护一个EPT的页表就可以了。</p><p>硬件翻译的整个过程对与Guest来讲则是透明的，因此在Guest发生缺页异常则不需要再进行Guest与Host模式之间的切换了，也就节省了大量的上下文切换的开销。</p><h2>总结</h2><p>这一节课，我们一起学习了虚拟机是如何处理内存的。在这节课的开始部分，我们一起了解了虚拟化技术产生的动机和它的基本原理。</p><p>在这个基础上，我们又研究了虚拟机内部是如何管理内存的。在虚拟机Guest启动的时候，宿主机Host肯定是运行在保护模式的，也就是说，分页机制已经开启。但是Guest仍然运行在实模式下，所以Guest会采用虚拟8086模式运行。在这种情况下，因为Guest是直接操作GPA的，所以VMM只需要做好GPA到HPA的转换就行了。</p><p>当Guest进入保护模式后，Guest也会维护自己的页表，我们把这个页表叫做虚拟机页表，也就是gPT。显然gPT是不能将Guest的虚拟地址转换成真正的物理地址的，这就需要VMM来做一次处理，将gPT替换为自己精心准备过的页表，也就是影子页表，sPT。</p><p>不过，影子页表的维护和切换的效率十分低下，为了解决这个问题，硬件厂商提供了EPT，它可以协助MMU进行第二次页表转换。也就是说，MMU负责将GVA转换为GPA，然后EPT再将GPA转换为HPA，来完成真正的内存页映射。</p><p>学习完今天这节课，我相信你已经足够掌握内存虚拟化的大部分知识了。但是虚拟化技术中还有CPU虚拟化、中断虚拟化等等广泛的话题，如果你对虚拟化技术十分感兴趣的话，可以参考<a href=\"https://book.douban.com/subject/25743939\">《KVM虚拟化技术：实战与原理解析》</a>和<a href=\"https://book.douban.com/subject/35238691\">《深度探索Linux系统虚拟化：原理与实现》</a>等书，这样你就可以对虚拟化技术有更全面的掌握了。</p><h2>思考题</h2><p>在HVA到HPA的转换过程中，当前的实现是主动调用get_user_pages来分配物理页，我们又知道VMM运行在内核态，实际上，它是有能力直接为GPA分配物理内存，而不必再借助HVA的，那为什么KVM要选择保留HVA呢？欢迎你在留言区分享你的想法和收获，我在留言区等你。</p><p><img src=\"https://static001.geekbang.org/resource/image/01/86/0163161202033bbf9e73c7276e3a9486.jpg?wh=2284x1386\" alt=\"\"></p><p>好啦，这节课到这就结束啦。欢迎你把这节课分享给更多对计算机内存感兴趣的朋友。我是海纳，我们下节课再见！</p>","float_qrcode":"https://static001.geekbang.org/resource/image/83/1c/83e244537b714e899fd2e7896e22371c.png","article_cover_hidden":false,"is_required":true,"score":"31637251200","like_count":1,"article_subtitle":"","audio_download_url":"https://static001.geekbang.org/resource/audio/06/e5/068cb03880312709ba3b84f2637066e5.mp3","had_viewed":false,"article_title":"12 | 内存虚拟化：云原生时代的奠基者","column_bgcolor":"#F6F7FB","offline_package":"{}","audio_title":"11.17海纳 12_kvm.MP3_R.mp3","audio_time_arr":{"m":"23","s":"33","h":"00"},"text_read_percent":0,"cid":450,"article_cshort":"<p>你好，我是海纳。</p><p>今天的这节课呢，是软件篇中的最后一节课了，在前面的课程里，我们整体介绍了单机系统上内存管理的基础知识。这节课，我们就结合前面学习的内容，一起来探讨下，虚拟化中的内存管理，因为我们前面讲过了内存知识，在这个基础上，你再来学习虚拟化中的内存管理，就会简单多了。</p><p>当前，云计算已经成为各种网络服务的主流形式，但云计算不是一蹴而就的，它的发展也经过了长期的探索和演变。在演变的过程中，扮演核心角色的就是主机虚拟化技术。它经历了虚拟机和容器两大阶段，其中<strong>虚拟机以VMWare和KVM等为代表，容器以Docker为代表</strong>。</p><p>虽然现在Docker技术非常火爆，甚至某种程度上，人们在讨论云化的时候往往就是指容器化，但是虚拟机技术在长期的发展中，也留下了非常宝贵的技术积累，这些积累在各种特定的场景里还在发挥着重要作用。</p><p>举一个我曾经遇到过的一个真实案例：在Windows上快速预览Android游戏。这个操作听起来很神奇是吧，那如何才能打破架构上的壁垒，达到快速执行的目的呢？这就需要对虚拟化的基本原理掌握得比较好，从而在虚拟机层面做很多优化。所以掌握虚拟化技术绝不仅仅只应用于云服务的场景，它可能会在各种意想不到的场景中发挥奇效。</p>","video_width":0,"column_could_sub":true,"video_id":"","sku":"100094901","video_cover":"","author_name":"海纳","column_is_onboard":true,"audio_url":"https://res001.geekbang.org/media/audio/06/e5/068cb03880312709ba3b84f2637066e5/ld/ld.m3u8","chapter_id":"2314","column_had_sub":true,"column_cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg","neighbors":{"left":{"article_title":"11 | 即时编译：高性能JVM的核心秘密","id":445925},"right":{"article_title":"13 | 存储电路：计算机存储芯片的电路结构是怎样的？","id":450519}},"rate_percent":0,"footer_cover_data":{"img_url":"https://static001.geekbang.org/resource/image/f0/fe/f0c47daf3262a8df6cf352d89896abfe.png","link_url":"https://time.geekbang.org/article/427012","mp_url":""},"float_app_qrcode":"","column_is_experience":false,"rate":{"1":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"2":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"3":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0}},"product_id":100094901,"had_liked":false,"id":446677,"free_get":false,"is_video_preview":false,"article_summary":"这节课，我将带你从内存入手来学习虚拟化技术。","column_sale_type":0,"float_qrcode_jump":"https://time.geekbang.org/serv/v4/misc/jump?uri=https%3A%2F%2Ftime.geekbang.org%2Fhybrid%2Fmp%2Fjump%3Furl%3Dhttps%253A%252F%252Fstatic001.geekbang.org%252Fresource%252Fimage%252Fee%252Fb2%252Feedd4090668aabfac91b2b445d27cbb2.png","column_id":450,"article_sharetitle":"12 | 内存虚拟化：云原生时代的奠基者","article_poster_wxlite":"https://static001.geekbang.org/render/screen/53/d9/532e558277045075696e34ab5384b1d9.jpeg","article_features":0,"comment_count":4,"audio_md5":"068cb03880312709ba3b84f2637066e5","offline":{"size":22633639,"file_name":"5d4b9222196ca28a3e34870b26ce6498","download_url":"https://static-acl-001.geekbang.org/resource/zip/article/446677/5d4b9222196ca28a3e34870b26ce6498.zip?auth_key=1641482237-a677e1c5966643c886c4da62ffbf057e-0-75d1c0d12b0aca9a16bdb6f683ff473f"},"video_size":0,"hls_videos":[],"video_time":"","article_could_preview":false,"article_ctime":1637251200,"share":{"content":"学好内存，掌握系统开发诀窍","title":"海纳 · 编程高手必学的内存知识","poster":"https://static001.geekbang.org/resource/image/bc/c5/bcaf3ff69e538f8749970edf2301bbc5.jpg","cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg"}},"447556":{"text_read_version":0,"audio_size":10362634,"article_cover":"https://static001.geekbang.org/resource/image/4b/41/4b8e4ca3b6e5d15b23da531f3a47ec41.jpg","subtitles":[],"product_type":"c1","audio_dubber":"海纳","is_finished":false,"like":{"had_done":false,"count":8},"audio_time":"00:10:47","video_height":0,"article_content":"<p>你好，我是海纳。</p><p>随着第12节课的完结，《编程高手必学的内存知识》的软件篇也就落下帷幕了。这段时间里，我和同学们进行了很多交流，看到有些人的留言和困惑，比如代码太多，不知道从哪里入手；再比如，有些代码需要的前置理论知识太多，不知道从哪里获取资料；还有感觉自己什么都想学，但又学得不深入等等。</p><p>其实这些问题，在我学习基础软件的道路上也曾经遇到过。所以做为过来人，我想和你分享一下这些年我学习基础软件的经验。我总结出了五条经验，和你一起探讨。</p><h2>以有涯随无涯，殆矣</h2><p>相信很多人都会有这样的困惑：在计算机知识上，我应该每一种都掌握一点，还是应该钻研一种呢？</p><p>我们知道，计算机相关的知识是十分庞大的，从芯片设计到图形渲染，从网络协议到编译优化，这些分门别类的知识，没有人能够完全掌握。正如庄子所说，知识是没有边界的，而且个人的精力是有限的，所以我们一定要选择最适合自己的道路去学习，切不可贪多务得。</p><p>对于年轻人来说，多尝试不同的方向，多了解不同的领域是必要的。但是在找到自己喜欢的方向后，还是要注重知识的深度，在一个领域重点深挖下去。</p><p>我们经常说既要注重知识的广度，又要注重知识的深度，要做一专多能的人才。但广度和深度是有区分的，人必须要先吃透一个领域，才能建立起自己的比较优势。</p><!-- [[[read_end]]] --><p>深度的建立，我建议你不必在所有的领域都精通，只要精通一个方向，就可以在团队中找到自己的位置，发挥独特的价值。</p><p>在建立起深度以后，就可以考虑一下拓宽广度了。这样你不仅可以更轻松地与其他领域的专家交流，提高工作效率，还能从全局把握软件的整体架构，为将来晋升到架构师做准备。</p><p>关于广度的建立，我建议你从自己的工作出发，先学习上下游的知识是最容易的。比如说，我最先开始学习操作系统内核，实现到文件系统的时候，我想实现自己的可执行文件，就需要了解编译器的知识，那么从操作系统过渡到编译器就是自然而然的。</p><p><strong>总之，请找到你自己热爱的方向，然后把有限的精力投入到最有价值的地方。</strong></p><h2>千里之行，始于足下</h2><p>工作两三年的程序员最爱问的问题是：我感觉现在每天都在写基本的增删查改，没有前途，我该怎么办？或者是：我觉得只有做操作系统和编译器才是真正的“软件开发”，我现在做的事情没有价值，我不想做了……像这样的问题，我经常会在微信上收到。</p><p>或许你也有类似的困惑，所以趁着这个机会，我来做一个统一的回答：在这世上，只有小程序员，没有小软件，不要好高骛远，你要永远记得路在脚下的道理。我见过太多喜欢拽名词的程序员，他们总觉得各种概念、专用名词和缩写说起来很有格调，而对于勤勤恳恳地实现业务需求、改进日常工作效率却兴趣缺缺。这种想法是不对的。</p><p>我有一个朋友，在一家互联网公司用Go语言写逻辑，他来问我说：“每天都在写增删查改，想学底层知识，又不知道如何下手”。我就问他：“既然你每天都用Go语言，应该对Go语言很熟吧，你能不能和我讲一下Goroutine怎么实现的？defer特性是怎么实现的？Go的GC算法与其他语言中的GC算法对比的优劣是什么样呢？”他支支吾吾很难回答上来。</p><p>其实，人对于熟悉的东西往往会熟视无睹，就像你每天都在使用的工具，如果别人随口一问，你未必能回答得上来。实际上，这些东西学习的门槛并不高，我们的工具链里有太多开源的软件，它们的源代码很容易获取，一些流行度比较高的工具，对它们的注释和解析更是唾手可得。所以只要你愿意留心，往深处挖掘，路就在你的脚下。</p><p>再来接着说我这个朋友，当时我给他的建议就是把Go语言吃透。比如说，可以先把所有的语法都熟练掌握，再去学习语言库的实现，然后可以进一步学习Docker和k8s的底层原理，再到Go的编译器实现和Go的内存管理等等高阶的知识。</p><p>通过这个故事，我想告诉你的是，<strong>虽然每个人的工作职责、学历背景和成长历程都不一样，但希望你一定不要好高骛远，只去贪图口嗨的快乐，而是应该去探索一条适合自己的成长之路，要脚踏实地。</strong></p><h2>锲而不舍，金石可镂</h2><p>我在评论区看到有同学说“内存知识学起来很难”，其实越是往计算机底层原理深挖，门槛就越高，学习的曲线也就更陡峭。当遇到瓶颈期的时候，锲而不舍就非常重要了。你不要怕慢，其实一天只需要搞懂一个小问题，取得一点点小进步就是可以的。</p><p>我曾经在学习Linux内核的时候，就一直有一个疑问：一个进程的空间是4G，那么简单计算一下，光是为了编码这4G的空间，就需要4M的页表，更不要说还有页目录表。这么看的话，启动一个进程至少就得4M的物理空间。那为什么不管是Windows还是Linux，启动一个hello world进程 ，我看到进程所占的内存资源都不大呢？</p><p>为了搞清楚这个问题，我就去翻看Linux内核代码。由于当时最新的内核版本是2.6，我就尝试从代码层面去理解，但这部分的代码太复杂了，我只能一点点地看，今天知道了mm_struct是干嘛的，明天搞懂了vm_area_struct……就这样一步步往前走。直到有一天，不知怎么，突然就把所有的关键点串了起来，页表设置申请分配的过程就全看明白了。</p><p><strong>原来，页表是按需要分配的</strong>。看到结果我哑然失笑，花费了那么多时间去查看那么复杂的代码，最终得到的结果就这么简单？</p><p>但实际上，在这个过程中，我收获的决不是只有这个“愚蠢”问题的答案，而是我每天都坚持看一点代码，即使看不懂也在一点点坚持，一个结构一个结构地、一个函数一个函数地去分析，最终把内存管理的其他知识也融会贯通了。</p><p>金庸在《侠客行》里讲石破天练太玄经的过程，和我经历的这个过程真的很像，我建议你去看一看这本小说，相信你会对如何坚持学习有很大感悟。</p><p><strong>总之，希望你记住，坚持不懈、锲而不舍才是学习底层知识的必备心理素质。</strong></p><h2>不愤不启，不悱不发</h2><p>在这个课程里，我看到了很多留言，很多同学的思考是非常有启发性的，比如说，Keepgoing同学在<a href=\"https://time.geekbang.org/column/article/440471\">第8节课</a>的留言就总结得非常好。这个留言总结了三种重定位时机，真正把符号重定位搞明白了。明显看出他经过了很深入的思考，这时候他再去提问就真正问到本质了。</p><p>为什么我要专门讲这一点呢？主要是因为我们这个专栏聚焦于内存管理，它不可能面面俱到，所以在各个环节难免会有一些你觉得没有太搞清楚的地方。这时候，思考就显得非常重要了。</p><p>我在每节课留下的思考题，你一定要去认真思考，这样才能有所收获。<strong>即使你一时半会儿想不到正确答案，那也要先进入“愤”和“悱”的状态，然后再来提问，这样再有人对你进行一点“启”和“发”，你就能立即获得非常深刻的理解了</strong>。</p><p>如果我只是简单地告诉你问题的答案，而你缺少了思考的过程，那么你不光是难以深刻理解这个问题，甚至还有可能会误解我的意思，从而建立了错误的概念。这样的话，你学得越多，就错得越多了，这可能还不如不学。那么在碰到问题的时候，我们应该怎么去思考呢？</p><p>其实，思考的过程就是要多建立抽象思维、多举例和多类比。比如说，我们在学习内存分配算法的时候，一开始学习的就是实际具体的内存分配算法。我们了解到，按需分配会产生外部碎片，把内存分割成相同大小的页，页与页之间紧密排列，可以解决外部碎片，但不能避免内部碎片。</p><p>后来，我们又了解到，进程调度的过程也是一个对时间区间进行分配的过程，如果你清楚寄存器分配算法的话，你应该也能理解，它的本质是对变量生命周期，进行时间区间的分配。</p><p>这样我们就通过多个领域的学习，建立起了一种更高的抽象思维：<strong>这些问题的本质都是区间的分配和管理。</strong></p><p>在这种更高级的思维下，我们再去学习空闲链表、伙伴系统和线段树等用于区间管理的数据结构和算法，你就能触类旁通、事半功倍了。比如Slab分配器，虽然我们这个课程里没有提到过，但你从区间管理的视角去看，它不过就是类似于页管理那样，把空间分割成更小的同等大小的多块空间而已，一种大小适应一种内核数据结构。</p><p><strong>建立了这种抽象思维，当你下次再遇到类似的问题，就能举一反三了。</strong></p><h2>学而不思则罔，思而不学则殆</h2><p>学与思的有机结合，我相信你从中学时代就已经非常熟悉了，这里我还是想结合具体的例子再讲一下如何去运用“学”和“思”。</p><p>当你了解了一个现象或者一个API的用法，想要知道它的原理的时候，不要直接就去翻代码。这时候，我建议你停下来思考一下，如果这个功能让你来设计，你会怎么做？</p><p>当你把每个环节都设计得差不多了，再去源代码里验证，看看别人的设计和你的设计是不是相同的。如果差别比较大，你就可以对比两种设计，找出它们各自的优缺点，同时思考自己设计的差距在哪里。如果你的设计比较好，那就正好是一个可以贡献开源，提升个人影响力的好机会了。</p><p>我之前在研究Linux信号机制的时候，就是先停下来想：<strong>PCB中肯定要有信号相关的数据结构，内核提供signal系统调用肯定还要有能力，根据ID号找到相应的PCB，然后把它里面与信号相关的标志置位，那目标进程什么时候能处理这个信号呢</strong>？</p><p>经过思考，我猜想：因为信号的处理都是在内核中的，所以放在中断处理的前面或者后面是比较合理的，毕竟中断才是用户态和内核态切换的时机。</p><p>想清楚了这些以后，我再去翻看代码，在我猜想的地方，果然都找到了相关的数据结构和函数。其实，如果你稍加留意就会发现，我们专栏的<a href=\"https://time.geekbang.org/column/article/444178\">第10节课</a>就是按照这种思路去写的：<strong>学思结合，思在前，学在后</strong>。这也是我学习基础软件的基本思路。我希望你在接下来的课程学习中，可以多去注意体会。</p><p>好了，以上就是我想和你分享的内容，我向你大概介绍了学习计算机底层原理和基础软件的几条注意事项，希望能对你有所帮助。我们下节课再见！</p>","float_qrcode":"https://static001.geekbang.org/resource/image/83/1c/83e244537b714e899fd2e7896e22371c.png","article_cover_hidden":false,"is_required":true,"score":"61637510400","like_count":5,"article_subtitle":"","audio_download_url":"https://static001.geekbang.org/resource/audio/80/aa/80b638b366534bb797bf04926b1157aa.mp3","had_viewed":false,"article_title":"不定期福利第一期 | 海纳：我是如何学习计算机知识的？","column_bgcolor":"#F6F7FB","offline_package":"{}","audio_title":"11.21 海纳 12.5_summary_R.mp3","audio_time_arr":{"m":"10","s":"47","h":"00"},"text_read_percent":0,"cid":450,"article_cshort":"<p>你好，我是海纳。</p><p>随着第12节课的完结，《编程高手必学的内存知识》的软件篇也就落下帷幕了。这段时间里，我和同学们进行了很多交流，看到有些人的留言和困惑，比如代码太多，不知道从哪里入手；再比如，有些代码需要的前置理论知识太多，不知道从哪里获取资料；还有感觉自己什么都想学，但又学得不深入等等。</p><p>其实这些问题，在我学习基础软件的道路上也曾经遇到过。所以做为过来人，我想和你分享一下这些年我学习基础软件的经验。我总结出了五条经验，和你一起探讨。</p><h2>以有涯随无涯，殆矣</h2><p>相信很多人都会有这样的困惑：在计算机知识上，我应该每一种都掌握一点，还是应该钻研一种呢？</p><p>我们知道，计算机相关的知识是十分庞大的，从芯片设计到图形渲染，从网络协议到编译优化，这些分门别类的知识，没有人能够完全掌握。正如庄子所说，知识是没有边界的，而且个人的精力是有限的，所以我们一定要选择最适合自己的道路去学习，切不可贪多务得。</p><p>对于年轻人来说，多尝试不同的方向，多了解不同的领域是必要的。但是在找到自己喜欢的方向后，还是要注重知识的深度，在一个领域重点深挖下去。</p><p>我们经常说既要注重知识的广度，又要注重知识的深度，要做一专多能的人才。但广度和深度是有区分的，人必须要先吃透一个领域，才能建立起自己的比较优势。</p>","video_width":0,"column_could_sub":true,"video_id":"","sku":"100094901","video_cover":"","author_name":"海纳","column_is_onboard":true,"audio_url":"https://res001.geekbang.org/media/audio/80/aa/80b638b366534bb797bf04926b1157aa/ld/ld.m3u8","chapter_id":"2364","column_had_sub":true,"column_cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg","neighbors":{"left":{"article_title":"24 | GC实例：Python和Go的内存管理机制是怎样的？","id":470918},"right":{"article_title":"不定期福利第二期 | 软件篇答疑","id":454080}},"rate_percent":0,"footer_cover_data":{"img_url":"https://static001.geekbang.org/resource/image/f0/fe/f0c47daf3262a8df6cf352d89896abfe.png","link_url":"https://time.geekbang.org/article/427012","mp_url":""},"float_app_qrcode":"","column_is_experience":false,"rate":{"1":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"2":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"3":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0}},"product_id":100094901,"had_liked":false,"id":447556,"free_get":false,"is_video_preview":false,"article_summary":"请找到你自己热爱的方向，然后把有限的精力投入到最有价值的地方。","column_sale_type":0,"float_qrcode_jump":"https://time.geekbang.org/serv/v4/misc/jump?uri=https%3A%2F%2Ftime.geekbang.org%2Fhybrid%2Fmp%2Fjump%3Furl%3Dhttps%253A%252F%252Fstatic001.geekbang.org%252Fresource%252Fimage%252Fee%252Fb2%252Feedd4090668aabfac91b2b445d27cbb2.png","column_id":450,"article_sharetitle":"不定期福利第一期 | 海纳：我是如何学习计算机知识的？","article_poster_wxlite":"https://static001.geekbang.org/render/screen/46/dd/46d9f994f38c11b148574b623cb5ffdd.jpeg","article_features":0,"comment_count":3,"audio_md5":"80b638b366534bb797bf04926b1157aa","offline":{"size":10877463,"file_name":"3f879a962017954f5bf766e4720243c4","download_url":"https://static-acl-001.geekbang.org/resource/zip/article/447556/3f879a962017954f5bf766e4720243c4.zip?auth_key=1641482445-6bbce9e508e94994b98148346c5a597b-0-89712f51946574db885b55fc23ea0971"},"video_size":0,"hls_videos":[],"video_time":"","article_could_preview":false,"article_ctime":1637510400,"share":{"content":"学好内存，掌握系统开发诀窍","title":"海纳 · 编程高手必学的内存知识","poster":"https://static001.geekbang.org/resource/image/bc/c5/bcaf3ff69e538f8749970edf2301bbc5.jpg","cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg"}},"450519":{"text_read_version":0,"audio_size":22746689,"article_cover":"https://static001.geekbang.org/resource/image/5c/10/5c02d6937bb27e771a0e5d129a2a2910.jpg","subtitles":[],"product_type":"c1","audio_dubber":"海纳","is_finished":false,"like":{"had_done":false,"count":2},"audio_time":"00:23:41","video_height":0,"article_content":"<p>你好，我是海纳。</p><p>之前的课程，我们从软件的角度学习了内存管理的基本知识。从这一节课开始，我们把注意力转向内存的硬件实现。掌握硬件篇的知识，是你学习计算机组成原理和体系结构的基础。而且，计算机体系结构中最常用的手段就是合理地使用各种器件，通过体系手段来使得它们扬长避短，形成有机的整体。</p><p>可以说，深刻地掌握计算机的体系结构，就是你写出高性能代码的关键。那么，这么重要且基础的部分，为什么我会放到现在才讲呢？这是因为，程序员日常打交道的是软件接口，硬件的感知度不高。所以在有了前面软件篇的知识后，我们才能更好地理解硬件上的各种晦涩的概念。</p><p>整个硬件篇的内容主要就是聚焦于，各种不同的存储器和它们的器件是如何组成高效、大容量、低成本的存储体系结构的。而各类存储器的基本原理是存储体系结构的基础。</p><p>我们把用于存储数据的电路叫做存储器，按照到CPU距离的远近，存储器主要分为寄存器、缓存和主存。今天这节课，我们就来重点分析这三种存储器的特点、原理，以及应用场景。</p><p>存储器是由基本的存储单元组成的，要想搞清楚存储器原理，我们还要先搞明白基本的存储单元是什么，它又是怎么工作的，我们先按寄存器、缓存和主存的顺序，逐个分析。</p><p>首先，我们来看寄存器的存储单元是什么样的。</p><!-- [[[read_end]]] --><h2>寄存器存储单元</h2><p>在<a href=\"https://time.geekbang.org/column/article/430173\">导学（一）</a>里，我曾经讲过组合逻辑电路和时序逻辑电路的区别。<strong>组合逻辑电路是指，输出仅由输入信号的状态决定的电路。而时序逻辑电路是指，电路的输出值同时依赖于电路过去的状态和现在的输入，所以时序逻辑电路中含有用于记忆电路状态的存储单元</strong>。</p><p>接下来，我们就从最简单的具有存储功能的电路开始，逐步将它扩展成相对复杂的存储电路，以此来深度拆解存储电路的运行原理。</p><h3>RS锁存器</h3><p><strong>我们把具有存储信息能力的电路，称为存储器</strong>。其中，RS锁存器(Latch)是最简单的一种存储器电路，它可以存储一个比特，如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/1c/e8/1c535857b2d5d0725662f1af835fb5e8.jpg?wh=2284x1378\" alt=\"\"></p><p>上图中的电路由两个或非门组成，它的特点是，图上方的或非门的输出作为图下方的或非门的输入，反过来，图下方的或非门的输出，也是图上方的或非门的输入。我来分析一下这个电路的特点，你就能理解这个电路是如何完成一个比特的存储的。</p><p>一开始，输入端R和S都是低电压，代表0，上方的或非门输出为0，下方的或非门输出为1，这是一种合法状态。或者下方的或非门的输出为0，上方的或非门的输出为1，这也是一种合法状态。也就是说这个电路在R和S都为0的时候，有两种合法的稳定状态。</p><p>如果此时，S变成高电压，也就是1，那么下方或非门的输出就变成0，进而导致上方或非门的输出变成1，也就是Q变成1。这个时候，如果S又变成低电压的话，因为上方或非门的输出为1，所以下方或非门的输出仍然保持为0。而输入端R仍然为0，这就使得上方或非门的两个输入端都为0，进而Q的高电压可以得到保持。</p><p>于是，我们就看到了神奇的一幕：输入端S变为1以后，可以将输出端Q变成1，但是当S变为0以后，输出端仍然保持1。这就说明这个电路可以存储1。</p><p>当R和S都是0、Q为1时，如果此时R变成1，由于电路是对称的，可以很容易分析得到Q将变成0，Q反（表示$\\bar{Q}$，下同）变成了1。同样地，如果此时R再变成0，Q为0，Q反为1的状态仍然可以保持。这就实现了一个可以保持一个比特的存储器。</p><p>经过上面的分析，我们总结出它的真值表，如下所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/d5/11/d5b3f0931bcc285518f390ac0aec3d11.jpg?wh=2284x1300\" alt=\"\"></p><p>简单说，就是当S为1，R为0时，Q为1；当S为0，R为1时，Q为0。当S和R都为0时，Q保持原来的状态，若原来Q的状态是0，则Q保持为0，如果原来是1，则保持为1。Q和Q反一直保持相反的状态。</p><p>R和S都为1是非法状态，这并不是说这种电路真的不允许R和S都为1，而是当它们都为1时，Q和Q反都为0，这不符合Q和Q反保持相反状态的假设，从而失去了存储信息的功能。所以我们对于S和R都为1的情况就不再讨论了。</p><p>除了或非门组成的锁存器，还可以使用与门和或门构成锁存器，也可以使用与非门来构成锁存器。锁存器的特点是输入一旦发生变化，输出端立即就能反应出这种变化。我们知道，RS锁存器的输入信号可能会不稳定，那当输入端的毛刺影响输出端时，该怎么办呢？</p><h3>D锁存器</h3><p>为了让存储电路可以保持记忆的能力，我们可以考虑为这种电路引入保持位，当保持位为1时，数据可以被存储进电路，当存储完成以后，保持位变为0，输入就不能再影响输出了。</p><p>如下图所示，通过增加两个与门来实现保持位C。当保持位为1时，RS锁存器的功能与不加保持位时完全相同，当保持位为0时，则R端和S端不论取什么值，都不再起作用了。</p><p><img src=\"https://static001.geekbang.org/resource/image/a5/f7/a5cb989f8322070e1764faabdc0fddf7.jpg?wh=2284x1300\" alt=\"\"></p><p>我们再对上图进行更深一步的分析，R和S同时为0的情况，可以使用保持位为0来代替。R和S同时为1的情况是非法情况，所以我们可以使用一个非门，把R和S合并成一个信号D，这样的话，电路的输入端就可以进一步化简，如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/e1/c6/e1e3db1b06e9b3b6a8397c94d99fdac6.jpg?wh=2284x1300\" alt=\"\"></p><p>上面电路的保持位为1时，输出Q可以反应输入D的变化，当保持位为0时，输出Q保持原来的值不变。这种电路被称为电平触发的D型锁存器。其中的D代表数据Data。</p><p>我们知道信号在传输的过程中容易产生毛刺等不稳定的现象，所以在保持位为1的期间，如果输入信号还是不能稳定的话，那么输出将随着输入的改变而发生相应的变化。这种情况下，锁存器的状态就难以稳定。我们把在保持位为1期间，锁存器发生多次翻转的情况称为空翻。</p><p>在实际应用中，人们还是希望存储电路有良好的稳定性，从这个角度上看，电平触发的锁存器的抗干扰能力相对较差。</p><h3>D触发器</h3><p>为了解决锁存器稳定性不好的问题，人们在实现存储电路时，常用的方法是使用两个D锁存器来实现一个触发器。如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/0d/bb/0d7c3c6cf319e32557f2ff192f8f85bb.jpg?wh=2284x1300\" alt=\"\"></p><p>D触发器有D(Data)和C(Clock)两个输入信号，Q和Q反两个输出信号。D触发器由两个D锁存器组成。当时钟信号C为0时，前端锁存器输出信号D的值，后端锁存器保持之前的数据。当C为1时，前端锁存器保持之前的数据，后端锁存器将前端锁存器保持的数据直接通过Q输出。</p><p>可以看出来，对于D触发器而言，只有在C从0变成1的瞬间，输入数据D的值才会反应到输出Q上。所以这种触发器就被称为上升边沿触发的触发器。</p><p>虽然触发器可以在一个CPU时钟周期内完成读写，是最快的一类存储单元，但是它的造价十分高昂，用于存储一个比特的触发器就要使用几十个晶体管。所以它往往用来实现CPU内部的寄存器，一个寄存器的位宽不过64比特，它的电路面积的消耗还是可以接受的。</p><p>但如果我们需要大规模，比如几十K，甚至几M的存储时，就不得不考虑更加节约的方案了，这就要用到大容量的存储器。接下来，我们再来看一下大容量存储器的工作原理。</p><h2>大容量存储器</h2><p>普通的大容量存储器大致可以分为两类，<strong>一类是只能读，不能写，这种存储器称为只读存储器</strong>（Read Only Memory，ROM），在生产的时候，厂家将内容写入存储器之后，用户就只能从其中读取数据，不能再更改其中的内容了。</p><p><strong>另一类是可以支持读和写操作的存储器，它们被称为随机访问存储器</strong>(Random Access Memory, RAM)，随机访问这个名字是与顺序访问相对应的。早期的存储设备，例如纸带，磁带等等，只能顺序访问，如果想要跳到某一个特定位置进行播放，只能将磁带快进到我们所需要的那个位置，然后再顺序地访问该位置的数据。而支持随机访问的存储器则可以用同样的速度访问存储器内任何位置上的数据。</p><p>由于ROM大多用在一次烧制，永远不需要更新的地方，这往往不是软件程序员所关心的话题。所以，我们这个专栏课里所讲的内存，都是指可读写的RAM，所以我们下面就来详细地分析RAM的工作原理。</p><p><strong>RAM大体上分为两类：静态随机访问存储器（Static RAM, SRAM）和动态随机访问存储器（Dynamic RAM，DRAM）</strong>。SRAM的特点是速度快、造价高，往往用来制作高速缓存，集成在CPU里，它的容量一般不会超过几兆字节。DRAM的特点是速度慢、造价低，计算机中的主存就是DRAM，单根内存条就可以有十几，甚至几十G的容量。</p><p>接下来，我们就分别考察SRAM和DRAM的电路结构，来搞清楚它们的原理。</p><h3>SRAM的电路结构</h3><p>为了节约电路面积，SRAM采用了6管式的存储电路。如图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/19/01/19b788c9626d58382842f8db7059b201.jpg?wh=2284x1300\" alt=\"\"></p><p>在上图中，M1和M3两个MOS管，是N沟道场效应管，在高电压时导通；而M2和M4这两个MOS，则是P沟道场效应管，在低电压时导通。本质上，M1和M2一起组成了一个非门，M3和M4一起组成了另一个非门，这两个非门的输出互为对方的输入，这样，两个非门就组成了一种可以存储比特值的电路。</p><p>访问SRAM时，字线(Word Line, WL)加高电平，使得每个基本单元的两个控制开关：晶体管M5与M6开通，把存储单元与位线(Bit Line, BL)连通。位线用于读取或写入基本单元的保存状态。</p><p>我们假定储存的内容为1，即在Q处的电平为高。读取周期开始时，两条位线预先设成高电平，随后字线WL变成高电平，使得两个访问控制晶体管M5与M6导通。Q的高电平使得晶体管M1导通，而Q反与BL反的预充值不同，使得BL反经由M1与M5放电而变成逻辑0。在位线BL一侧，Q反的低电平使得M4导通，再加上M6通路，位线就连接到VDD的高电压。</p><p>如果储存的内容为0，相反的电路状态将会使BL反为1，而BL为0。这时，只需要BL与BL反有一个很小的电位差，读取的放大电路就会辨识出哪条位线是1，哪条是0。也就是说，当敏感度越高时，读取的速度就越快。</p><p>在写入周期开始时，把要写入的状态加载到位线。如果要写入0，则设置BL反为1且BL为0。随后字线WL加载为高电平，位线的状态被加载进SRAM的基本单元。</p><p>简单说，<strong>SRAM存储单元的特点是使用6个晶体管来实现。其中两个P型MOS管和两个N型MOS管组成两个反相器用于存储信息。还有两个用于控制存储单元是否选通。6管SRAM的结构比触发器简单，速度也比较快</strong>。</p><p>讲到这里，你就清楚SRAM的电路结构了，下面我们来看看DRAM的电路结构。</p><h3>DRAM的电路结构</h3><p>DRAM相比起SRAM，它的电路结构更简单，它是由一个CMOS开关和一个电容组成的，如图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/21/9d/215f58b1255e6433c69310044cac089d.jpg?wh=2284x1503\" alt=\"\"></p><p>当WL为高电压时，开关打开，存储单元选通，如果此时数据线BL为高电压，则向电容中充电，这就是DRAM存储单元中写1，如果数据线为低电压，电容放电，这就是往存储单元中写0。</p><p>在读取的时候，同样要打开开关，如果电容放电，那么就表示这个单元里原来存的值是1，如果电容不放电，则表示原来的值是0。可见，DRAM的读取是破坏性的，它会使得原来为1的存储单元变成0。</p><p>为了解决这个问题，在读取DRAM的数据的时候，人们要想办法给原来为1的比特再进行一次充电。另外，电容本身也会缓慢漏电，所以存储器也要每隔一段时间就为电容补充电荷。这也是Dynamic这个名称的由来。</p><p><strong>DRAM的存储单元使用一个MOS管和一个电容实现，其特点是电路相比SRAM更加简单，也就更容易大规模集成，成本也更低，但是它的读取速度比较慢</strong>。</p><p>到这里，我们就把用于存储一个比特的常见电路介绍完了。但存储器的容量是十分巨大的，这又是怎么做到的呢？换句话说，一个字节是由8个比特组成，而一根容量为4G的内存条，它包含的比特是$4\\times2^{32}\\times8$=$2^{37}$个。存储器是如何知道CPU要读写哪个比特的呢？这就要进一步了解存储器是如何对字节和比特进行地址编码的。</p><h2>存储器的地址编码</h2><p>存储器在存储数据时，一定要区分数据是要存在哪个地方的，也就是说，我们要想办法对存储器里的各个单元进行编码，并且能将地址总线的数据转换成相应存储单元的使能信号，然后进一步区分控制总线的读写信号。<strong>如果是读操作，存储控制器就要将存储单元内的值读入数据总线，如果是写操作，控制器就应该把数据总线的数据写入存储单元</strong>。</p><p>我们假设存储器是按字节进行编码，一次读写最少也要操作一个字节，数据总线的宽度是8位。这种存储器在制造时，要将每一个字节的第n比特的位线都连接到数据总线的第n位，也就是8个比特的位线分别与数据总线的8位相连。而存储控制器则根据地址总线的值将对应的字节的字线设成高电平，以选通目标字节。</p><p>它的简单示意图如下所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/c3/c8/c3fb9f555509c0e8370b4b4a5feb43c8.jpg?wh=2284x1752\" alt=\"\"></p><p>从这个角度，我们也就能理解，存储单元的字线和位线为什么要这样命名了。这是因为字线可以对存储单元所在的机器字进行选通，而位线是真的接在了数据总线的某一位上。</p><p>现在的核心问题在于，<strong>控制器如何对地址信号进行转换，使得目标字节的字线变成高电压</strong>？</p><p>其实，这个工作需要译码器来完成。我们以4位地址总线来举例。首先，4位地址总线可以编码的地址是0x0~0xf，共计16个。如上图所示。</p><p>上图中的电路有4个输入，16个输出，16个输出分别对应了存储器的16个存储单元的字线。当地址总线上的数据是0000时，我们希望第0个字节的字线是高电压，当地址总线上的数据是0001时，第1个字节的字线就是高电压，依次类推。我们会发现，地址总线上的数据刚好就是存储单元地址编码的二进制数。这里也就说明了为什么地址总线的宽度会影响存储编码范围了。</p><p>我们可以把这个电路的真值表总结出来：</p><p><img src=\"https://static001.geekbang.org/resource/image/73/ab/7384a91ab0a84e6fbae496ab5e2211ab.jpg?wh=2284x1389\" alt=\"\"></p><p>接下来，我们由真值表写出16个输出端的表达式：</p><p>Y0 = (~A0) &amp; (~A1) &amp; (~A2) &amp; (~A3)，Y2 = (~A0）&amp; (~A1) &amp; (~A2) &amp; A3……，Y15 = A0 &amp; A1 &amp; A2 &amp; A3</p><p>这个表达式再翻译成组合逻辑电路是简单的，只需要将A0、A1、A2、A3都取反，然后将这个4个信号使用与门连接，它的输出就是Y0了。同样地，把A0、 A1、 A2取反，再与A3一起使用与门连接，这个输出就是Y1。</p><p>作为示意，我画出了Y0、Y1和Y15的电路图，其余的作为练习，希望你可以模仿我的例子自己补充。</p><p><img src=\"https://static001.geekbang.org/resource/image/79/c6/7957215f5ecf0af75a9164999dba98c6.jpg?wh=2284x1675\" alt=\"\"></p><p>这种电路就是译码器，我们把4输入，16输出的译码器称为4-16线译码器。使用译码器就可以解决字线选通的问题。</p><p>但这样做会带来一个新的问题，那就是随着存储器地址范围变大，译码器的输出端口会变得非常多，比如32位地址总线会有4G个输出端。为了解决这个问题，人们把地址平分成高低两个部分，并且把存储单元做成矩阵式排列，使用高地址决定存储单元所在的行，使用低地址决定它所在的列。</p><p>为了方便，我们仍然以16个字节为例。它排成存储矩阵以后是这样子的：</p><p><img src=\"https://static001.geekbang.org/resource/image/6d/49/6d97c0c3715e45bc411b58321422ac49.jpg?wh=2284x1683\" alt=\"\"></p><p>我们将4位地址分成两组，分别送入两个译码器。高地址送入行译码器，用于选通行线，低地址送入列译码器，用于选通列线。只有行线和列线同时选通的存储单元才是有效的。在这种设计里，32位地址总线的字线由原来的4G下降为64K+64K=128K。这样就大大减少了字线的数量。</p><p>使行线和列线同时生效也有两种做法，一种是在每个存储单元引入一个与门，这需要为每个字节多增加两个MOS管，会降低芯片的集成度。</p><p>另一种是在存储芯片里再增加一行的缓存，读取时分两步进行，第一步先使行线生效，将目标存储单元所在行，整行读入到缓存中；第二步再使列线生效，从缓存中读取目标单元的值。这种做法需要两次读取，性能会差一些。</p><p>存储器的编码主要依赖译码器这种电路结构。它的特点是每一个输出端与一个存储单元的字线相连。译码器将地址总线发送过来的地址作为输入，然后只有一个输出成为高电平，这样就选通了一个存储单元，从而实现了对多个存储单元的地址编码。</p><p>在不同场景根据相应的约束，选择可能就会不一样。从上面的分析中，我们可以看到，没有哪一种存储电路是十全十美的，每一种都有自己的优点和缺点，如何把它们用在合适的场景，从而以系统化的方法做到扬长避短，才是存储系统设计的首要目标。</p><h2>只读存储器简介</h2><p>虽然我们说只读存储器往往被用在固件中或者闪存中，断电后，存储的数据也不会消失。它不是内存，但作为现在存储体系的必要组成，对它有所了解还是必要的。所以，这里我就对ROM做一个简单的介绍。</p><p>存储的本质就是把0和1存到特定的地方，然后在需要恢复的时候，再把数据恢复出来。RAM的特点是存储数据时需要加电，一旦断电，则存储的数据消失。而ROM中的信息断电也不会消失，它非常像纸笔这种记录信息的手段，不易更改，保存时间长。</p><p>早期的ROM，是基于熔丝制作的，写入操作是通过烧断熔丝进行的，这种类型的ROM只能在初始化时写入一次，之后就不能再更改了。当前，这种类型的ROM已经不存在了。</p><p>人们在MOS管上增加浮置栅，这个特殊的浮栅中可以存储电荷。当有电荷时代表1，没有电荷时代表0。在一定的条件下可以为浮栅充电，也可以为它放电。所以，这就是可编程ROM（Programmable ROM, PROM)。</p><p>早期的PROM，是可以使用紫外线进行擦除的，这就是紫外线可擦除可编程ROM（Ultral Violet Erasable Programmable ROM，UV-EPROM)。使用光进行擦除，有一个缺点就是擦除速度比较慢，而且可擦除次数有限。</p><p>为了克服上面的两个缺点，人们又发明了电可擦除ROM（Electronic Erasable ROM, EEROM)。<strong>这就是当今使用得最广泛的一种存储器件，它有个专门的名字叫做闪存</strong>。</p><p>到这里，几种存储器件是如何存储一个比特的，这个问题，我们就已经搞清楚了。</p><h2>总结</h2><p>这节课，我们分别学习了CPU中寄存器的存储单元，SRAM和DRAM的存储单元，以及ROM的存储单元的基本原理。从中，我们能了解到，CPU中的寄存器使用触发器存储一个比特，读写速度最快，但所占电路面积最大。</p><p>在这节课开头，我带你从最简单的锁存器电路开始，先改进成电平触发的D锁存器，然后通过两个D锁存器搭建了上升沿触发的D触发器。D触发器配合时钟工作，提供稳定的，可控的，快速的存储能力。</p><p>D触发器的优点很多，但是缺点也很明显，那就是电路比较复杂、成本高、难以高度集成。所以我们使用6管存储单元来替换D触发器，6管存储单元所组成的存储器就是静态随机读写存储器（SRAM）。</p><p>SRAM的读写速度比较快，造价也更低，但集成度仍然不如单MOS管的动态随机存储器（DRAM）。DRAM的原理是使用电容的充放电状态来代表0和1，电容的特点是读数据时，它的状态会被破坏，所以需要以一定的频率对电容进行充电刷新。</p><p>最后，我们简单了解了ROM和闪存的发展历史和分类，从中也可以看到ROM这个名字其实已经名不副实了。我们常把闪存归到外存储器分类中，所以我们这个专栏就没有使用大量的篇幅对它进行介绍，但是闪存和当前非常流行的非易失性内存是存储系统的重要部分。如果你对这部分内容比较感兴趣，可以继续阅读<a href=\"https://book.douban.com/subject/3245122\">《大话存储 》</a>等资料进行学习。</p><h2>思考题</h2><p>我们在设计各种存储器件的时候，要不断地平衡它们的优缺点。请你思考一下，在存储电路设计中人们要平衡哪些因素呢？举个例子，既然D触发器的速度最快，那我使用D触发器来制作主存可以吗？为什么？欢迎你在留言区分享你的想法和收获，我在留言区等你。</p><p><img src=\"https://static001.geekbang.org/resource/image/df/16/df56a0f855c975883001af146e9fed16.jpg?wh=2284x1601\" alt=\"\"></p><p>好啦，这节课到这就结束啦。欢迎你把这节课分享给更多对计算机内存感兴趣的朋友。我是海纳，我们下节课再见！</p>","float_qrcode":"https://static001.geekbang.org/resource/image/83/1c/83e244537b714e899fd2e7896e22371c.png","article_cover_hidden":false,"is_required":true,"score":"41637683200","like_count":1,"article_subtitle":"","audio_download_url":"https://static001.geekbang.org/resource/audio/ed/ce/ed8e941773650977795920286a2d05ce.mp3","had_viewed":false,"article_title":"13 | 存储电路：计算机存储芯片的电路结构是怎样的？","column_bgcolor":"#F6F7FB","offline_package":"{}","audio_title":"11.23 海纳 13_circuit.MP3_R.mp3","audio_time_arr":{"m":"23","s":"41","h":"00"},"text_read_percent":0,"cid":450,"article_cshort":"<p>你好，我是海纳。</p><p>之前的课程，我们从软件的角度学习了内存管理的基本知识。从这一节课开始，我们把注意力转向内存的硬件实现。掌握硬件篇的知识，是你学习计算机组成原理和体系结构的基础。而且，计算机体系结构中最常用的手段就是合理地使用各种器件，通过体系手段来使得它们扬长避短，形成有机的整体。</p><p>可以说，深刻地掌握计算机的体系结构，就是你写出高性能代码的关键。那么，这么重要且基础的部分，为什么我会放到现在才讲呢？这是因为，程序员日常打交道的是软件接口，硬件的感知度不高。所以在有了前面软件篇的知识后，我们才能更好地理解硬件上的各种晦涩的概念。</p><p>整个硬件篇的内容主要就是聚焦于，各种不同的存储器和它们的器件是如何组成高效、大容量、低成本的存储体系结构的。而各类存储器的基本原理是存储体系结构的基础。</p><p>我们把用于存储数据的电路叫做存储器，按照到CPU距离的远近，存储器主要分为寄存器、缓存和主存。今天这节课，我们就来重点分析这三种存储器的特点、原理，以及应用场景。</p><p>存储器是由基本的存储单元组成的，要想搞清楚存储器原理，我们还要先搞明白基本的存储单元是什么，它又是怎么工作的，我们先按寄存器、缓存和主存的顺序，逐个分析。</p><p>首先，我们来看寄存器的存储单元是什么样的。</p>","video_width":0,"column_could_sub":true,"video_id":"","sku":"100094901","video_cover":"","author_name":"海纳","column_is_onboard":true,"audio_url":"https://res001.geekbang.org/media/audio/ed/ce/ed8e941773650977795920286a2d05ce/ld/ld.m3u8","chapter_id":"2389","column_had_sub":true,"column_cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg","neighbors":{"left":{"article_title":"12 | 内存虚拟化：云原生时代的奠基者","id":446677},"right":{"article_title":"14 | CPU Cache：访存速度是如何大幅提升的？","id":460545}},"rate_percent":0,"footer_cover_data":{"img_url":"https://static001.geekbang.org/resource/image/f0/fe/f0c47daf3262a8df6cf352d89896abfe.png","link_url":"https://time.geekbang.org/article/427012","mp_url":""},"float_app_qrcode":"","column_is_experience":false,"rate":{"1":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"2":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"3":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0}},"product_id":100094901,"had_liked":false,"id":450519,"free_get":false,"is_video_preview":false,"article_summary":"从这一节课开始，我们把注意力转向内存的硬件实现。掌握硬件篇的知识，是你学习计算机组成原理和体系结构的基础。","column_sale_type":0,"float_qrcode_jump":"https://time.geekbang.org/serv/v4/misc/jump?uri=https%3A%2F%2Ftime.geekbang.org%2Fhybrid%2Fmp%2Fjump%3Furl%3Dhttps%253A%252F%252Fstatic001.geekbang.org%252Fresource%252Fimage%252Fee%252Fb2%252Feedd4090668aabfac91b2b445d27cbb2.png","column_id":450,"article_sharetitle":"13 | 存储电路：计算机存储芯片的电路结构是怎样的？","article_poster_wxlite":"https://static001.geekbang.org/render/screen/0e/75/0e3a367ed63cac657d1f7ce798dac175.jpeg","article_features":0,"comment_count":13,"audio_md5":"ed8e941773650977795920286a2d05ce","offline":{"size":23413658,"file_name":"16738db2e67b9c5da01e33a485a2f98a","download_url":"https://static-acl-001.geekbang.org/resource/zip/article/450519/16738db2e67b9c5da01e33a485a2f98a.zip?auth_key=1641482253-99902b36f42d48fe97c110913835c93d-0-aeafad59aa857be855b3737e7d098f55"},"video_size":0,"hls_videos":[],"video_time":"","article_could_preview":false,"article_ctime":1637683200,"share":{"content":"学好内存，掌握系统开发诀窍","title":"海纳 · 编程高手必学的内存知识","poster":"https://static001.geekbang.org/resource/image/bc/c5/bcaf3ff69e538f8749970edf2301bbc5.jpg","cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg"}},"454080":{"text_read_version":0,"audio_size":11015239,"article_cover":"https://static001.geekbang.org/resource/image/43/86/43df0f7ce9514492cd08e6fcab7d3a86.jpg","subtitles":[],"product_type":"c1","audio_dubber":"海纳","is_finished":false,"like":{"had_done":false,"count":4},"audio_time":"00:11:28","video_height":0,"article_content":"<p>你好，我是海纳。</p><p>随着课程的不断更新，同学们的留言也越来越精彩。我也经常会到这门课的留言区转一转，大部分的疑问，我都已经在留言区做了回复，但我知道你可能在一些地方还存在着疑问。所以，我在这里将同学们问得比较多的问题提炼出来，统一地进行解答，同时，我还整理了软件篇中难度比较大的课后思考题答案，希望能给你带来帮助。</p><p>那我们先从高频问题的解答开始吧。</p><h2>高频留言答疑</h2><p><strong>问题一：JVM的内存布局和Linux进程的内存布局有什么关系？它们是一样的吗？</strong></p><p>这个问题看起来很难，但你可以从JVM的原理入手来找到突破点。典型的JVM，比如Hotspot，它们运行起来以后，也是操作系统上一个普通进程而已。所以，我们可以推论，hotspot进程也有自己的代码段、数据段、堆和栈。我们按照从简单到复杂的顺序来展开Hotspot进程中的内存布局。</p><p>首先，我们先看代码段。Hotspot运行以后，它的代码段里存放的不是Java代码，而是虚拟机自己的代码。这些代码都是由C++编写的，它实现了虚拟机的逻辑。Hotspot会新开辟一段区域，叫做Metaspace，用于加载Java的class文件，并且Java的类信息、方法定义的字节码都存储在这里。所以，从Linux进程的角度看，这一块内存其实是操作系统的堆内存，只不过被用来存储Java字节码了而已。</p><!-- [[[read_end]]] --><p>然后，我们再看栈。Java原生支持多线程，它的线程创建，本质上是使用pthread编程接口实现的。所以Java的线程栈是直接复用了系统线程栈的。但是，你一定要注意，hotspot栈的操作方式和gcc编译C代码的操作方式不一样。Hotspot将栈帧分成了变量表和操作数栈两部分。<strong>字节码在执行过程中是在操作栈帧里的操作数栈</strong>。这一点你要引起重视，因为我看到很多同学都没能正确区分函数栈和栈帧里的操作数栈。操作数栈就是在栈里还嵌套了一个栈，这是基于栈的虚拟机常见的结构。</p><p>接下来，我们来讨论即时编译（JIT）所使用的代码段。在<a href=\"https://time.geekbang.org/column/article/445925\">第11节课</a>我们已经介绍了，JIT使用的代码段是通过mmap得到的、可写可执行的内存区域。严格地说，JIT代码段位于mmap使用的文件映射区，但是由于它被用来当做代码段使用，所以，我们还是把这段内存区域称为代码段。我曾经在课程里讲过，有一些情况下，代码段的地址完全可以比堆的地址更高，这里就是一个实际的例子。</p><p>最后，我们再来讨论堆内存。Hotspot中所说的堆，相比Linux进程中的堆，范围更小。Hotspot首先通过malloc/mmap申请内存区域，然后，Java中的所有对象都会在这块内存区域中创建。这块内存区域就是hotspot虚拟机中的堆，也就是Java程序员日常工作中称呼为“Java堆”的区域。</p><p>经过这些讨论以后，我们发现，Java语言虚拟机在实现的过程中，自由度非常大。它可以任意地将自己的申请来的内存转变为metaspace，用来存储Java类信息和方法的字节码，也可以将它转变为JIT所使用的代码段。</p><p>如果你想深入学习虚拟机的更多知识，可以参考我写的<a href=\"https://book.douban.com/subject/34442805\">《自己动手写Python虚拟机》</a>，这本书可以帮助你了解虚拟机使用内存的更多细节。</p><p><strong>问题二：专栏里说mmap可以修改堆大小，那映射的区域为啥不属于堆呢？</strong></p><p>这个问题非常典型，很多同学问了好几个相似的问题。核心就是纠结到底哪一块内存是堆，哪一块内存是栈。那么，mmap出来的内存到底是文件映射区呢，还是堆呢？</p><p>其实，我们只要把握住一条原则就可以了：<strong>决定一块内存区域的性质的，不是它的地址，而是它的作用。</strong></p><p>我们知道mmap的作用有很多。比如，人们会使用私有文件映射来加载动态库，这是文件映射区的核心作用。但是在glibc中，malloc函数的实现依赖于mmap使用私有匿名映射分配内存，这样分配的内存，会被glibc用于堆内存。所以这块内存从地址上说，是位于文件映射区的，但它起的是堆的作用，所以它被认为是堆内存。</p><p>你可以回忆一下，<a href=\"https://time.geekbang.org/column/article/435493\">第5节课</a>，我们在实现协程的时候，使用malloc申请了一块内存区域，用作协程的栈。然后，在<a href=\"https://time.geekbang.org/column/article/440452\">第9节课</a>中，我们讲malloc的实现在向操作系统申请内存时，可能会有两种操作：<strong>它有可能使用sbrk，这就是从传统的堆区域里分配一块内存；它也有可能使用mmap，这就是从文件映射区分配内存</strong>。所以我们在实现协程的时候，就可能会在堆内存区域（brk指针下面），或者文件映射区域里“偷”了一段内存用于程序栈。</p><p>由此可见，内存区域的划分是非常灵活的。我们前面<a href=\"https://time.geekbang.org/column/article/431904\">第3节课</a>的示意图只是描述了典型的Linux进程的内存布局，但这并不意味着所有内存区域都只能按照这种方式，进行严格地划分，永远一成不变。要牢记，<strong>内存区域的性质不是由它的地址决定的，而是由它的作用来决定的。</strong></p><p><strong>问题三：Java程序中的堆外内存是指什么？</strong></p><p>这个问题的产生，根源还是Java中堆的概念和Linux进程中堆的概念范围不同。我们知道，Java对象都是在Java堆中创建，Java中的堆是由JVM托管，并且其中的对象会自动分配，自动回收。</p><p>堆外内存是指不在Java堆中管理的用户数据。常见的堆外内存是使用Java原生接口（Java Native Interface, JNI）的场景，JNI中可以使用malloc申请内存，然后把确定性的数据都放到这部分内存中。所谓确定性的数据，是指用户明确地知道这些数据的生命周期，可以确定性地手动申请和释放。<strong>我们往往把大规模确定性数据放在堆外内存中，这样做的好处是避免了垃圾回收器经常扫描搬移这部分数据，从而带来性能的下降</strong>。</p><p>然后我们再回到Linux进程的视角看看，不管是Java堆还是堆外内存，其本质还是在进程的堆空间分配的。所以显然，堆外内存的概念只能是指不在Java堆中的那部分内存，而不能是进程堆空间之外的内存。</p><h2>软件篇思考题解析</h2><p><a href=\"https://time.geekbang.org/column/article/431400\">第2节课</a><strong>的思考题：段寄存器还起作用吗？</strong></p><p><strong>解析</strong>：其实在32位Linux操作系统上，数据段和代码段都是从地址0开始且段大小为4G。也就是说，所有的段都被映射到了0～4G这段空间，并且从软件的层面废弃了分段机制。虽然Linux也设置了GDT和LDT为分段机制提供了相应的数据结构，但显然，Linux的内存管理只是为了敷衍CPU，并没有真的使用这种机制。</p><p>有同样待遇的还有TSS这个结构。我们在<a href=\"https://time.geekbang.org/column/article/433530\">第4节课</a>用了很小的篇幅提了一下TSS结构。实际上，在i386的设计中TSS结构扮演了重要的角色。CPU希望每个进程都有一个自己专属的TSS，并通过’'jmp tss\"指令切换进程。但是从内核版本2.2开始，Linux也不再为每个进程创建一个TSS了，而是每个CPU都只有一个，切换进程时，只要将进程的上下文恢复进TSS结构就相当于完成了进程切换。</p><p>从这个角度看，x86 CPU其实有一些设计是属于过度设计。在现代CPU中，这些设计都被简化了。</p><p><a href=\"https://time.geekbang.org/column/article/431904\">第3节课</a><strong>的思考题：堆应该被授予怎样的权限？</strong></p><p><strong>解析</strong>：其实这个问题和上面问题二是一样的。堆内存区域可能会被用作JIT代码段，也可以被用作协程栈，文件映射区也有同样的情况。所以堆被授予怎样的权限，关键还是要看堆内存被用来做什么。如果是被用于JIT代码段，那显然，它的权限就是可读、可写和可执行了。</p><p><a href=\"https://time.geekbang.org/column/article/435493\">第5节课</a><strong>的思考题：请你思考，线程的栈切换，是更类似协程那种提前创建好的方式，还是更类似于进程那种按需写时复制？为什么？</strong></p><p><strong>解析</strong>：分析这个问题的关键在于明确线程与进程之间是怎样的关系。我们在课程中反复强调，一个进程中的所有线程共享进程的资源，包括页表，文件等。所以线程是可以访问进程的内存空间的，这就导致按需写时复制机制不能实现。所以线程一定要采用类似协程那种，提前把栈准备好的方式。</p><p><a href=\"https://time.geekbang.org/column/article/440471\">第8节课</a><strong>的思考题：在生成一个动态库文件的时候，我们一定要加shared选项，但-fPIC选项是必然要加的吗？有没有不需要用这个选项的情况呢？如果没有，为什么？如果有的话，又是什么情况呢？</strong></p><p><strong>解析</strong>：要回答这个问题，我们就要从地址无关代码产生的原因去推导。我们知道对于函数foo，如果它的调用者和它不在同一个动态库中，那它们之间的相对位移在不同的进程中就不是固定的。为了解决这个问题，只能在生成调用者的机器码时，将它生成为地址无关的。</p><p>如果相对位移不存在，那么这个问题也就不存在了，自然就不需要再产生地址无关代码啦。也就是说，<strong>如果一个动态库没有调用其他动态库的函数，这个动态库就不必生成地址无关代码</strong>。只不过，通常情况下，一个功能完善的动态库往往会依赖各种其他的动态库，例如最常见的glibc等。所以，在实际工作中，大多数的动态库在编译时还是要带上-fPIC选项的。</p><p><a href=\"https://time.geekbang.org/column/article/446677\">第12节课</a><strong>的思考题：在HVA到HPA的转换过程中，当前的实现是主动调用get_user_pages来分配物理页。我们又知道VMM运行在内核态，实际上，它是有能力直接为GPA分配物理内存，而不必再借助HVA的，那为什么KVM要选择保留HVA呢？</strong></p><p><strong>解析</strong>：这么做的主要是原因是，如果直接将GPA映射到HPA的话，那么显然，虚拟机能使用的最大物理内存是受主机物理内存限制的。而我们在<a href=\"https://time.geekbang.org/column/article/430073\">第1节课</a>就已经知道，通常，主机的虚拟内存空间远超物理内存。</p><p>如果把GPA映射到HVA，那么我们就可以充分使用主机的虚拟内存机制，为虚拟机提供比较大的客户物理地址空间。而且由于我们可以使用缺页中断来管理主机虚拟内存的映射，这样也就不必再为客户虚拟机提前分配主机物理内存了，也就进一步节约了主机的物理内存。</p><p>今天的答疑就到这里了，不知道你消化得如何？如果还有疑问，请大胆地在留言区提出来，也欢迎你把你的课后思考和心得都记录下来，与我和其他同学一起讨论。我是海纳，我们下节课再见！</p>","float_qrcode":"https://static001.geekbang.org/resource/image/83/1c/83e244537b714e899fd2e7896e22371c.png","article_cover_hidden":false,"is_required":true,"score":"61637856000","like_count":6,"article_subtitle":"","audio_download_url":"https://static001.geekbang.org/resource/audio/b4/fb/b4b47836dd463442ea10d254bc71ccfb.mp3","had_viewed":false,"article_title":"不定期福利第二期 | 软件篇答疑","column_bgcolor":"#F6F7FB","offline_package":"{}","audio_title":"11.25 海纳 faq_software_R.mp3","audio_time_arr":{"m":"11","s":"28","h":"00"},"text_read_percent":0,"cid":450,"article_cshort":"<p>你好，我是海纳。</p><p>随着课程的不断更新，同学们的留言也越来越精彩。我也经常会到这门课的留言区转一转，大部分的疑问，我都已经在留言区做了回复，但我知道你可能在一些地方还存在着疑问。所以，我在这里将同学们问得比较多的问题提炼出来，统一地进行解答，同时，我还整理了软件篇中难度比较大的课后思考题答案，希望能给你带来帮助。</p><p>那我们先从高频问题的解答开始吧。</p><h2>高频留言答疑</h2><p><strong>问题一：JVM的内存布局和Linux进程的内存布局有什么关系？它们是一样的吗？</strong></p><p>这个问题看起来很难，但你可以从JVM的原理入手来找到突破点。典型的JVM，比如Hotspot，它们运行起来以后，也是操作系统上一个普通进程而已。所以，我们可以推论，hotspot进程也有自己的代码段、数据段、堆和栈。我们按照从简单到复杂的顺序来展开Hotspot进程中的内存布局。</p><p>首先，我们先看代码段。Hotspot运行以后，它的代码段里存放的不是Java代码，而是虚拟机自己的代码。这些代码都是由C++编写的，它实现了虚拟机的逻辑。Hotspot会新开辟一段区域，叫做Metaspace，用于加载Java的class文件，并且Java的类信息、方法定义的字节码都存储在这里。所以，从Linux进程的角度看，这一块内存其实是操作系统的堆内存，只不过被用来存储Java字节码了而已。</p>","video_width":0,"column_could_sub":true,"video_id":"","sku":"100094901","video_cover":"","author_name":"海纳","column_is_onboard":true,"audio_url":"https://res001.geekbang.org/media/audio/b4/fb/b4b47836dd463442ea10d254bc71ccfb/ld/ld.m3u8","chapter_id":"2364","column_had_sub":true,"column_cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg","neighbors":{"left":{"article_title":"不定期福利第一期 | 海纳：我是如何学习计算机知识的？","id":447556},"right":{"article_title":"期末测试 | 来赴一场满分之约吧！","id":471878}},"rate_percent":0,"footer_cover_data":{"img_url":"https://static001.geekbang.org/resource/image/f0/fe/f0c47daf3262a8df6cf352d89896abfe.png","link_url":"https://time.geekbang.org/article/427012","mp_url":""},"float_app_qrcode":"","column_is_experience":false,"rate":{"1":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"2":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"3":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0}},"product_id":100094901,"had_liked":false,"id":454080,"free_get":false,"is_video_preview":false,"article_summary":"决定一块内存区域的性质的，不是它的地址，而是它的作用。","column_sale_type":0,"float_qrcode_jump":"https://time.geekbang.org/serv/v4/misc/jump?uri=https%3A%2F%2Ftime.geekbang.org%2Fhybrid%2Fmp%2Fjump%3Furl%3Dhttps%253A%252F%252Fstatic001.geekbang.org%252Fresource%252Fimage%252Fee%252Fb2%252Feedd4090668aabfac91b2b445d27cbb2.png","column_id":450,"article_sharetitle":"不定期福利第二期 | 软件篇答疑","article_poster_wxlite":"https://static001.geekbang.org/render/screen/7f/5a/7f1e9a60e1791264aee2c5753a747d5a.jpeg","article_features":0,"comment_count":4,"audio_md5":"b4b47836dd463442ea10d254bc71ccfb","offline":{"size":11196883,"file_name":"83d95421d91d71f9e25a4ed7fcf2e00b","download_url":"https://static-acl-001.geekbang.org/resource/zip/article/454080/83d95421d91d71f9e25a4ed7fcf2e00b.zip?auth_key=1641482460-7760be98308340dd931bdf9ade34d6a5-0-82aa28929f61c70d16c48ae4b07a3473"},"video_size":0,"hls_videos":[],"video_time":"","article_could_preview":false,"article_ctime":1637856000,"share":{"content":"学好内存，掌握系统开发诀窍","title":"海纳 · 编程高手必学的内存知识","poster":"https://static001.geekbang.org/resource/image/bc/c5/bcaf3ff69e538f8749970edf2301bbc5.jpg","cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg"}},"460545":{"text_read_version":0,"audio_size":20519078,"article_cover":"https://static001.geekbang.org/resource/image/20/60/204367629cae903203e36dbce5b42560.jpg","subtitles":[],"product_type":"c1","audio_dubber":"海纳","is_finished":false,"like":{"had_done":false,"count":7},"audio_time":"00:21:22","video_height":0,"article_content":"<p>你好，我是海纳。</p><p>经过上一节课的学习，我们了解到不同的物理器件，它们的访问速度是不一样的：速度快的往往代价高、容量小；代价低且容量大的，速度通常比较慢。为了充分发挥各种器件的优点，计算机存储数据的物理器件不会只选择一种，而是以CPU为核心，由内而外地组建了一整套的存储体系结构。它将各种不同的器件组合成一个体系，让各种器件扬长避短，从而形成一种快速、大容量、低成本的内存系统。</p><p>而我们要想写出高性能的程序，就必须理解这个存储体系结构，并运用好它。因此，今天这节课我们就来看看常见的存储结构是如何搭建的，并借此把握好影响程序性能的主要因素，从而对程序性能进行优化。</p><h2>存储体系结构的核心</h2><p>作为程序员，我们肯定是希望有无限资源的快速存储器，来存放程序的数据。而现实是，快速存储器的制造成本很高，速度慢的存储器相对便宜。所以从成本角度来说，计算机的存储结构被设计成分层的，一般包括寄存器、缓存、内存、磁盘等。</p><p>其中，缓存又是整个存储体系结构的灵魂，它让内存访问的速度接近于寄存器的访问速度。所以，要想深入理解存储体系结构，我们就要围绕“缓存”这个核心来学习。</p><p>在过去的几十年，处理器速度的增长远远超过了内存速度的增长。尤其是在2001～2005年间，处理器的时钟频率在以55%的速度增长，而同期内存速度的增长仅为7%。为了缩小处理器和内存之间的速度差距，缓存被设计出来。</p><!-- [[[read_end]]] --><p>我们说，距离处理器越近，访问速度就越快，造价也就越高，同时容量也会更小。缓存是处理器和内存之间的一个桥梁，通常分为多层，包括L1层、L2层、L3层等等。缓存的速度介于处理器和内存之间，访问处理器内部寄存器的速度在1ns以内（一个时钟周期），访问内存的速度通常在50～100ns（上百个时钟周期）之间。那么对于缓存来讲，靠近处理器最近的L1层缓存的访问速度在1ns～2ns（3个时钟周期）左右，外层L2和L3层的访问速度在10ns～20ns（几十个时钟周期）之间。</p><p><img src=\"https://static001.geekbang.org/resource/image/21/3e/2188f0ef807cc3802f176a480cyyab3e.jpg?wh=2284x1062\" alt=\"\"></p><p>根据程序的空间局部性和时间局部性原理，一个处理得当的程序，缓存命中率要想达到70～90%并非难事。因此，<strong>在存储系统中加入缓存，可以让整个存储系统的性能接近寄存器，并且每字节的成本都接近内存，甚至是磁盘</strong>。</p><p>可见缓存结合了寄存器速度快和内存造价低的优点，是整个存储体系的灵魂之所在。明白了这一点后，接下来我们拆解一下缓存的物理架构。</p><h2>缓存的物理架构</h2><p>缓存是由SRAM（静态随机存储）组成的，它的本质是一种时序逻辑电路，具体的每个单元（比特）由一个个锁存器构成，锁存器的功能就是让电路具有记忆功能，这一点我们在之前讲过。</p><p>SRAM的单位造价还是比较高的，而且要远高于内存的组成结构“DRAM（动态随机存储）”的造价。这是因为要实现一个锁存器需要六个晶体管，而实现一个DRAM仅需要一个晶体管和一个电容，但是DRAM因为结构简单，单位面积可以存放更多数据，所以更适合做内存。为了兼顾这两者的优缺点，于是它们中间需要加入缓存。</p><p>在制造方面，DRAM因为有电容的存在，不再是单纯的逻辑电路，所以不能用CMOS工艺制造，而SRAM可以。这也是为什么缓存可以集成到芯片内部，而内存是和芯片分开制造的。</p><p>在了解了缓存的内部构成之后，我们再来看看缓存是怎样集成到芯片上的。</p><p>缓存集成到芯片的方式有多种。在过去的单核时代，处理器和各级缓存都只有一个，因此缓存的集成方式相对单一，就是把处理器和缓存直接相连。2004年，Intel取消了4GHz奔腾处理器的研发计划，这意味着处理器以提升主频榨取性能的时代结束，多核处理器开始成为主流。</p><p>在多核芯片上，缓存集成的方式主要有以下三种：</p><ul>\n<li><strong>集中式缓存</strong>：一个缓存和所有处理器直接相连，多个核共享这一个缓存；</li>\n<li><strong>分布式缓存</strong>：一个处理器仅和一个缓存相连，一个处理器对应一个缓存；</li>\n<li><strong>混合式缓存</strong>：在L3采用集中式缓存，在L1和L2采用分布式缓存。</li>\n</ul><p><img src=\"https://static001.geekbang.org/resource/image/d7/96/d7ae48f9f7d705b22cb95c53e423d096.jpg?wh=2284x1392\" alt=\"\"></p><p>现代的多核处理器大都采用混合式的方式将缓存集成到芯片上，一般情况下，L3是所有处理器核共享的，L1和L2是每个处理器核特有的。</p><p>了解了缓存的物理架构后，我们来看一下缓存的工作原理</p><h2>缓存的工作原理</h2><p>首先，我们来理解一个概念，cache line。cache line是缓存进行管理的一个最小存储单元，也叫缓存块。从内存向缓存加载数据也是按缓存块进行加载的，一个缓存块和一个内存中相同容量的数据块（下称内存块）对应。这里，我们先从如何管理缓存块的角度，来看下缓存块的组织形式：</p><p><img src=\"https://static001.geekbang.org/resource/image/28/65/28b90193d04c1247f8e3fbb076b15965.jpg?wh=2284x1312\" alt=\"\"></p><p>上图中的小方框就代表一个缓存块。从图中，你也可以看到，整个缓存由组（set）构成，每个组由路（way）构成。所以整个缓存容量等于组数、路数和缓存块大小的乘积：</p><p>$整个缓存容量=组数\\times路数\\times缓存块大小$</p><p>为了简化寻址方式，内存地址确定的数据块总是会被放在一个固定的组，但可以放在组内的任意路上，也就是说，对于一个特定地址数据的访问，它如果要载入缓存，那么它放在上图中的行数是固定的，但是具体放到哪一列是不固定的。根据缓存中组数和路数的不同，我们将缓存的映射方式分为三类：</p><ul>\n<li><strong>直接相连映射</strong>：缓存只有一个路，一个内存块只能放置在特定的组上；</li>\n<li><strong>全相连映射</strong>：缓存只有一个组，所有的内存块都放在这一个组的不同路上；</li>\n<li><strong>组组相连映射</strong>：缓存同时由多个组和多个路。</li>\n</ul><p>对于直接相连映射，当多个内存块映射到同一组时，会产生冲突，因为只有一列，这个时候就需要将旧的缓存块换出，同时将新的缓存块放入，所以<strong>直接相连映射会导致缓存块被频繁替换</strong>。</p><p>而<strong>全相连映射可以在很大程度上避免冲突，不过，当要查询某个缓存块时，需要逐个遍历每个路，而且电路实现也比较困难</strong>。一个折中的办法就是，采用组组相连映射。这种方式与直接相连映射相比，产生冲突的可能性更小，与全相连映射相比，查询效率更高，实现也更简单。</p><p>上面的举例比较简单，我们再来看这样一种情况：缓存的组数一直是$2^{n}$。虽然组数为$2^{n}$利于查询和定位，但是如果一个程序刚好以$2^{n}$间隔寻址，就会导致地址更多的被映射到同一个组，而另外一些组就会被映射得很少。因此，也有些缓存的组数会设计成一个质数，这样即便程序以$2^{n}$间隔寻址，落到相同组的可能性会大大减小，这样一来，缓存各个组的利用率就会相对均衡。</p><p>那一个内存块具体是怎样映射到一个缓存块的呢？我们先来看看缓存块的内部结构：</p><p><img src=\"https://static001.geekbang.org/resource/image/df/ce/dfcfd3dc2b0a6cc3305ed6188c24cece.jpg?wh=2284x798\" alt=\"\"></p><p>其中，V（valid）表示这个缓存块是否有效，或者说是否正在被使用；M（modified）表示这个缓存块是否被写，也就是“脏”位；B表示缓存块的bit个数。</p><p>假设要寻址一个32位的地址，缓存块的大小是64字节，缓存组织方式是4路组相连，缓存大小是8K。经过计算我们得到缓存一共有32个组（$8\\times1024\\div64\\div4=32$）。那么对于任意一个32位的地址Addr ，它映射到缓存的组号（set index）为 Addr对组数32取模，组号同时也等于Addr的第6~10位（ (Addr &gt;&gt; 6) &amp; 0x1F ），Addr的低6位很好理解，它是缓存块的内部偏移（$2^{6}$为64字节），那么高21位是用来干嘛的呢？我们接着往下看。</p><p>确定需要被映射到哪个组之后，我们需要在该组的路中进行查询。查询方式也很简单，直接将每个缓存块tag的bit位和地址Addr的高21位逐一进行匹配。如果相等，就说明该内存块已经载入到缓存中；如果没有匹配的tag，就说明缓存缺失，需要将内存块放到该组的一个空闲缓存块上；如果所有路的缓存块都正在被使用，那么需要选择一个缓存块，将其移出缓存，把新的内存块载入。</p><p>上面这个过程涉及到缓存块状态转换，而状态转换又涉及到有效位V、脏位M、标签tag。总体来讲，缓存的状态转换有以下几种情况：</p><p><img src=\"https://static001.geekbang.org/resource/image/6d/c2/6d2bfe107cb8a633ea8d9eb363021dc2.jpg?wh=2284x1296\" alt=\"\"></p><p>这里我们提到了缓存块替换，当同组的缓存块都被用完时，需要选择一个缓存块被换出，那么应该选谁被换出呢？这就和缓存块替换策略有关了。</p><h2>缓存块替换策略</h2><p>缓存块替换策略需要达到的一个目标是：<strong>被替换出的数据块应该是将来最晚会被访问的块</strong>。然而，对将来即将发生的事情是没有办法预测的，因为处理器并不知道程序将来会访问哪个地址。因此，<strong>现在的缓存替换策略都采用了最近最少使用算法（Least Recently Used ，LRU）或者是类似LRU的算法</strong>。</p><p>LRU的原理很简单，比如程序要顺序访问 B1 、B2、B3、B4、B5这几个地址块，并且这几个缓存块都映射到缓存的同一个组，同时我们假设缓存采用4路组组相连映射，那么当访问B5时，B1就需要被替换出来。要实现这一点，有很多种方式，其中最简单也最容易实现的是利用位矩阵来实现。</p><p>首先，我们定义一个行、列都与缓存路数相同的矩阵。当访问某个路对应的缓存块时，先将该路对应的所有行置为1，然后再将该路对应的所有列置为0。最终结果体现为，缓存块访问时间的先后顺序，由矩阵行中1的个数决定，最近最常访问缓存块对应行1的个数最多。</p><p>假设现在一个四路相连的缓存组包含数据块 B1、B2、B3、B4, 数据块的访问顺序为 B2、B3、B1、B4，那么LRU矩阵在每次访问后的变化如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/d5/8e/d5f649a8cfff5fea806122bc18b0fb8e.jpg?wh=2284x725\" alt=\"\"></p><p>你会发现，最终B2对应行的1的个数最少，所以B2将会被优先替换。</p><p>在理解了缓存结构和它的工作原理以后，我们就可以来讨论这节课的核心内容了：如何正确地使用缓存，才可以写出高性能的程序？</p><h2>缓存对程序性能的影响</h2><p>通过前面的分析，我们已经知道，CPU将未来最有可能被用到的内存数据加载进缓存。<strong>如果下次访问内存时，数据已经在缓存中了，这就是缓存命中，它获取目标数据的速度非常快。如果数据没在缓存中，这就是缓存缺失，此时要启动内存数据传输，而内存的访问速度相比缓存差很多</strong>。所以我们要避免这种情况。下面，我们先来了解一下哪些情况容易造成缓存缺失，以及具体会对程序性能带来怎样的影响。</p><h3>缓存缺失</h3><p>缓存性能主要取决于缓存命中率，也就说缓存缺失（cache miss）越少，缓存的性能就越好。一般来说，引起缓存缺失的类型主要有三种：</p><ul>\n<li><strong>强制缺失</strong>：第一次将数据块读入到缓存所产生的缺失，也被称为冷缺失（cold miss），因为当发生缓存缺失时，缓存是空的（冷的）；</li>\n<li><strong>冲突缺失</strong>：由于缓存的相连度有限导致的缺失；</li>\n<li><strong>容量缺失</strong>：由于缓存大小有限导致的缺失。</li>\n</ul><p>第一类强制缺失最容易理解，因为第一次将数据读入缓存时，缓存中不会有数据，这种缺失无法避免。</p><p>第二类冲突缺失是因为相连度有限导致的，这里我用一个例子给你说明一下。在这个例子中，第一步我们可以通过getconf命令查看缓存的信息：</p><pre><code># getconf -a |grep CACHE\nLEVEL1_ICACHE_SIZE                 32768\nLEVEL1_ICACHE_ASSOC                8\nLEVEL1_ICACHE_LINESIZE             64\nLEVEL1_DCACHE_SIZE                 32768\nLEVEL1_DCACHE_ASSOC                8\nLEVEL1_DCACHE_LINESIZE             64\nLEVEL2_CACHE_SIZE                  262144\nLEVEL2_CACHE_ASSOC                 4\nLEVEL2_CACHE_LINESIZE              64\nLEVEL3_CACHE_SIZE                  3145728\nLEVEL3_CACHE_ASSOC                 12\nLEVEL3_CACHE_LINESIZE              64\nLEVEL4_CACHE_SIZE                  0\nLEVEL4_CACHE_ASSOC                 0\nLEVEL4_CACHE_LINESIZE              0\n</code></pre><p>在这个缓存的信息中，L1Cache（LEVEL1_ICACHE和LEVEL1_DCACHE分别表示指令缓存和数据缓存，这里我们只关注数据缓存）的cache line 大小为64字节，路数为8路，大小为32K，可以计算出缓存的组数为64组（$32K\\div8\\div64=64$）。</p><p>第二步，我们使用一个程序来测试缓存的影响：</p><pre><code>// cache.c\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n \n#define M  64\n#define N  10000000\nint main( )\n{\n   printf(&quot;%ld&quot;,sizeof(long long));\n   long long (*a)[N] = (long long(*)[N])calloc(M * N, sizeof(long long));\n \n   for(int i = 0; i &lt; 100000000; i++) {\n       for(int j = 0; j &lt; 4096; j+=512) {\n           a[5][j]++;\n       }\n   }\n   return 0;\n}\n</code></pre><p>上面代码中定义了一个二维数组，数组中元素的类型为long long ，元素大小为8字节。所以一个cache line 可以存放 $64\\div8$=$8$个元素。一组是8路，所以一组可以存放$8\\times8$=$64$个元素。一路包含64个cache line，因为前面计算出缓存的组数为64，所以一路可以存放$8\\times64$=$512$个元素。</p><p>代码中的第一层循环是执行次数，第二层循环是以512 为间隔访问元素，即每次访问都会落在同一个组内的不同cache line ，因为一组有8路，所以我们迭代到 $512\\times8$=$4096$的位置。这样可以使同一组刚好可以容纳二层循环需要的地址空间。运行结果如下：</p><pre><code># gcc cache.c\n# time ./a.out\n8\nreal 0m2.670s\nuser 0m2.671s\nsys 0m0.001s\n</code></pre><p>第三步，当我们将第二层循环的迭代次数扩大一倍，也就是8192时，运行结果如下：</p><pre><code># gcc cache.c\n# time ./a.out\n8\nreal 0m16.693s\nuser 0m16.700s\nsys 0m0.001s\n</code></pre><p><strong>虽然运算量增加了一倍，但运行时间却增加了6倍，相当于性能劣化三倍</strong>。劣化的根本原因就是当i &gt; 4096时，也就是访问4096之后的元素，同一组的cache line 已经全部使用，必须进行替换，并且之后的每次访问都会发生冲突，导致缓存块频繁替换，性能劣化严重。</p><p>第三类缓存容量缺失，可以认为是除了强制缺失和冲突缺失之外的缺失，也很好理解，当程序运行的某段时间内，访问地址范围超过缓存大小很多，这样缓存的容量就会成为缓存性能的瓶颈，这里要注意和冲突缺失加以区别，冲突缺失指的是在同一组内的缺失，而容量缺失描述范围是整个缓存。</p><h3>程序局部性</h3><p>在<a href=\"https://time.geekbang.org/column/article/430073\">第1节课</a>里，我们已经讲过，程序局部性分为时间局部性和空间局部性。如果程序有非常好的局部性，那么在程序运行期间，缓存缺失就很少发生。</p><p>我们对上面的例子进行修改，以此来验证程序局部性对缓存命中率的影响，进一步可以观察它对性能产生怎样的影响。</p><pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n \n#define M  10000\n#define N  10000\nint main( )\n{\n   printf(&quot;%ld&quot;,sizeof(long long));\n   long long (*a)[N] = (long long(*)[N])calloc(M * N, sizeof(long long));\n   \n   for(int i = 0; i &lt; M; i++) {\n       for(int j = 0; j &lt; N; j++) {\n           a[i][j]++;\n       }\n   }\n   return 0;\n}\n</code></pre><p>这里主要进行了两处修改：<strong>一是修改了迭代次数，方便测试；二是将之前间隔访问数组中的部分元素修改为顺序访问整个数组，访问方式按二维数组的行逐次访问</strong>。测试结果如下：</p><pre><code># gcc -O0 cache.c\n# time ./a.out\n8\nreal 0m1.245s\nuser 0m0.797s\nsys 0m0.449s\n</code></pre><p>但当我们按列访问时，也就是将内层循环条件提到外面：</p><pre><code>for(int j = 0; j &lt; N; j++) {\n    for(int i = 0; i &lt; M; i++) {\n        a[i][j]++;\n    }\n}\n</code></pre><p>运行结果为：</p><pre><code># gcc -O0 cache.c\n# time ./a.out\n8\nreal 0m2.527s\nuser 0m1.980s\nsys 0m0.548s\n</code></pre><p>可以看到，性能也出现了2倍的劣化，这次劣化的主要原因是当按行访问时地址是连续的，下次访问的元素和当前大概率在同一个cache line（一个元素8字节，而一个cache line 可以容纳8个元素），但是当按列访问时，由于地址跨度大，下次访问的元素基本不可能还在同一个cache line，因此就会增加cache line被替换的次数，所以性能劣化。</p><p>你需要注意的是，<strong>这次编译选项都添加了-O0选项，告诉编译器不要进行优化，因为现在的编译器很聪明，能够识别出这种循环外提的优化，所以我们要先关掉优化</strong>。</p><p>在理解了缓存缺失对程序性能的影响后，我们来看一类非常典型的因为缓存使用不当而引起的性能下降的问题，这类问题统称为伪共享。</p><h2>伪共享</h2><p>伪共享（false-sharing）的意思是说，<strong>当两个线程同时各自修改两个相邻的变量，由于缓存是按缓存块来组织的，当一个线程对一个缓存块执行写操作时，必须使其他线程含有对应数据的缓存块无效。这样两个线程都会同时使对方的缓存块无效，导致性能下降</strong>。</p><p>我们具体来看这样一个例子：</p><pre><code>#include &lt;stdio.h&gt;\n#include &lt;pthread.h&gt;\n \nstruct S{\n   long long a;\n   long long b;\n} s;\n \nvoid *thread1(void *args)\n{\n    for(int i = 0;i &lt; 100000000; i++){\n        s.a++;\n    }\n    return NULL;\n}\n \nvoid *thread2(void *args)\n{\n    for(int i = 0;i &lt; 100000000; i++){\n        s.b++;\n    }\n    return NULL;\n}\n \nint main(int argc, char *argv[]) {\n    pthread_t t1, t2;\n    s.a = 0;\n    s.b = 0;\n    pthread_create(&amp;t1, NULL, thread1, NULL);\n    pthread_create(&amp;t2, NULL, thread2, NULL);\n    pthread_join(t1, NULL);\n    pthread_join(t2, NULL);\n    printf(&quot;a = %lld, b = %lld\\n&quot;, s.a, s.b);\n    return 0;\n}\n</code></pre><p>在这个例子中，main函数中创建了两个线程，分别修改结构体S中的a 、b 变量。a 、b均为long long 类型，都占8字节，所以a 、b 在同一个cache line中，因此会发生为伪共享的情况。程序的运行结果为：</p><pre><code># gcc -Wall false_sharing.c -lpthread\n# time ./a.out\na = 100000000, b = 100000000\n \nreal 0m0.790s\nuser 0m1.481s\nsys 0m0.008s\n</code></pre><p>解决伪共享的办法是，将a 、b不要放在同一个cache line，这样两个线程分别操作不同的cache line 不会相互影响。具体来讲，我们需要对结构体S做出如下修改：</p><pre><code>struct S{\n   long long a;\n   long long nop_0;\n   long long nop_1;\n   long long nop_2;\n   long long nop_3;\n   long long nop_4;\n   long long nop_5;\n   long long nop_6;\n   long long nop_7;\n   long long b;\n} s;\n</code></pre><p>因为在a、b中间插入了8个long long类型的变量，中间隔了64字节，所以a、b会被映射到不同的缓存块，程序执行结果如下：</p><pre><code># gcc -Wall false_sharing.c -lpthread\n# time ./a.out\na = 100000000, b = 100000000\n \nreal 0m0.347s\nuser 0m0.693s\nsys 0m0.001s\n</code></pre><p>在这个结果中，你可以看到，性能有一倍的提升。</p><p>其实，伪共享是一种典型的缓存缺失问题，在并发场景中很常见。<strong>在Java的并发库里经常会看到为了解决伪共享而进行的数据填充。这是大家在写并发程序时也要加以注意的</strong>。</p><h2>总结</h2><p>今天这节课，我们先介绍了存储体系结构的架构和工作原理。其中，缓存又是整个存储体系结构的灵魂，它让内存访问的速度接近于寄存器的访问速度。缓存对程序员是透明的，程序员不必使用特定的API接口来操作缓存工作，它是自动工作的。但如果我们的代码写得不好的话，我们就会感受到缓存不能起作用时的性能下降了。</p><p>缓存的映射方式包括了直接相连、全相连、组组相连三种。直接相连映射会导致缓存块被频繁替换；而全相连映射可以很大程度上避免冲突，但查询效率低；组组相连映射，与直接相连映射相比，产生冲突的可能性更小，与全相连映射相比，查询效率更高，实现也更简单。</p><p>如果要访问的数据不在缓存中，这就是缓存缺失。当发生缓存缺失时，就需要往缓存中加载目标地址的数据。如果缓存空间不足了，就需要对缓存块进行替换，替换的策略多采用LRU策略。</p><p>缓存缺失对性能影响非常大。缓存缺失主要包括强制缺失，冲突缺失和容量缺失。为了避免缓存缺失我们一定要注意程序的局部性，虽然编译器会帮我们做很多事情，但编译器还是有很多情况是无法优化的。</p><p>伪共享是一类非常典型的缓存缺失问题。它是由于多个线程都反复使对方的缓存块无效，带来的性能下降。为了解决这一类问题，我们可以考虑让多个线程所共同访问的对象，在物理上隔离开，保证它们不会落在同一个缓存块里。</p><p>好了，这节课到这里就结束了。下节课，我将带你探讨缓存一致性问题是如何解决的。</p><h2>思考题</h2><p>cache被翻译成缓存，buffer被翻译成缓冲区。那么，请你思考一下，cache和buffer这两个词的区别是什么？它们分别用在什么场景下？除了我们这节课所讲的物理缓存外，你还知道哪些缓存结构？</p><p><img src=\"https://static001.geekbang.org/resource/image/37/02/37f9f73e902efa495855c083f5997f02.jpg?wh=2284x1608\" alt=\"\"></p><p>好啦，这节课到这就结束啦。欢迎你把这节课分享给更多对计算机内存感兴趣的朋友。我是海纳，我们下节课再见！</p>","float_qrcode":"https://static001.geekbang.org/resource/image/83/1c/83e244537b714e899fd2e7896e22371c.png","article_cover_hidden":false,"is_required":true,"score":"41638115200","like_count":3,"article_subtitle":"","audio_download_url":"https://static001.geekbang.org/resource/audio/24/e2/2491c7c09aaf2b63dba8d34222b623e2.mp3","had_viewed":false,"article_title":"14 | CPU Cache：访存速度是如何大幅提升的？","column_bgcolor":"#F6F7FB","offline_package":"{}","audio_title":"11.28 海纳 14_cache_L.mp3","audio_time_arr":{"m":"21","s":"22","h":"00"},"text_read_percent":0,"cid":450,"article_cshort":"<p>你好，我是海纳。</p><p>经过上一节课的学习，我们了解到不同的物理器件，它们的访问速度是不一样的：速度快的往往代价高、容量小；代价低且容量大的，速度通常比较慢。为了充分发挥各种器件的优点，计算机存储数据的物理器件不会只选择一种，而是以CPU为核心，由内而外地组建了一整套的存储体系结构。它将各种不同的器件组合成一个体系，让各种器件扬长避短，从而形成一种快速、大容量、低成本的内存系统。</p><p>而我们要想写出高性能的程序，就必须理解这个存储体系结构，并运用好它。因此，今天这节课我们就来看看常见的存储结构是如何搭建的，并借此把握好影响程序性能的主要因素，从而对程序性能进行优化。</p><h2>存储体系结构的核心</h2><p>作为程序员，我们肯定是希望有无限资源的快速存储器，来存放程序的数据。而现实是，快速存储器的制造成本很高，速度慢的存储器相对便宜。所以从成本角度来说，计算机的存储结构被设计成分层的，一般包括寄存器、缓存、内存、磁盘等。</p><p>其中，缓存又是整个存储体系结构的灵魂，它让内存访问的速度接近于寄存器的访问速度。所以，要想深入理解存储体系结构，我们就要围绕“缓存”这个核心来学习。</p><p>在过去的几十年，处理器速度的增长远远超过了内存速度的增长。尤其是在2001～2005年间，处理器的时钟频率在以55%的速度增长，而同期内存速度的增长仅为7%。为了缩小处理器和内存之间的速度差距，缓存被设计出来。</p>","video_width":0,"column_could_sub":true,"video_id":"","sku":"100094901","video_cover":"","author_name":"海纳","column_is_onboard":true,"audio_url":"https://res001.geekbang.org/media/audio/24/e2/2491c7c09aaf2b63dba8d34222b623e2/ld/ld.m3u8","chapter_id":"2389","column_had_sub":true,"column_cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg","neighbors":{"left":{"article_title":"13 | 存储电路：计算机存储芯片的电路结构是怎样的？","id":450519},"right":{"article_title":"15 | MESI协议：多核CPU是如何同步高速缓存的？","id":461801}},"rate_percent":0,"footer_cover_data":{"img_url":"https://static001.geekbang.org/resource/image/f0/fe/f0c47daf3262a8df6cf352d89896abfe.png","link_url":"https://time.geekbang.org/article/427012","mp_url":""},"float_app_qrcode":"","column_is_experience":false,"rate":{"1":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"2":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"3":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0}},"product_id":100094901,"had_liked":false,"id":460545,"free_get":false,"is_video_preview":false,"article_summary":"被替换出的数据块应该是将来最晚会被访问的块。","column_sale_type":0,"float_qrcode_jump":"https://time.geekbang.org/serv/v4/misc/jump?uri=https%3A%2F%2Ftime.geekbang.org%2Fhybrid%2Fmp%2Fjump%3Furl%3Dhttps%253A%252F%252Fstatic001.geekbang.org%252Fresource%252Fimage%252Fee%252Fb2%252Feedd4090668aabfac91b2b445d27cbb2.png","column_id":450,"article_sharetitle":"14 | CPU Cache：访存速度是如何大幅提升的？","article_poster_wxlite":"https://static001.geekbang.org/render/screen/de/2f/def6725a8df71b9529f0f4f744f80d2f.jpeg","article_features":0,"comment_count":11,"audio_md5":"2491c7c09aaf2b63dba8d34222b623e2","offline":{"size":21035876,"file_name":"6b160f7dec78867330d282a6502b0f01","download_url":"https://static-acl-001.geekbang.org/resource/zip/article/460545/6b160f7dec78867330d282a6502b0f01.zip?auth_key=1641482269-53acf4bafcee4a519e30e86f234895a2-0-06eebf65d444541fac4475d221171095"},"video_size":0,"hls_videos":[],"video_time":"","article_could_preview":false,"article_ctime":1638115200,"share":{"content":"学好内存，掌握系统开发诀窍","title":"海纳 · 编程高手必学的内存知识","poster":"https://static001.geekbang.org/resource/image/bc/c5/bcaf3ff69e538f8749970edf2301bbc5.jpg","cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg"}},"461801":{"text_read_version":0,"audio_size":20106092,"article_cover":"https://static001.geekbang.org/resource/image/ba/b4/ba6faf610cf1819db26d2287ee12b0b4.jpg","subtitles":[],"product_type":"c1","audio_dubber":"海纳","is_finished":false,"like":{"had_done":false,"count":3},"audio_time":"00:20:56","video_height":0,"article_content":"<p>你好，我是海纳。</p><p>上节课，我们学习了为什么要设计缓存，以及缓存和内存的映射方式。你还记得吗？在上节课结束的部分，我讲到了只要数据的访问者和被访问者之间的速度不匹配，就可以考虑使用缓存进行加速。</p><p>但是我们知道，天下没有免费的午餐，缓存在带来性能提升的同时，也引入了缓存一致性问题。缓存一致性问题的产生主要是因为在多核体系结构中，如果有一个CPU修改了内存中的某个值，那么必须有一种机制保证其他CPU能够观察到这个修改。于是，人们设计了协议来规定一个CPU对缓存数据的修改，如何同步到另一个CPU。</p><p>今天我们就来介绍在多核体系结构下，如何解决缓存一致性问题。另外，按照从简单到困难的顺序，我还会介绍最简单的VI协议和比较完善的MESI协议。学习完这节课后，你就知道缓存一致性问题是如何被解决的，还会了解到如何设计协议对缓存一致性进行管理。</p><p>在缓存一致性的问题中，因为CPU修改自己的缓存策略至关重要，所以我们就从缓存的写策略开始讲起。</p><h2>缓存写策略</h2><p>在高速缓存的设计中，有一个重要的问题就是：当CPU修改了缓存中的数据后，这些修改什么时候能传播到主存？解决这个问题有两种策略：<strong>写回（Write Back）和写直达（Write Through）</strong>。</p><!-- [[[read_end]]] --><p>当CPU采取写回策略时，对缓存的修改不会立刻传播到主存，只有当缓存块被替换时，这些被修改的缓存块，才会写回并覆盖内存中过时的数据；当CPU采取写直达策略时，缓存中任何一个字节的修改，都会立刻传播到内存，这种做法就像穿透了缓存一样，所以用英文单词“Through”来命名。</p><p>同时，当某个CPU的缓存中执行写操作，修改其中的某个值时，其他CPU的缓存所保有该数据副本的更新策略也有两种：<strong>写更新（Write Update）和写无效（Write Invalidate）</strong>。</p><p>如果CPU采取写更新策略，每次它的缓存写入新的值，该CPU都必须发起一次总线请求，通知其他CPU将它们的缓存值更新为刚写入的值，所以写更新会很占用总线带宽。如果一个CPU缓存执行了写操作，其他CPU需要多次读这个被写过的数据时，那么写更新的效率就会变得很高，因为写操作执行之后马上更新其他缓存中的副本，所以可以使其他处理器立刻获得最新的值。</p><p>如果在一个CPU修改缓存时，将其他CPU中的缓存全部设置为无效，这种策略叫做写无效。这意味着，当其他CPU再次访问该缓存副本时，会发现这一部分缓存已经失效，此时CPU就会从内存中重新载入最新的数据。</p><p>在具体的实现中，绝大多数CPU都会采用写无效策略。这是因为多次写操作只需要发起一次总线事件即可，第一次写已经将其他缓存的值置为无效，之后的写不必再更新状态，这样可以有效地节省CPU核间总线带宽。基于这个原因，我们这节课也只讨论写无效策略。</p><p>另一个方面是，当前要写入的数据不在缓存中时，根据是否要先将数据加载到缓存中，写策略又分为两种：<strong>写分配（Write Allocate）和写不分配（Not Write Allocate）</strong>。</p><p>在写入数据前将数据读入缓存，这是写分配策略。当缓存块中的数据在未来读写概率较高，也就是程序空间局部性较好时，写分配的效率较好；在写入数据时，直接将要写入的数据传播内存，而并不将数据块读入缓存，这是写不分配策略。当数据块中的数据在未来使用的概率较低时，写不分配性能较好。</p><p>如果缓存块的大小比较大，该缓存块未来被多次访问的概率也会增加，这种情况下，写分配的策略性能要优于写不分配。这节课，我们将“写直达”与“写不分配”组合起来讲解，把“写回”和“写分配”组合起来讲解，其他的组合情况，做为练习，大家可以根据这两种情况自行推导。</p><p><strong>从缓存和内存的更新关系看，写策略分为写回和写直达；从写缓存时CPU之间的更新策略来看，写策略分为写更新和写无效；从写缓存时数据是否被加载来看，写策略又分为写分配和写不分配。</strong></p><p>在介绍完缓存写策略这些概念之后，我们来具体看下什么是缓存一致性问题。</p><h2>缓存一致性问题</h2><p>所谓缓存一致性，就是保证同一个数据在每个CPU的私有缓存（一般为L1 Cache）中副本是相同的。考虑下面的例子：</p><pre><code>global sum = 0\n \n// Thread1：\nsum += 3\n \n// Thread2：\nsum += 5\n</code></pre><p>假设Thread1由CPU核P1执行，Thread2 由P2执行，那么P1、P2的私有缓存和主存的状态可能出现下表所示的情况：</p><p><img src=\"https://static001.geekbang.org/resource/image/31/91/31011687f8f4d9fda7bcc20fbf6f3391.jpg?wh=2284x1709\" alt=\"\"></p><p>我先带你理解下表格中的信息，然后再结合上面的例子具体分析。在这个表里，脏是缓存块的一个标识位，用来表示缓存中的数据有没有被改写，如果该缓存块的内容被修改，并且还没有同步到主存，就称它为脏的；</p><p>sum对于Thread1和Thread2是共享的。初始状态sum的值为0，Thread1将sum加3，Thread2将sum加5。正常来说，我们期望内存中的sum值是8。但实际两个线程执行结束后，内存中的sum的取值根据缓存状态的传播情况，就会有不同的取值。</p><p>上表中展示了一种内存中sum值为5的操作序列。但是，第5步和第6步的顺序有可能会对调，所以sum值还有可能是3。如果第3步，P1的缓存中的值能被正确地传播到P2，那么P2的sum值就为8，所以最终内存中的值还有可能是8。</p><p>通过上面的例子我们可以看出，为了保证缓存一致性，必须解决两个问题，分别是<strong>写传播（第3步）和事务串行化（第5和第6步）</strong>。</p><p>写传播是指，一个处理器对缓存中的值进行了修改，需要通知其他处理器，也就是需要用到“写更新”或者“写无效”策略。</p><p>事务串行化是指，多个处理器对同一个值进行修改，在同一时刻只能有一个处理器写成功，必须保证写操作的原子性，多个写操作必须串行执行。我们将会在下节课对事务串行化进行介绍，这节课只重点关注写传播。</p><p>那怎样解决写传播所带来的缓存一致性问题呢？那就需要缓存一致性协议，前面提到缓存中的值同步给主存有两种策略（写回和写直达），而且，不同的写策略，对应不同的缓存一致性协议。所以，接下来我们分别介绍基于写直达和写回的缓存一致性协议。</p><h2>基于“写直达”的缓存一致性协议</h2><p>写直达的缓存一致性协议是比较简单的，我们假设一个单级缓存，它既可以接收来自处理器的请求，也可以处理来自总线侦听器的总线侦听请求，其中，处理器的请求包含：</p><ul>\n<li><strong>PrRd</strong>: 处理器请求从缓存块中读出；</li>\n<li><strong>PrWr</strong>: 处理器请求向缓存块写入。</li>\n</ul><p>来自总线的请求包含：</p><ul>\n<li><strong>BusRd</strong>: 总线侦听到一个来自另一个处理器的读出缓存请求；</li>\n<li><strong>BusWr</strong>: 总线侦听到来自另一个处理器写入缓存的请求。在“写直达”策略中，BusWr即另一个处理器向内存的写入请求。</li>\n</ul><p>每个缓存块都有两种状态，包括：</p><ol>\n<li><strong>Valid(V)：缓存块是有效且干净的，意味着该缓存块中的内容与主存中相同</strong>；</li>\n<li><strong>Invalid(I)：缓存块无效，访问该缓存块会出现缓存缺失</strong>。</li>\n</ol><p>这里我们用一个状态机来表示基于“写直达”一致性协议的缓存块状态变化，也就是缓存一致性协议的本质。如上面所介绍的，<strong>在这里我们只讨论写“写无效”和“写直达”的组合策略</strong>，因为写直达会导致更新直接穿透缓存，所以这种情况下只能采用写不分配策略，所以我们这里讨论的策略组合是写无效、写直达和写不分配。如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/59/27/59679343c2a90b67457b35b0ef3ab027.jpg?wh=2284x2116\" alt=\"\"></p><p>在上图中，“/”前表示的是请求，这个请求可能来自CPU自己，也可能来自总线，“/”后表示的是当前请求所引起的总线事件，“-”表示不产生总线事件。</p><p>我们先看图的左边，这部分代表了当前CPU所发起的操作，考虑缓存块的状态为I。I状态代表了两种情况：<strong>尚未使用的缓存块和无效的缓存块，尚未使用的缓存块其中也没有有效的数据，所以可以与无效的缓存块同等对待</strong>。</p><p>先讨论状态I，当处理器发出读请求时，发现缓存缺失，但是要把数据加载进缓存，这时，总线上随即产生一个BusRd请求，内存控制器响应BusRd，将所需的块从内存中取出，取出的块放入缓存中，同时将状态设置为V，表示当前缓存的状态有效。当处理器发出写请求时，因为采用写直达策略，写操作通过BusWr被传递到内存，而不是将数据写入缓存，所以状态仍为无效。</p><p>接着考虑状态V。当处理器发出读请求时，该数据在缓存中被找到，缓存命中，不会产生总线事务，缓存块状态不变。当处理器发出写请求时，缓存块被更新，并且这个更新通过BusWr被传递到内存，缓存块的状态保持有效。</p><p>接下来我们看图的右边，这部分代表总线发起的请求。我们还是分别讨论状态I和状态V。先讨论状态I，所有侦听到的BusRd和BusWr都不会影响它，保持无效，所以这种情况被忽略。</p><p>接着，我们考虑状态V，当一个BusRd被侦听到时，这意味着有其他处理器遇到了缓存缺失，并且需要从主存中取出需要的块，所以该缓存块的状态不用改变，但是当侦听到一个BusWr时，表示有其他处理器想要获取该缓存块的唯一所有权（要保证事务串行化），所以该缓存块的状态变为I。</p><p>讨论到这，我们再来看缓存一致性中的数据同步问题，你就能很好的理解了。<strong>“写传播”的缓存一致性的缺点是需要很高的带宽。原因是对于缓存块的每次写入，都会触发BusWr从而占用带宽。相反的是，在“写无效”缓存策略下，如果同一个缓存块中的数据被多次写入，只需占用一次总线带宽来失效其他处理器的缓存副本即可</strong>。</p><p>接下来我们介绍下基于“写回”策略的缓存一致性协议，它也被称为MESI协议。</p><h2>MESI协议</h2><p>同基于“写直达”的缓存一致性协议一样，我们先来了解MESI协议中，处理器对缓存的请求:</p><ul>\n<li><strong>PrRd</strong>：处理器请求从缓存块中读出；</li>\n<li><strong>PrWr</strong>：处理器请求向缓存块写入。</li>\n</ul><p>而总线对缓存的请求和“写直达”的缓存一致性协议稍有不同，分别是:</p><ul>\n<li><strong>BusRd</strong>：总线侦听到一个来自另一个处理器的读出缓存请求；</li>\n<li><strong>BusRdX</strong>：总线侦听到来自另一个尚未取得该缓存块所有权的处理器读独占（或者写）缓存的请求；</li>\n<li><strong>BusUpgr</strong>：侦听到一个其他处理器要写入本地缓存块上的数据的请求；</li>\n<li><strong>Flush</strong>：总线侦听到一个缓存块被另一个处理器写回到主存的请求；</li>\n<li><strong>FlushOpt</strong>：侦听到一个缓存块被放置在总线以提供给另一个处理器的请求，和Flush类似，但只不过是从缓存到缓存的传输请求。</li>\n</ul><p>缓存块的状态分为4种，也是MESI协议名字的由来：</p><ul>\n<li><strong>Modified（M）：缓存块有效，但是是“脏”的，其数据与主存中的原始数据不同，同时还表示处理器对于该缓存块的唯一所有权，表示数据只在这个处理器的缓存上是有效的</strong>；</li>\n<li><strong>Exclusive（E）：缓存块是干净有效且唯一的</strong>；</li>\n<li><strong> Shared（S）：缓存块是有效且干净的，有多个处理器持有相同的缓存副本</strong>；</li>\n<li><strong>Invalid（I）：缓存块无效</strong>。</li>\n</ul><p>同样，我们用状态机来表示缓存块状态的变化，如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/8c/55/8cc76feda0028d378cae41530f530a55.jpg?wh=2284x893\" alt=\"\"></p><p>这个状态机看起来比较复杂，首先，图中的黑色箭头表示是由当前处理器发起的，红色箭头表示，这个事件是从总线来的，也就是由其他处理器发起的。</p><p>我们先看由处理器发起的请求（黑线部分）：</p><ul>\n<li>\n<p><strong>M状态</strong>：读写操作都不会改变状态，并且因为能够确定不会有其他副本，因此不会产生任何总线事务；</p>\n</li>\n<li>\n<p><strong>E状态</strong>：任何对该缓存块的读操作都会缓存命中，且不触发任何总线事务。一个对E状态的写操作，也不会产生总线事务，只需将缓存块状态改为M；</p>\n</li>\n<li>\n<p><strong>S状态</strong>：当处理器读时，缓存命中，不产生总线事务。当处理器写时，需要产生BusUpgr事件，通知其他处理器我要写这个缓存块，并将缓存块状态置为M；</p>\n</li>\n<li>\n<p><strong>I状态</strong>：当处理器发出读请求时，遇到缓存块缺失，要把数据加载进缓存，产生一个BusRd总线请求。内存控制器响应BusRd请求，将所需要的缓存块从内存中取出，同时会检查有没有其他处理器也有该缓存块拷贝，如果发现拷贝则将状态置为S,并且把其他有拷贝的处理器的状态也相应地置为S；如果没有发现其他拷贝，则将状态置为E。</p>\n</li>\n</ul><p>接下来，我们看下由总线发起的请求（红色部分）：</p><ul>\n<li>\n<p><strong>M状态</strong>：该缓存块是整个系统里唯一有效的，并且内存的数据也是过时的。因此当侦听到BusRd时，缓存块必须被清空以保证写传播，所以会产生Flush事件。并且将状态置为S。当侦听到BusRdX时，也必须产生Flush事件，因为有其他处理器要写，所以当前缓存块置为I；</p>\n</li>\n<li>\n<p><strong>E状态</strong>：当侦听到BusRd请求时，说明另一个处理器遇到了缓存缺失，并试图获取该缓存块，因为最终的结果是要将这个缓存块，放在不止一个处理器缓存上，所以状态必须被置为S。这样就会产生FlushOpt事件，来完成缓存到缓存的传输。</p>\n</li>\n</ul><p>当BusRdX被侦听到时，说明有其他处理器想要独占这个缓存块上的数据，这种情况下，本地缓存块将会被清空并且状态需要置为I，同时也会产生FlushOpt事件，完成缓存到缓存的传输，将当前数据的最新值同步给需要进行写操作的其他处理器。</p><p>而当侦听到BusUpgr时，说明其他处理器要写当前处理器持有的缓存副本，所以要将状态置为I，但是不必产生总线事务；</p><ul>\n<li>\n<p><strong>S状态</strong>：当侦听到BusRd时，也就是另一个处理器遇到缓存缺失而试图获取该缓存块，因为S状态本身是共享的，所以状态保持S不变；</p>\n</li>\n<li>\n<p><strong>I状态</strong>：侦听到的BusRd、BusRdX、BusUpgr都不会影响它，所以忽略该情况，状态保持不变。</p>\n</li>\n</ul><p>总体来讲，<strong>MESI协议通过引入了Modified和Exclusive两种状态，并且引入了处理器缓存之间可以相互同步的机制，非常有效地降低了CPU核间带宽。它是当前设计中进行CPU核间通讯的主流协议，被广泛地使用在各种CPU中</strong>。</p><h2>总结</h2><p>好了，这节课到这里就结束了。这节课我们介绍了缓存的写策略、多核情况下缓存面临的缓存一致性问题，以及如何使用缓存一致性协议来解决这类问题。</p><p>因为缓存一致性问题是由CPU对自己的缓存进行写操作，而未能及时通知到其他CPU所引起的，所以缓存的写策略会深刻地影响缓存一致性问题的解决。</p><p>从缓存和内存的更新关系看，写策略分为写回和写直达；从写缓存时，CPU之间的更新策略来看，写策略分为写更新和写无效；从写缓存时数据是否被加载来看，写策略又分为写分配和写不分配。其中，写更新和写不分配这两种策略在现实中比较少出现，所以我们这节课就不再对它们展开详细的讨论了。</p><p>接着，我们讨论了在写回策略和写直达策略中，缓存的状态和它的状态迁移的情况。状态迁移要考虑两种动作：<strong>一是本CPU所发起的请求，以Pr开头；另一个是其他CPU发起的请求，这些请求最终会通过总线发送过来，以Bus开头</strong>。一个CPU发起请求的同时，还会产生总线事件。</p><p>在写回策略中主要包括失效和有效两种状态；在写直达策略中又通过引入独占和修改状态，提升了缓存同步的效率。</p><p>你要注意的是，<strong>缓存一致性协议是个约定，具体实现上实际是由硬件电路保证的，虽然我们在写程序时可能没有涉及这方面的知识，但是作为一个资深程序员，了解其背后的原理是非常有必要的</strong>。</p><h2>思考题</h2><p>你能列举一下在工作中，你还遇到哪些场景需要类似的一致性算法的吗？（小提示：所有类似的有一致性需求的场景，都可以采用类似MESI协议的做法来解决）。欢迎你在留言区分享你的想法和收获，我在留言区等你。</p><p><img src=\"https://static001.geekbang.org/resource/image/1c/01/1c928908539529b39fe7bdfa987d7201.jpg?wh=2284x1635\" alt=\"\"></p><p>好啦，这节课到这就结束啦。欢迎你把这节课分享给更多对计算机内存感兴趣的朋友。我是海纳，我们下节课再见！</p>","float_qrcode":"https://static001.geekbang.org/resource/image/83/1c/83e244537b714e899fd2e7896e22371c.png","article_cover_hidden":false,"is_required":true,"score":"41638288000","like_count":6,"article_subtitle":"","audio_download_url":"https://static001.geekbang.org/resource/audio/65/9e/65ff61f32bd7f70bee37e200de23ec9e.mp3","had_viewed":false,"article_title":"15 | MESI协议：多核CPU是如何同步高速缓存的？","column_bgcolor":"#F6F7FB","offline_package":"{}","audio_title":"11.30 海纳15_mesi_01.MP3","audio_time_arr":{"m":"20","s":"56","h":"00"},"text_read_percent":0,"cid":450,"article_cshort":"<p>你好，我是海纳。</p><p>上节课，我们学习了为什么要设计缓存，以及缓存和内存的映射方式。你还记得吗？在上节课结束的部分，我讲到了只要数据的访问者和被访问者之间的速度不匹配，就可以考虑使用缓存进行加速。</p><p>但是我们知道，天下没有免费的午餐，缓存在带来性能提升的同时，也引入了缓存一致性问题。缓存一致性问题的产生主要是因为在多核体系结构中，如果有一个CPU修改了内存中的某个值，那么必须有一种机制保证其他CPU能够观察到这个修改。于是，人们设计了协议来规定一个CPU对缓存数据的修改，如何同步到另一个CPU。</p><p>今天我们就来介绍在多核体系结构下，如何解决缓存一致性问题。另外，按照从简单到困难的顺序，我还会介绍最简单的VI协议和比较完善的MESI协议。学习完这节课后，你就知道缓存一致性问题是如何被解决的，还会了解到如何设计协议对缓存一致性进行管理。</p><p>在缓存一致性的问题中，因为CPU修改自己的缓存策略至关重要，所以我们就从缓存的写策略开始讲起。</p><h2>缓存写策略</h2><p>在高速缓存的设计中，有一个重要的问题就是：当CPU修改了缓存中的数据后，这些修改什么时候能传播到主存？解决这个问题有两种策略：<strong>写回（Write Back）和写直达（Write Through）</strong>。</p>","video_width":0,"column_could_sub":true,"video_id":"","sku":"100094901","video_cover":"","author_name":"海纳","column_is_onboard":true,"audio_url":"https://res001.geekbang.org/media/audio/65/9e/65ff61f32bd7f70bee37e200de23ec9e/ld/ld.m3u8","chapter_id":"2389","column_had_sub":true,"column_cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg","neighbors":{"left":{"article_title":"14 | CPU Cache：访存速度是如何大幅提升的？","id":460545},"right":{"article_title":"16 | 内存模型：有了MESI为什么还需要内存屏障？","id":462113}},"rate_percent":0,"footer_cover_data":{"img_url":"https://static001.geekbang.org/resource/image/f0/fe/f0c47daf3262a8df6cf352d89896abfe.png","link_url":"https://time.geekbang.org/article/427012","mp_url":""},"float_app_qrcode":"","column_is_experience":false,"rate":{"1":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"2":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"3":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0}},"product_id":100094901,"had_liked":false,"id":461801,"free_get":false,"is_video_preview":false,"article_summary":"今天我们就来介绍在多核体系结构下，如何解决缓存一致性问题。","column_sale_type":0,"float_qrcode_jump":"https://time.geekbang.org/serv/v4/misc/jump?uri=https%3A%2F%2Ftime.geekbang.org%2Fhybrid%2Fmp%2Fjump%3Furl%3Dhttps%253A%252F%252Fstatic001.geekbang.org%252Fresource%252Fimage%252Fee%252Fb2%252Feedd4090668aabfac91b2b445d27cbb2.png","column_id":450,"article_sharetitle":"15 | MESI协议：多核CPU是如何同步高速缓存的？","article_poster_wxlite":"https://static001.geekbang.org/render/screen/9c/b3/9c97f783ec9f1c1f7ef44713b32e61b3.jpeg","article_features":0,"comment_count":9,"audio_md5":"65ff61f32bd7f70bee37e200de23ec9e","offline":{"size":21610415,"file_name":"7d1d52289aa6118ec9d2e078f51cd228","download_url":"https://static-acl-001.geekbang.org/resource/zip/article/461801/7d1d52289aa6118ec9d2e078f51cd228.zip?auth_key=1641482285-1aa3da18f5974a69ad8fe264cf035433-0-bbb7437530496c3aea6ef93d086777e7"},"video_size":0,"hls_videos":[],"video_time":"","article_could_preview":false,"article_ctime":1638288000,"share":{"content":"学好内存，掌握系统开发诀窍","title":"海纳 · 编程高手必学的内存知识","poster":"https://static001.geekbang.org/resource/image/bc/c5/bcaf3ff69e538f8749970edf2301bbc5.jpg","cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg"}},"462113":{"text_read_version":0,"audio_size":18663653,"article_cover":"https://static001.geekbang.org/resource/image/d8/c1/d8ed1d9caababf63e97b640700e1d6c1.jpg","subtitles":[],"product_type":"c1","audio_dubber":"海纳","is_finished":false,"like":{"had_done":false,"count":6},"audio_time":"00:19:25","video_height":0,"article_content":"<p>你好，我是海纳。</p><p>上一节课，我们学习了MESI协议，我们了解到，MESI协议能够解决多核 CPU体系中，多个CPU之间缓存数据不一致的问题。但是，如果CPU严格按照MESI协议进行核间通讯和同步，核间同步就会给CPU带来性能问题。既要遵守协议，又要提升性能，这就对CPU的设计人员提出了巨大的挑战。</p><p>那严格遵守MESI协议的CPU会有什么样的性能问题呢？我们又可以怎么来解决这些问题呢？今天我们就来仔细分析一下。搞清楚了这些问题，你会对C++内存模型和Java内存模型有更加深入的理解，在分析并发问题时能够做到有的放矢。</p><h2>严守MESI协议的CPU会有啥问题？</h2><p>我们上节课说过，MESI代表的是Modified、Exclusive、Shared、Invalid这四种缓存状态，遵守MESI协议的CPU缓存会在这四种状态之间相互切换。这种CPU缓存之间的关系是这样的：</p><p><img src=\"https://static001.geekbang.org/resource/image/1d/57/1dabf3dccd6113d76b29c05dd3ea3c57.jpg?wh=2284x1407\" alt=\"\"></p><p>从上面这张图你可以看到，Cache和主内存(Memory)是直接相连的。一个CPU的所有写操作都会按照真实的执行顺序同步到主存和其他CPU的cache中。</p><p>严格遵守MESI协议的CPU设计，在它的某一个核在写一块缓存时，它需要通知所有的同伴：我要写这块缓存了，如果你们谁有这块缓存的副本，请把它置成Invalid状态。Invalid状态意味着该缓存失效，如果其他CPU再访问这一缓存区时，就会从主存中加载正确的值。</p><!-- [[[read_end]]] --><p><strong>发起写请求的CPU中的缓存状态可能是Exclusive、Modified和Share，每个状态下的处理是不一样的。</strong></p><p>如果缓存状态是Exclusive和Modified，那么CPU一个核修改缓存时不需要通知其他核，这是比较容易的。</p><p>但是在Share状态下，如果一个核想独占缓存进行修改，就需要先给所有Share状态的同伴发出Invalid消息，等所有同伴确认并回复它“Invalid acknowledgement”以后，它才能把这块缓存的状态更改为Modified，这是保持多核信息同步的必然要求。</p><p>这个过程相对于直接在核内缓存里修改缓存内容，非常漫长。这也就会导致，某个核请求独占时间比较长。</p><p>那怎么来解决这个问题呢？</p><h2>写缓冲与写屏障</h2><p>CPU的设计者为每个核都添加了一个名为<strong>store buffer</strong>的结构，store buffer是硬件实现的缓冲区，它的读写速度比缓存的速度更快，所有面向缓存的写操作都会先经过store buffer。</p><p>不过，由于中文材料中经常将cache和buffer都翻译成缓冲，或者缓存，很容易混淆概念，所以在这里，我想强调一下cache和buffer的区别。</p><p>cache这个词，往往意味着它所存储的信息是副本。cache中的数据即使丢失了，也可以从内存中找到原始数据（不考虑脏数据的情况），<strong>cache存在的意义是加速查找</strong>。</p><p><strong>但是buffer更像是蓄水池</strong>，你可以理解成它是一个收作业的课代表，课代表会把所有同学的作业都收集齐以后再一次性地交到老师那里。buffer中的数据没有副本，一旦丢失就彻底丢失了。store buffer也是同样的道理，它会收集多次写操作，然后在合适的时机进行提交。</p><p>增加了store buffer以后的CPU缓存结构是这样的：</p><p><img src=\"https://static001.geekbang.org/resource/image/fd/a0/fdd8b173b8cea302fb1884a897fbaea0.jpg?wh=2284x1716\" alt=\"\"></p><p>在这样的结构里，如果CPU的某个核再要对一个变量进行赋值，它就不必等到所有的同伴都确认完，而是直接把新的值放入store buffer，然后再由store buffer慢慢地去做核间同步，并且将新的值刷入到cache中去就好了。而且，每个核的store buffer都是私有的，其他核不可见。</p><p>为了让你更好理解核间同步的问题，我们现在来举个例子。我们使用两个CPU，分别叫做CPU0和CPU1，其中CPU0负责写数据，而CPU1负责读数据，我们看看在增加了store buffer这个结构以后，它们在进行核间同步时会遇到什么问题。</p><p>假如CPU0刚刚更新了变量a的值，并且将它放到了store buffer中，CPU0自己接着又要读取a的值，此时，它会在自己的store buffer中读到正确的值。</p><p>那如果在这一次修改的a值被写入cache之前，CPU0又一次对a值进行了修改呢？那也没问题，这次更新就可以直接写入store buffer。因为store buffer是CPU0私有的，修改它不涉及任何核间同步和缓存一致性问题，所以效率也得到了比较大的提升。</p><p>但用store buffer也会有一个问题，那就是<strong>它并不能保证变量写入缓存和主存的顺序</strong>，你先来看看下面这个代码：</p><pre><code>// CPU0\nvoid foo() {\n    a = 1;\n    b = 1;\n}\n\n// CPU1\nvoid bar() {\n    while (b == 0) continue;\n    assert(a == 1);\n}\n</code></pre><p>你可以看到，在这个代码块中，CPU0执行foo函数，CPU1执行bar函数。但在对变量a和b进行赋值的时候，有两种情况会导致它们的赋值顺序被打乱。</p><p><strong>第一种情况是CPU的乱序执行</strong>。在Cache的基本原理一课中，我们已经讲过CPU为了提升运行效率和提高缓存命中率，采用了乱序执行。</p><p><strong>第二种情况是store buffer在写入时，有可能b所对应的缓存行会先于a所对应的缓存行进入独占状态，也就是说b会先写入缓存。</strong></p><p>这种情况完全是有可能的。你想，如果a是Share状态，b是Exclusive状态，那么尽管CPU0在执行时没有乱序，这两个变量由store buffer写入缓存时也是不能保证顺序的。</p><p>那这个时候，我们假设CPU1开始执行时，a和b所对应的缓存行都是Invalid状态。当CPU1开始执行第9行的时候，由于b所对应的缓存区域是Invalid状态，它就会向总线发出BusRd请求，那么CPU1就会先把b的最新值读到本地，完成变量b的值的更新，从而跳出第9行的循环，继续执行第10行。</p><p>这时，CPU1的a缓存区域也处于Invalid状态，它也会产生BusRd请求，但我们前面分析过，CPU0中对a的赋值可能会晚于b，所以此时CPU1在读取变量a的值时，加载的就可能是老的值，也就是0，那这个时候第10行的assert就会执行失败。</p><p>我们再举一个更极端的例子分析一下：</p><pre><code>// CPU0\nvoid foo() {\n    a = 1;\n    b = a;\n}\n</code></pre><p>这个例子中，b和a之间因为有数据依赖，是不可能乱序执行的，这就意味着上面我们分析的情况一是不会发生的。但由于store buffer的存在，情况二仍然可能发生，其结果就像我们上面分析的那样，CPU执行第10行时会失败。这会让人感到更加匪夷所思。</p><p>为了解决这个问题，CPU设计者就引入了<strong>内存屏障，屏障的作用是前边的读写操作未完成的情况下，后面的读写操作不能发生</strong>。这就是Arm上dmb指令的由来，它是数据内存屏障（Data Memory Barrier）的缩写。</p><p>我们还是继续沿用前面CPU0和CPU1的例子，不过这一次我加入了内存屏障：</p><pre><code>// CPU0\nvoid foo() {\n    a = 1;\n    smp_mb();\n    b = 1;\n}\n\n// CPU1\nvoid bar() {\n    while (b == 0) continue;\n    assert(a == 1);\n}\n</code></pre><p>在这里，smp_mb就代表了多核体系结构上的内存屏障。由于在不同的体系结构上，指令各不相同，我们使用一个函数对它进行封装。加上这一道屏障以后，CPU会保证a和b的赋值指令不会乱序执行，同时写入cache的顺序也与程序代码保持一致。</p><p><strong>所以说，内存屏障保证了，其他CPU能观察到CPU0按照我们期望的顺序更新变量</strong>。</p><p>总的来说，store buffer的存在是为提升写性能，放弃了缓存的顺序一致性，我们把这种现象称为<strong>弱缓存一致性</strong>。在正常的程序中，多个CPU一起操作同一个变量的情况是比较少的，所以store buffer可以大大提升程序的运行性能。但在需要核间同步的情况下，我们还是需要这种一致性的，这就需要软件工程师自己在合适的地方添加内存屏障了。</p><p>好了，到这里你可能也发现了，我们前面说的都是CPU核间同步的“写”的问题，但是核间同步还有另外一个瓶颈，也就是“读”的问题。那这个又要怎么解决呢？我们现在就来看看。</p><h2>失效队列与读屏障</h2><p>我们前面说过，当一个CPU向同伴发出Invalid消息的时候，它的同伴要先把自己的缓存置为Invalid，然后再发出acknowledgement。这个从“把缓存置为Invalid”到“发出acknowledgement”的过程所需要的时间也是比较长的。</p><p>而且，由于store buffer的存在提升了写入速度，那么invalid消息确认速度相比起来就慢了，这就带来了速度的不匹配，很容易导致store buffer的内容还没有及时更新到cache里，自己的容量就被撑爆了，从而失去了加速的作用。</p><p>为了解决这个问题，CPU设计者又引入了<strong>“invalid queue”</strong>，也就是失效队列这个结构。加入了这个结构后，收到Invalid消息的CPU，比如我们称它为CPU1，在收到Invalid消息时立即向CPU0发回确认消息，但这个时候CPU1并没有把自己的cache由Share置为Invalid，而是把这个失效的消息放到一个队列中，等到空闲的时候再去处理失效消息，这个队列就是invalid queue。</p><p>经过这样的改进后，CPU1响应失效消息的速度大大提升了，带有invalid queue的缓存结构是这样的：</p><p><img src=\"https://static001.geekbang.org/resource/image/27/17/277a9efe9e9431c76499813b4f629317.jpg?wh=2284x1803\" alt=\"\"></p><p>我们还是以前面CPU0和CPU1中的例子来做说明。</p><p>假如，CPU0和CPU1的缓存中都有变量a的副本，也就是说变量a所对应的缓存行在CPU0和CPU1中都是Share状态。CPU1中没有变量b的副本，b所对应缓存在CPU0中是Exclusive状态。</p><p>当CPU0在将变量a写入缓存的时候，会产生Invalid消息，这个消息到达CPU1以后，CPU1不再立即处理它了，而是将这个消息放入invalid queue，并且立即向CPU0回复了invalid acknowledgement消息。</p><p>CPU0在得到这个确认消息以后，就可以独占该缓存了，直接将这块缓存变为Modified状态，然后把a写入。在a写入以后，foo函数中的内存屏障就可以顺利通过了，接下来就可以写入变量b的新值。由于b是Exclusive的，它的更新比较简单，你可以自己思考一下。</p><p>接下来我们再看CPU1中的操作。</p><p>当CPU1发起对b的请求时，由于b不在缓存中，所以它会向总线发出BusRd请求，总线会把CPU0缓存中的b的新值1更新到CPU1。同时，b所在的缓存行在两个CPU中都变为Share状态。</p><p>CPU1得到了b的新值以后，就可以退出第10行的while循环，然后对a的值进行判断。但是由于a的Invalid消息还在invalid queue里，没有被及时处理，CPU1还是会使用自己的Cache中的a的原来的值，也就是0，这就出错了。</p><p><strong>你会发现，在这个过程中，虽然CPU1并没有乱序执行两条读指令，但是实际产生的效果却好像是先读到了b的值，后读到了a的值</strong>。如果是在严格遵守MESI协议的CPU中，CPU0一定要确保a的值先更新到CPU1，然后才能继续对b赋值。但是放宽了缓存一致性以后，这段代码就有问题了。</p><p>解决的方法和写屏障的思路是一样的，我们需要引入一个内存屏障，它会让CPU暂停执行，直到它处理完invalid queue中的失效消息之后，CPU才会重新开始执行，例如：</p><pre><code>// CPU0\nvoid foo() {\n    a = 1;\n    smp_mb();\n    b = 1;\n}\n\n// CPU1\nvoid bar() {\n    while (b == 0) continue;\n    smp_mb();\n    assert(a == 1);\n}\n</code></pre><p>你看，这样在bar函数里增加了内存屏障以后，我们就可以保证a的新值是一定能读到的了。可见smp_mb可以同时对store buffer和invalid queue施加影响。</p><p>不过呢，你可能也会发现，在这个例子中，其实我们也不需要smp_mb有那么大的作用。我们只需要在第4行保证store buffer写入的顺序，在第11行保证invalid queue的顺序就好了。所以smp_mb相对于我们的需求来说，做的事情过多了，这也会导致不必要的性能下降。面对这种情况，CPU的设计者也进一步提供了单独的写屏障和读屏障。</p><h2>读写屏障分离</h2><p>分离的写屏障和读屏障的出现，是为了<strong>更加精细地控制store buffer和invalid queue的顺序</strong>。</p><p>再具体一点，写屏障的作用是让屏障前后的写操作都不能翻过屏障。也就是说，写屏障之前的写操作一定会比之后的写操作先写到缓存中。</p><p>读屏障的作用也是类似的，就是保证屏障前后的读操作都不能翻过屏障。假如屏障的前后都有缓存失效的信息，那屏障之前的失效信息一定会优先处理，也就意味着变量的新值一定会被优先更新。</p><p>这里我们讨论的都是读写屏障对store buffer和invalid queue的影响。其实，这里还隐含了一个事实，那就是对CPU乱序执行的影响：<strong>写屏障会禁止写操作的乱序</strong>。</p><p>这个要求虽然是隐含的，但仔细想一下却是显然的，理由很简单。你想，如果某个CPU在进行写操作的时候，实际的执行顺序都是乱序的话，那我们根本就无法讨论新的值什么时候传递到其他CPU。</p><p>而且，分离的读写屏障还有一个好处，就是它可以在需要使用写屏障的时候只使用写屏障，不会给读操作带来负面的影响，这种屏障也可以称为StoreStore barrier。同理，只使用读屏障也不会对写操作造成影响，这种屏障也可以称为LoadLoad barrier。例如我们前面CPU0和CPU1的例子，就可以进一步修改成这样：</p><pre><code>// CPU0\nvoid foo() {\n    a = 1;\n    smp_wmb();\n    b = 1;\n}\n\n// CPU1\nvoid bar() {\n    while (b == 0) continue;\n    smp_rmb();\n    assert(a == 1);\n}\n</code></pre><p>当然，这种修改只有在区分读写屏障的体系结构里才会有作用，比如alpha结构。而在X86和Arm中是没有作用的，这是因为X86采用的TSO模型不存在缓存一致性的问题，而Arm则是采用了另一种称为单向屏障的分类方式。这种单向屏障是怎样的呢？我们也来简单分析一下。</p><h2>单向屏障</h2><p>单向屏障(half-way barrier)也是一种内存屏障，但它并不是以读写来区分的，而是<strong>像单行道一样，只允许单向通行</strong>，例如Arm中的stlr和ldar指令就是这样。</p><p>stlr的全称是store release register，也就是以release语义将寄存器的值写入内存；ldar的全称是load acquire register，也就是以acquire语义从内存中将值加载入寄存器。我们重点就来看看release和acquire语义。</p><p>首先是<strong>release语义</strong>。如果我们采用了带有release语义的写内存指令，那么这个屏障之前的所有读写都不能发生在这次写操作之后，相当于在这次写操作之前施加了一个内存屏障。但它并不能保证屏障之后的读写操作不会前移。简单说，它的特点是<strong>挡前不挡后</strong>。</p><p>在支持乱序执行的CPU（当前高性能多核CPU基本都支持乱序执行）中，使用release语义的写内存指令比使用全量的dmb要有更好的性能。</p><p>需要注意的是，stlr指令除了具有StoreStore的功能，它同时还有LoadStore的功能。LoadStore barrier可以解决的问题是真实场景中比较少见的，所以在这里我们就先不关心它了。对于最常用的StoreStore的问题，我们在Arm中经常使用stlr这条带有release语义的写指令来解决，尽管它的能力相比我们的诉求还是大了一些。</p><p>接着我们再来看一下与release语义相对应的<strong>acquire语义</strong>。它的作用是这个屏障之后的所有读写都不能发生在barrier之前，但它不管这个屏障之前的读写操作。简单说就是<strong>挡后不挡前</strong>。</p><p>与stlr相对称的是，它同时具备LoadLoad barrier的能力和StoreLoad barrier的能力。在实际场景中，我们使用最多的还是LoadLoad barrier，此时我们会使用ldar来代替。</p><h2>总结</h2><p>好了，今天我们这节课的内容就讲完了，我们简单回顾一下。</p><p>在这节课，我们讲解了在CPU的具体实现中，通过放宽MESI协议的限制来获得性能提升。具体来说，我们引入了store buffer和 invalid queue，它们采用放宽MESI协议要求的办法，提升了写缓存核间同步的速度，从而提升了程序整体的运行速度。</p><p>但在这放宽的过程中，我们也看到会不断地出现新的问题，也就是说，一个CPU的读写操作在其他CPU看来出现了乱序。甚至，即使执行写操作的CPU并没有乱序执行，但是其他CPU观察到的变量更新顺序确实是乱序的。这个时候，我们就必须加入内存屏障来解决这个问题。</p><p>最后我们也学习了读写屏障分离和单向屏障。在不同的情况下，我们需要的内存屏障是不同的。使用功能强大的内存屏障会给系统带来不必要的性能下降，为了更精细地区分不同类型的屏障，CPU的设计者们提供了分离的读写屏障(alpha)，或者是单向屏障(Arm)。</p><p>如何正确地使用内存屏障是一件很考验功底的事情，如果该加的地方没加，会带来非常严重的正确性问题。在操作系统，数据库，编译器等领域，会产生非常深远的影响，其代价甚至是完全无法接受的。而在不需要加的地方，如果你施加了比较重的屏障则可能带来性能下降，成为系统瓶颈。关于读写屏障更多的实际应用案例，你可以参考下我们华为JDK公众号发布的<a href=\"https://mp.weixin.qq.com/s/74c5PYvUC2UoacQKRmpNfw\">这篇文章</a>。</p><h2>思考题</h2><p>假如以下代码是Java代码，你可以看到，代码中采用了full fence来保证缓存一致性。请阅读Java的<a href=\"http://openjdk.java.net/jeps/171\">相关API文档</a>并思考，fullFence是否合理？如果不合理，应该使用哪个API对它进行替代呢？欢迎在留言区分享你的想法，我在留言区等你。</p><pre><code>// CPU0\nvoid foo() {\n    a = 1;\n    unsafe.fullFence();\n    b = 1;\n}\n\n// CPU1\nvoid bar() {\n    while (b == 0) continue;\n    unsafe.fullFence();\n    assert(a == 1);\n}\n</code></pre><p>其实，除此之外，上面这段代码还有一种改法。我给你一个小提示，就是使用volatile关键字。我们会在第17课对volatile关键字进行讲解，如果你有兴趣也可以提前预习一下。</p><p><img src=\"https://static001.geekbang.org/resource/image/3d/5f/3da3d49a6f7f24889ff20902b2a0425f.jpg?wh=2284x1176\" alt=\"\"></p><p>欢迎你把这节课分享给更多对MESI协议和内存屏障感兴趣的人。我是海纳，我们下节课见。</p>","float_qrcode":"https://static001.geekbang.org/resource/image/83/1c/83e244537b714e899fd2e7896e22371c.png","article_cover_hidden":false,"is_required":true,"score":"41638460800","like_count":3,"article_subtitle":"","audio_download_url":"https://static001.geekbang.org/resource/audio/1d/ea/1d3411336af59a697599a022fa0fbcea.mp3","had_viewed":false,"article_title":"16 | 内存模型：有了MESI为什么还需要内存屏障？","column_bgcolor":"#F6F7FB","offline_package":"{}","audio_title":"12.2 海纳16_membar.MP3_R.mp3","audio_time_arr":{"m":"19","s":"25","h":"00"},"text_read_percent":0,"cid":450,"article_cshort":"<p>你好，我是海纳。</p><p>上一节课，我们学习了MESI协议，我们了解到，MESI协议能够解决多核 CPU体系中，多个CPU之间缓存数据不一致的问题。但是，如果CPU严格按照MESI协议进行核间通讯和同步，核间同步就会给CPU带来性能问题。既要遵守协议，又要提升性能，这就对CPU的设计人员提出了巨大的挑战。</p><p>那严格遵守MESI协议的CPU会有什么样的性能问题呢？我们又可以怎么来解决这些问题呢？今天我们就来仔细分析一下。搞清楚了这些问题，你会对C++内存模型和Java内存模型有更加深入的理解，在分析并发问题时能够做到有的放矢。</p><h2>严守MESI协议的CPU会有啥问题？</h2><p>我们上节课说过，MESI代表的是Modified、Exclusive、Shared、Invalid这四种缓存状态，遵守MESI协议的CPU缓存会在这四种状态之间相互切换。这种CPU缓存之间的关系是这样的：</p><p><img src=\"https://static001.geekbang.org/resource/image/1d/57/1dabf3dccd6113d76b29c05dd3ea3c57.jpg?wh=2284x1407\" alt=\"\"></p><p>从上面这张图你可以看到，Cache和主内存(Memory)是直接相连的。一个CPU的所有写操作都会按照真实的执行顺序同步到主存和其他CPU的cache中。</p><p>严格遵守MESI协议的CPU设计，在它的某一个核在写一块缓存时，它需要通知所有的同伴：我要写这块缓存了，如果你们谁有这块缓存的副本，请把它置成Invalid状态。Invalid状态意味着该缓存失效，如果其他CPU再访问这一缓存区时，就会从主存中加载正确的值。</p>","video_width":0,"column_could_sub":true,"video_id":"","sku":"100094901","video_cover":"","author_name":"海纳","column_is_onboard":true,"audio_url":"https://res001.geekbang.org/media/audio/1d/ea/1d3411336af59a697599a022fa0fbcea/ld/ld.m3u8","chapter_id":"2389","column_had_sub":true,"column_cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg","neighbors":{"left":{"article_title":"15 | MESI协议：多核CPU是如何同步高速缓存的？","id":461801},"right":{"article_title":"17 | NUMA：非均匀访存带来了哪些提升与挑战？","id":464090}},"rate_percent":0,"footer_cover_data":{"img_url":"https://static001.geekbang.org/resource/image/f0/fe/f0c47daf3262a8df6cf352d89896abfe.png","link_url":"https://time.geekbang.org/article/427012","mp_url":""},"float_app_qrcode":"","column_is_experience":false,"rate":{"1":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"2":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"3":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0}},"product_id":100094901,"had_liked":false,"id":462113,"free_get":false,"is_video_preview":false,"article_summary":"发起写请求的CPU中的缓存状态可能是Exclusive、Modified和Share，每个状态下的处理是不一样的。","column_sale_type":0,"float_qrcode_jump":"https://time.geekbang.org/serv/v4/misc/jump?uri=https%3A%2F%2Ftime.geekbang.org%2Fhybrid%2Fmp%2Fjump%3Furl%3Dhttps%253A%252F%252Fstatic001.geekbang.org%252Fresource%252Fimage%252Fee%252Fb2%252Feedd4090668aabfac91b2b445d27cbb2.png","column_id":450,"article_sharetitle":"内存模型：有了MESI为什么还需要内存屏障？","article_poster_wxlite":"https://static001.geekbang.org/render/screen/38/c0/388083d6468a958279f293bdc59553c0.jpeg","article_features":0,"comment_count":15,"audio_md5":"1d3411336af59a697599a022fa0fbcea","offline":{"size":19163015,"file_name":"b0a89f6ce8c663eddd609740d08765ce","download_url":"https://static-acl-001.geekbang.org/resource/zip/article/462113/b0a89f6ce8c663eddd609740d08765ce.zip?auth_key=1641482301-273c74bf6ee1484bb0fcc4eeb8487f46-0-2a2b8185acd3ea7fcfc2766686034e7b"},"video_size":0,"hls_videos":[],"video_time":"","article_could_preview":false,"article_ctime":1638460800,"share":{"content":"学好内存，掌握系统开发诀窍","title":"海纳 · 编程高手必学的内存知识","poster":"https://static001.geekbang.org/resource/image/bc/c5/bcaf3ff69e538f8749970edf2301bbc5.jpg","cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg"}},"464090":{"text_read_version":0,"audio_size":21035595,"article_cover":"https://static001.geekbang.org/resource/image/e0/b0/e05aa2fb70da1e92e686e0204fyyccb0.jpg","subtitles":[],"product_type":"c1","audio_dubber":"海纳","is_finished":false,"like":{"had_done":false,"count":0},"audio_time":"00:21:54","video_height":0,"article_content":"<p>你好，我是海纳。</p><p>在硬件篇的最后一节课，我们来看两个比较重要的物理内存问题。在<a href=\"https://time.geekbang.org/column/article/430073\">第1节课</a>，我们讲到物理内存就是指主存，这句话是不太精确的，其实大型服务器的物理内存是由很多部分组成的，主要包含<strong>外设所使用的内存和主存</strong>。</p><p>这节课，我们先会对计算机是如何组织外设所使用的内存进行分析，因为这是你了解设备驱动开发的基础；接下来，我们将分析主存，不过在展开之前，你还是需要了解一下它的内部结构，才能更好的理解。</p><p>如果你从CPU的角度去看，就会发现物理内存并不是平坦的，而是坑坑洼洼的。正是因为这样的特点，也就导致CPU对物理内存的访问速度也不一样。同时，有些内存可以使用CPU Cache，有些则不可以。我们把这种组织方式称为<strong>异质（Heterogeneity）式</strong>的结构。</p><p>再往深入拆解，在异质式结构中，CPU不仅仅对外设内存和主存的访问速度不一样，它访问主存不同区间的速度也不一样。换句话说，<strong>不同的CPU访问不同地址主存的速度各不相同</strong>，我们把采用这种设计的内存叫做非一致性访存（Non-uniform memory access，NUMA）。</p><p>通常，在进行应用程序内存管理时，正确使用NUMA可以极大地提升应用程序的吞吐量；相应地，如果NUMA的配置不合理，也有可能带来比较大的负面影响。而且，在多核体系结构的服务器上，合理地通过控制NUMA的绑定，来提升应用程序的性能，对于服务端程序员至关重要。为了帮助你合理运用NUMA，今天这节课，我们就来详细分析NUMA会为应用程序带来哪些提升与挑战。</p><!-- [[[read_end]]] --><p>NUMA的内容比较多，我放在了这节课的后半部分讲解。我们先来分析计算机是如何组织外设所使用的内存的？</p><h3>再论物理内存</h3><p><strong>外设所需要的内存主要包括外设的工作内存、DMA区域和用于IO映射的内存</strong>。在Linux系统上，我们可以使用以下命令查看物理内存分布情况：</p><pre><code>$ cat /proc/iomem\n00000000-00000fff : reserved\n00001000-0009fbff : System RAM\n0009fc00-0009ffff : reserved\n000a0000-000bffff : PCI Bus 0000:00\n000c0000-000c8dff : Video ROM\n000c9000-000c99ff : Adapter ROM\n000f0000-000fffff : reserved\n  000f0000-000fffff : System ROM\n00100000-3f7fefff : System RAM\n  01000000-0172ac34 : Kernel code\n  0172ac35-01d1c9bf : Kernel data\n  01e74000-01fdbfff : Kernel bss\n3f7ff000-3f7fffff : reserved\n3f800000-3fffffff : RAM buffer\n40000000-47ffffff : System RAM\nf0000000-fbffffff : PCI Bus 0000:00\n  f0000000-f1ffffff : 0000:00:02.0\n    f0000000-f015ffff : efifb\n  f2000000-f2ffffff : 0000:00:03.0\n    f2000000-f2ffffff : xen-platform-pci\n  f3000000-f300ffff : 0000:00:02.0\n  f3020000-f3020fff : 0000:00:02.0\n  f3021000-f3021fff : 0000:00:04.0\n    f3021000-f3021fff : ehci_hcd\nfc000000-ffffffff : reserved\n  fec00000-fec003ff : IOAPIC 0\n  fee00000-fee00fff : Local APIC\n</code></pre><p>你会发现，物理内存最重要的三个部分是：</p><ol>\n<li><strong>从640K（0xa0000）到1M（0xfffff）区间，是被ISA设备的RAM和ROM占据的</strong>；</li>\n<li><strong>从1M开始才是主存（System RAM），同时我们也注意到，主存并不是连续的</strong>；</li>\n<li><strong>物理内存的最后256M（0xf0000000到0xffffffff）保留给了PCI设备，用于IO内存映射</strong>。</li>\n</ol><p>接下来，我们就对这些内存进行详细地分析。先考察实模式下低于1M的内存，我们说从640K到1M这一段区间是预留给ISA设备的，由于早期的显卡是通过ISA总线和CPU进行通讯的，而现代显卡则是使用PCI/PCIe总线与CPU通讯，显卡作为最典型的外设，我就以它为例对这段内存进行说明。</p><p>你可能已经注意到，操作系统在刚启动的时候，显示器上会显示操作系统相关的信息，包括系统版本号、进入BIOS提示信息等内容，不过内容全是字符，没有漂亮的图形界面。在经过了系统引导之后，才有<strong>图形界面接口</strong>（Graph User Interface，GUI）。</p><p>其实，这就是显卡的两种工作模式：一种是<strong>字符模式</strong>，另一种是<strong>图形模式</strong>。在字符模式下，只能显示字符。而在图形模式下则可以对屏幕上的每一个像素进行操作。在Linux内核的加载启动阶段，选择了使用字符模式。当CPU进入保护模式以后，才开始初始化各种外设，设置它们的输入输出端口（IO Port）和相关的内存映射，在这之后，显卡才进入图形模式。</p><p>在字符模式下，BIOS会将显卡的显存映射到物理地址0xb8000（位于0xa0000~0xfffff区间内）。在实模式下，我们可以通过mov指令向这个地址直接写入数据，然后显示器就会显示对应的内容。例如，以下实模式代码就可以在屏幕的左上角显示白色的字符A：</p><pre><code> movw $0xb800, %ax\n movw %ax, %gs\n movl $0x0, %edi\n movb $0xf, %ah\n movb $0x41, %al\n movw %ax, %gs:(%edi)\n</code></pre><p>在保护模式下，显存仍然在物理地址0xb8000。但是，在保护模式下，我们只能使用线性地址来进行内存访问，所以操作系统必然要在准备内核空间页表项时，准备好从虚拟地址到物理地址的映射，将显存的物理地址通过页表管理起来。</p><p>这种工作方式的显存空间非常小。这是因为早期的VGA显卡也是ISA设备，而ISA设备可以使用的总内存，是从640KB到1MB之间的物理地址空间。在<a href=\"https://time.geekbang.org/column/article/430173\">导学（一）</a>里，我们讲解CPU总线的时候提到过，早期的CPU与外设之间的总线是ISA总线，后来PCI/PCIe总线因为具有更好的扩展性和远超ISA总线的速度得到普及。所以后来的显卡也不再使用这种，提前映射到物理内存的方式了，而是采用PCI总线来和CPU进行通讯，但因为兼容性问题，所以早期的设计得到了保留。</p><p>PCI总线上连接的设备称为<strong>PCI设备</strong>。上面的第三部分内存就是为PCI设备准备的。PCI设备的连接方式和详细的初始化过程，是由PCI Specification规定的。这部分内容属于设备驱动开发需要掌握的知识，与我们的课程关系不大，所以就不再详细介绍了。我们来重点关注CPU是如何与PCI设备通过内存进行交互的。</p><p>CPU与外设进行交互主要有两种手段，分别是<strong>IO端口(IO Port)和IO内存映射（Memory Mapped IO， MMIO）</strong>。IO端口是最基本的手段，在ISA设备上就在应用，它使用in/out等专属指令对外设的寄存器进行操作：<strong>设置、读取状态，以及控制数据传输</strong>。但是IO端口不适合进行大规模的数据传输，所以PCI设备主要还是通过MMIO进行数据通讯。</p><p>PCI设备在初始化时，操作系统会通过IO端口读取它的基地址寄存器组（Base Address Registers，BARs），寄存器组里描述了这个设备所需的内存空间的大小。然后，操作系统使用ioremap为它分配虚拟内存。</p><p>上面过程的详细步骤如下所示：</p><ol>\n<li>为外设分配物理内存。外设的物理内存可能由BIOS或者操作系统分配，如果是由操作系统分配，则需要由驱动程序主动调用request_mem_region进行物理地址分配。在32位系统上，往往是4G物理地址的最后256M预留给PCI设备，在64位系统上，则可以分配更多的物理内存；</li>\n<li>如果是操作系统负责分配的，则CPU通过IO端口将分配到的物理内存写入PCI设备，通过操作系统统一管理，PCI设备的IO内存就不会冲突了；</li>\n<li>使用ioremap分配虚拟地址空间，并映射到上一步获得的物理地址；</li>\n<li>使用ioremap返回的虚拟地址空间进行通讯。CPU可以使用普通的mov指令，像访问内存一样去访问外设的内存。</li>\n</ol><p><strong>IO端口主要用于状态读取和设置等控制命令的通讯，而IO内存映射主要用于大量的数据传输</strong>。</p><p>在理解了CPU是如何与物理内存中外设所需的内存交互后，我们再详细研究物理内存中最重要的部分：主存。我们在前面提到NUMA是提升应用程序性能的重要手段。接下来我们具体看一看NUMA为我们的应用程序带来了哪些提升和挑战。</p><h3>NUMA</h3><p>在多核服务器上，主存也并不是一段平坦的同质的内存。为了加速性能，人们发明了<strong>非一致性内存访问</strong>（Non-uniform memory access，NUMA），与之对应的是<strong>一致性内存访问</strong>（Uniform Memory Access， UMA）。</p><p>这里的一致性是指，<strong>同一个CPU对所有内存的访问的速度是一样的，因为物理内存是连续且集中的</strong>。</p><p>而非一致性是指，<strong>内存在物理上被分为了多个节点node，CPU可以访问所有节点，但是为了提升访问效率，CPU可以有选择地优先访问离自己近的内存节点</strong>。所以在多核处理器上，CPU也根据内存节点划分成多个组，每个组里的CPU访问同一个内存节点的效率是相同的。当然了，任何一个CPU都可以访问全部的内存节点，只不过因为“距离”远近的关系，访问效率不一样。</p><p>回顾历史，一致性内存访问（下称UMA）发展的时间很长，但是随着多核技术的发展，UMA存在的问题和面临的挑战越来越明显。</p><p><strong>因为UMA是基于总线的，CPU需要先经过前端总线（Front Side Bus，FSB）连接到北桥，然后北桥再连接到内存控制器进行内存访问</strong>。如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/6d/5c/6df74fc271278af73f5fbab80fa6ff5c.jpg?wh=2284x1251\" alt=\"\"></p><p>随着处理器核数的增多，UMA面临的挑战主要包括两个方面：</p><p>1.<strong>总线的带宽压力会越来越大，同时每个节点可用带宽会减少</strong>；</p><p>2.<strong>总线的长度也会因此而增加，进而增加访问延迟</strong>。</p><p>为了解决以上两个问题，<strong>NUMA架构逐渐成为主流</strong>。和UMA不同，在NUMA架构下每个 CPU 现在都有自己的本地内存节点，CPU与CPU之间点对点互联。使用这种方式的典型代表是intel的快速通道互联QPI（Intel QuickPath Interconnect）。如果一个CPU要访问远程节点的内存，则先通过QPI到达远程节点CPU的内存控制器，然后再进行数据传输。</p><p><img src=\"https://static001.geekbang.org/resource/image/74/2c/74dcbf79468d60a6cced6228c22cb22c.jpg?wh=2284x1105\" alt=\"\"></p><p>如上图所示，连接到 CPU1 的内存控制器的内存被认为是本地内存。连接到另一个 CPU 插槽 (CPU2) 的内存被视为 CPU1 的外部或远程内存。远程内存访问比本地内存访问有额外的延迟开销，因为它必须<strong>遍历互连</strong>（点对点链接）并连接到远程内存控制器。由于两者内存位置不同，访问方式也不同，因此这种系统会经历“不均匀”的内存访问时间。</p><p>UMA架构的优点很明显就是结构简单，所有的CPU访问内存都是一致的，都必须经过总线。然而它缺点我们再前面也提到了，就是随着处理器核数的增多，总线的带宽压力会越来越大。解决办法就只能扩宽总线，然而成本十分高昂，未来可能仍然面临带宽压力。<strong>而NUMA在扩展时只需要关注CPU之间的连接，不占用总线带宽，自然就成为现代处理器的选择</strong>。</p><p>在了解这些知识之后，我们来学习如何发挥NUMA的作用。接下来，我们来介绍numactl工具，方便你学习如何查看和使用NUMA信息。</p><h3>正确使用NUMA</h3><p>在开始实验之前，建议你找到一台服务器，因为个人电脑一般是不带NUMA的。首先我们可以使用numactl -H 命令，这个命令可以查看到机器上有多少个NUMA节点、每个节点包括哪些处理器核，以及不同节点之间访问速度的差异。如下图所示：</p><pre><code>available: 4 nodes (0-3)\nnode 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31\nnode 0 size: 128132 MB\nnode 0 free: 113084 MB\nnode 1 cpus: 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63\nnode 1 size: 129020 MB\nnode 1 free: 123298 MB\nnode 2 cpus: 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\nnode 2 size: 129020 MB\nnode 2 free: 122371 MB\nnode 3 cpus: 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127\nnode 3 size: 129019 MB\nnode 3 free: 124767 MB\nnode distances:\nnode   0   1   2   3\n  0:  10  16  32  33\n  1:  16  10  25  32\n  2:  32  25  10  16\n  3:  33  32  16  10\n</code></pre><p>代码中node distance 的含义在这里需要解释下，它代表了CPU访问不同内存节点的速度相对关系。访问本地内存节点速度记作10，上图也可以看到，每个节点到自身的distance都是10。访问其他节点的速度大于10，例如33，表示的是node0访问node3的速度是node0访问本地内存的速度的3.3倍，以此类推。</p><p>除此之外，<strong>还可以使用numactl --show来查看NUMA的默认策略</strong>，关于内存策略，我们在下面的内容中会继续介绍，建议你一定要认真地读下去哦。</p><pre><code>policy: default\npreferred node: current\n</code></pre><p>numactl工具还有一个重要的功能，那就是“绑核”。这个功能可以指定可执行程序运行在哪些CPU上，同时也可以指定程序在哪些内存节点进行内存分配。</p><p>绑核的意思就是<strong>将进程的运行环境和特定的CPU组，内存节点捆绑在一起</strong>。实际应用中，我们可以根据自身需求，调整绑核策略，来提升应用程序的性能，我们通过简单例子来学习如何绑核。例子的代码如下所示：</p><pre><code>#include&lt;stdlib.h&gt;\n\n#define N 100000000\nint main() {\n  int *a = (int*) malloc(N*sizeof(int));\n  for(int j=0; j&lt; 8192; j++) {\n    for(int i=j; i&lt; N; i+=8192) {\n      a[i] = j;\n    }\n  }\n  return 0;\n}\n</code></pre><p>接下来，我们使用以下两个命令来测试CPU和内存绑到相同节点和不同节点的性能：</p><pre><code>$ time numactl --membind=0 --cpunodebind=0 ./a.out\n$ time numactl --membind=0 --cpunodebind=3 ./a.out\n</code></pre><p>从上面程序的执行结果能够区分出将a.out的内存和CPU绑在相同的节点上，以及绑在不同节点上，这两种情况的性能差异。</p><p>实验的结果你可以自己找一台NUMA服务器测试。最终你会发现<strong>绑在相同核上的程序运行得更快</strong>，这是因为我们这个示例需要的内存比较少，也就4个G，是远小于当前机器单个节点的容量（128G）的，因此访问本地内存完全能满足应用的需求，本地内存的速度我们前面提到是大于远程访问的，所以运行的也就越快。</p><p>那么是不是我们都应该将应用绑在同一个节点上呢？答案是否定的，在这节课的结尾我会给大家讲一个常见的案例来说明这一点。</p><p>除了使用numactl之外，还可以在应用内部创建进程时进行绑核，这个可能在实际应用中对大家更有帮助，接下来我们来学习如何在创建进程时进行绑核。</p><p>libnuma是一套封装了NUMA相关操作的共享库，目的是为开发者提供一套绑核操作的API。使用也非常简单，只要在源码文件中引入相应的头文件，并且在编译时加入链接选项。就可以进行使用了。关于共享库的使用方法，相信你在学习前面的课程之后，应该能信手拈来。</p><p>下面我们来编写一个简单的例子，用来判断当前系统是否支持NUMA吧。代码如下：</p><pre><code>#include&lt;stdio.h&gt;\n#include&lt;numa.h&gt;\nint main() {\n  if(numa_available() &lt; 0) {\n    printf(&quot;your current system does not support NUMA!&quot;);\n  }\n  printf(&quot;max numa node id is %d\\n&quot;,numa_max_node());\n  return 0;\n}\n</code></pre><p>我们使用这条编译命令：</p><pre><code>gcc -o test-numa test-numa.c -lnuma \n</code></pre><p>然后就可以运行程序查看当前系统是否支持NUMA，以及系统中NUMA节点个数。</p><p>通过这个例子，我们看到了<strong>对应用程序进行正确的绑核操作，有利于提升应用程序的性能</strong>。前面的内容中也提到了影响性能的因素还有NUMA策略，所以，我们再来看一下NUMA内存策略的问题。</p><h3>NUMA内存策略</h3><p>所谓内存策略就是CPU访问内存节点的策略，分为先访问本地节点、先访问远程节点、只能访问本地节点等等。内存策略是libnuma提供的最主要的功能。现在实现的内存策略主要有4种，如下表所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/9f/17/9f8f3bebd41a9d81348byycf30614817.jpg?wh=2284x1519\" alt=\"\"></p><p>在了解了内存策略之后，我们可以使用set_mempolicy接口来对进程的内存策略进行调整，这里用一个实际举例来展示这些API的功能。</p><pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;stdint.h&gt;\n#include &lt;numa.h&gt;\n#include &lt;numaif.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;sys/mman.h&gt;\n\n#define N ((1UL &lt;&lt; 38) / sizeof(int)) // 128GB\n\nint main() {\n  uint64_t num_nodes = numa_num_configured_nodes();\n  uint64_t all_nodes_mask = (1 &lt;&lt; numa_num_configured_nodes()) - 1;\n  uint64_t my_nodes_mask = all_nodes_mask ^ 0b0110; \n\n  set_mempolicy(MPOL_BIND, &amp;my_nodes_mask, 1);\n\n  // allocate large array and write to it\n  int *a = malloc(N * sizeof(int));\n  for (size_t i=1; i &lt; N; i++) {\n    a[i] = 1;\n  }\n\n  free(a);\n  return 0;\n}\n</code></pre><p>在上面的例子中我们分配了一个比较大的内存，你在实际测试过程中可以根据自己机器上的内存节点大小进行调整即可（修改第9行左移的位数），使它接近一个内存节点的空闲内存大小。</p><p>你可以在不同机器上使用不同策略（修改代码的第16行第一个参数），来验证内存策略的效果。<strong>不同架构CPU在内存策略上的实现还是有较大的不同的，aarch64平台和x86平台的差异比较明显</strong>，如果你有兴趣的话，可以自行尝试。更多关于libnuma API的说明可以参考附录给出的文档。</p><p>在前些年，MySQL 会经常发现这样一个问题，就是<strong>明明操作系统还有很多内存，但是MySQL的性能在某个时间点会急剧下降，但只要关闭NUMA问题就可以得到解决</strong>。</p><p>背后的原因就是和NUMA的内存策略有关，linux系统有这样一个参数zone_reclaim_mode，它的作用是当本地节点内存空间不足时，决定如何回收内存，当它的值非0时，系统将先从当前节点回收内存，然后再进行分配。它的取值状态如下：</p><ul>\n<li>0：在回收本地内存之前，在其他内存节点分配内存；</li>\n<li>1：清理当前节点不用的页，然后在本地节点分配内存；</li>\n<li>2：将缓存中的脏页写到磁盘，释放部分内存；</li>\n<li>4：进行swap替换，释放内存。</li>\n</ul><p>而在出现问题的机器上通过查看/proc/sys/vm/zone_reclaim_mode，结果是0（默认也是0），也就是说当本地节点内存不足时，会从其他节点分配内存，看似没什么问题。但是实际上，即便/proc/sys/vm/zone_reclaim_mode为0，问题依然存在。这是怎么一回事呢？这里我就直接贴出当时linux内核的部分代码，你就明白了。</p><pre><code>static void __paginginit init_zone_allows_reclaim(int nid)\n{\n\tint i;\n\tfor_each_node_state(i, N_MEMORY)\n\t\tif (node_distance(nid, i) &lt;= RECLAIM_DISTANCE)\n\t\t\tnode_set(i, NODE_DATA(nid)-&gt;reclaim_nodes);\n\t\telse\n\t\t\tzone_reclaim_mode = 1;\n}\n</code></pre><p>代码的第5行是根据node_distance来判断系统是否支持NUMA（操作系统层面不涉及libnuma的API），如果有node_distance大于RECLAIM_DISTANCE（简单理解就是系统开启了NUMA）则将zone_reclaim_mode的状态置1，这个地方是写死的，所以即便修改了/proc/sys/vm/zone_reclaim_mode，实际生效的zone_reclaim_mode还是1，这样就导致大量的内存分配必须在本地节点。</p><p>而<strong>本地节点的内存已经满了，势必导致频繁swap，性能也因此骤降，所以这个问题也被称为“ swap insanity ”</strong>。这个问题的最终修复方式是将else分支去掉，完整的commit见附录。</p><h3>总结</h3><p>好啦，今天这节课到这里就结束啦，我们来回顾一下这节课的重点内容吧。这节课我们重新审视了物理内存的概念。在之前的课程里，当我们提到物理内存时，都是指的主存，通过这节课的学习，我们看到物理内存除了主存以外，还有设备内存和IO映射内存。</p><p>ISA总线的设备占用了640K至1M的物理空间做为设备的工作内存，例如VGA显卡的显存就位于0xb8000处。</p><p>ISA设备的扩展性很差，不能通过软件进行地址空间配置，性能也比较差，所以它就被PCI总线代替了。CPU和PCI设备交互的方式主要包括IO端口和IO内存映射两种方式。前者要使用专门的IO指令，后者则可以像操作普通内存一样操作IO内存。</p><p>初始化PCI设备时会调用ioremap对设备内存进行映射。ioremap的作用是通过软件的方式为PCI设备分配物理内存地址，然后再分配一段虚拟内存地址，并将这段虚拟内存地址映射到上一步分配的物理地址。ioremap的返回值就是这段虚拟内存地址的起始地址。在保护模式下，虚拟地址到物理地址的转换是由MMU负责的。CPU和外设的通讯使用的地址就是虚拟地址。</p><p>物理内存中最重要的组成部分是主存。<strong>主存也分为一致性访问和非一致性访问（NUMA）</strong>。</p><p>我们首先对NUMA的物理结构进行了介绍，了解到每个CPU都有自己专属的内存，访问自己的专属内存速度最快；虽然一个CPU也可以访问其他CPU的内存，但速度比较慢。</p><p>接着，我们又介绍了numactl工具，用于查看numa信息以及进行绑核操作。将进程正确地绑定在相应的核上可以极大地提升程序性能。</p><p>最后，我们讲到了NUMA上的内存策略，主要有四种：</p><ul>\n<li>bind：只在特定节点分配，如果空间不足则进行swap；</li>\n<li>interleave：本地和远程节点均可分配；</li>\n<li>preferred：指定某个节点分配，当内存不足时，优先选择离指定节点近的节点分配；</li>\n<li>local：优先在本地节点分配，当内存不足时，在其他节点分配。</li>\n</ul><p>只有正确地使用分配策略才能获得比较好的性能收益，我们通过一个MySQL的例子说明了内存策略的重要性。虽然这个问题已经被修复了，但其中的经验教训仍然值得我们学习。</p><h3>思考题</h3><p>在32位机器上，尽管地址总线有32位，可以支持物理地址4G编码，但是Linux实际上支持的内存也不足4G，这是为什么呢？欢迎你在留言区分享你的想法和收获，我在留言区等你。</p><p><img src=\"https://static001.geekbang.org/resource/image/45/bf/45b6485544f6a6e29454596ccf2c9ebf.jpg?wh=2284x921\" alt=\"\"></p><p>好啦，这节课到这就结束啦。欢迎你把这节课分享给更多对计算机内存感兴趣的朋友。我是海纳，我们下节课再见！</p><h3>附录</h3><p>参考文献：</p><p><a href=\"https://man7.org/linux/man-pages/man3/numa.3.html#top_of_page\">https://man7.org/linux/man-pages/man3/numa.3.html#top_of_page</a></p><p><a href=\"https://man7.org/linux/man-pages/man2/set_mempolicy.2.html\">https://man7.org/linux/man-pages/man2/set_mempolicy.2.html</a></p><p><a href=\"https://github.com/torvalds/linux/commit/4f9b16a64753d0bb607454347036dc997fd03b82\">https://github.com/torvalds/linux/commit/4f9b16a64753d0bb607454347036dc997fd03b82</a></p>","float_qrcode":"https://static001.geekbang.org/resource/image/83/1c/83e244537b714e899fd2e7896e22371c.png","article_cover_hidden":false,"is_required":true,"score":"41638720000","like_count":7,"article_subtitle":"","audio_download_url":"https://static001.geekbang.org/resource/audio/f5/a0/f524ee668yy5c7ec0511d95aeca861a0.mp3","had_viewed":false,"article_title":"17 | NUMA：非均匀访存带来了哪些提升与挑战？","column_bgcolor":"#F6F7FB","offline_package":"{}","audio_title":"12.5 海纳 17_numa_01.MP3","audio_time_arr":{"m":"21","s":"54","h":"00"},"text_read_percent":0,"cid":450,"article_cshort":"<p>你好，我是海纳。</p><p>在硬件篇的最后一节课，我们来看两个比较重要的物理内存问题。在<a href=\"https://time.geekbang.org/column/article/430073\">第1节课</a>，我们讲到物理内存就是指主存，这句话是不太精确的，其实大型服务器的物理内存是由很多部分组成的，主要包含<strong>外设所使用的内存和主存</strong>。</p><p>这节课，我们先会对计算机是如何组织外设所使用的内存进行分析，因为这是你了解设备驱动开发的基础；接下来，我们将分析主存，不过在展开之前，你还是需要了解一下它的内部结构，才能更好的理解。</p><p>如果你从CPU的角度去看，就会发现物理内存并不是平坦的，而是坑坑洼洼的。正是因为这样的特点，也就导致CPU对物理内存的访问速度也不一样。同时，有些内存可以使用CPU Cache，有些则不可以。我们把这种组织方式称为<strong>异质（Heterogeneity）式</strong>的结构。</p><p>再往深入拆解，在异质式结构中，CPU不仅仅对外设内存和主存的访问速度不一样，它访问主存不同区间的速度也不一样。换句话说，<strong>不同的CPU访问不同地址主存的速度各不相同</strong>，我们把采用这种设计的内存叫做非一致性访存（Non-uniform memory access，NUMA）。</p><p>通常，在进行应用程序内存管理时，正确使用NUMA可以极大地提升应用程序的吞吐量；相应地，如果NUMA的配置不合理，也有可能带来比较大的负面影响。而且，在多核体系结构的服务器上，合理地通过控制NUMA的绑定，来提升应用程序的性能，对于服务端程序员至关重要。为了帮助你合理运用NUMA，今天这节课，我们就来详细分析NUMA会为应用程序带来哪些提升与挑战。</p>","video_width":0,"column_could_sub":true,"video_id":"","sku":"100094901","video_cover":"","author_name":"海纳","column_is_onboard":true,"audio_url":"https://res001.geekbang.org/media/audio/f5/a0/f524ee668yy5c7ec0511d95aeca861a0/ld/ld.m3u8","chapter_id":"2389","column_had_sub":true,"column_cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg","neighbors":{"left":{"article_title":"16 | 内存模型：有了MESI为什么还需要内存屏障？","id":462113},"right":{"article_title":"18 | Java内存模型：Java中的volatile有什么用？","id":464954}},"rate_percent":0,"footer_cover_data":{"img_url":"https://static001.geekbang.org/resource/image/f0/fe/f0c47daf3262a8df6cf352d89896abfe.png","link_url":"https://time.geekbang.org/article/427012","mp_url":""},"float_app_qrcode":"","column_is_experience":false,"rate":{"1":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"2":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"3":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0}},"product_id":100094901,"had_liked":false,"id":464090,"free_get":false,"is_video_preview":false,"article_summary":"大型服务器的物理内存是由很多部分组成的，主要包含外设所使用的内存和主存。","column_sale_type":0,"float_qrcode_jump":"https://time.geekbang.org/serv/v4/misc/jump?uri=https%3A%2F%2Ftime.geekbang.org%2Fhybrid%2Fmp%2Fjump%3Furl%3Dhttps%253A%252F%252Fstatic001.geekbang.org%252Fresource%252Fimage%252Fee%252Fb2%252Feedd4090668aabfac91b2b445d27cbb2.png","column_id":450,"article_sharetitle":"17 | NUMA：非均匀访存带来了哪些提升与挑战？","article_poster_wxlite":"https://static001.geekbang.org/render/screen/d7/0e/d780ca4780722412f02abaf45968cd0e.jpeg","article_features":0,"comment_count":3,"audio_md5":"f524ee668yy5c7ec0511d95aeca861a0","offline":{"size":21712963,"file_name":"36eb2eaee83555f871e3fae023ddab39","download_url":"https://static-acl-001.geekbang.org/resource/zip/article/464090/36eb2eaee83555f871e3fae023ddab39.zip?auth_key=1641482317-be1c4176bc1f439998508d4cacb3e6a0-0-7e472138e0cbf65c37ac6acd3412e358"},"video_size":0,"hls_videos":[],"video_time":"","article_could_preview":false,"article_ctime":1638720000,"share":{"content":"学好内存，掌握系统开发诀窍","title":"海纳 · 编程高手必学的内存知识","poster":"https://static001.geekbang.org/resource/image/bc/c5/bcaf3ff69e538f8749970edf2301bbc5.jpg","cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg"}},"464954":{"text_read_version":0,"audio_size":18684694,"article_cover":"https://static001.geekbang.org/resource/image/c7/c6/c7f41163b97ceb8dfb0b1bae833f4ec6.jpg","subtitles":[],"product_type":"c1","audio_dubber":"海纳","is_finished":false,"like":{"had_done":false,"count":3},"audio_time":"00:19:27","video_height":0,"article_content":"<p>你好，我是海纳。</p><p>随着这节课的开始，我们将进入到专栏的最后一个模块：<strong>自动内存管理篇</strong>。在这个模块，你将会了解到，以Java为代表的托管型语言是如何自动进行内存管理和回收整理的，这将提高你使用Java、Python、 Go等托管型语言的能力。</p><p>为什么我要把自动内存管理篇放到最后才讲呢？因为要理解这一篇的内容，需要软件篇和硬件篇的知识做铺垫。比如说，在面试时，有一个问题面试官问到的频率非常高，但几乎没有人能回答正确，因为它需要的前置知识太多。这个问题是：Java中的volatile有什么用？如何正确地使用它？</p><p>这个问题之所以会频繁出现在面试中，是因为Java并发库中大量使用了volatile变量，在JVM的研发历史上，它在各种不同的体系结构下产生了很多典型的问题。那么，在开发并发程序的时候，深刻地理解它的作用是非常有必要的。</p><p>幸运的是，前面硬件篇的知识已经帮我们打好了足够的基础，今天我们就可以深入讨论这个问题了。由于在这个问题中，volatile的语义是由Java内存模型定义的，我们就先从Java内存模型这个话题聊起。</p><h2>Java内存模型</h2><p>我们知道在不同的架构上，缓存一致性问题是不同的，例如x86采用了TSO模型，它的<strong>写后写（StoreStore）和读后读（LoadLoad）</strong>完全不需要软件程序员操心，但是Arm的弱内存模型就要求我们自己在合适的位置添加StoreStore barrier和LoadLoad barrier。例如下面这个例子：</p><!-- [[[read_end]]] --><pre><code>public class MemModel {\n    static int x = 0;\n    static int y = 0;\n    public static void main(String[] args) {\n        Thread t1 = new Thread(new Runnable() {\n            @Override\n            public void run() {\n                x = 1;\n                y = 1;\n            }\n        });\n        Thread t2 = new Thread(new Runnable() {\n            @Override\n            public void run() {\n                while (y == 0);\n                if (x != 1) {\n                    System.out.println(&quot;Error!&quot;);\n                }\n            }\n        });\n        t2.start();\n        t1.start();\n        try {\n            t1.join();\n            t2.join();\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        }\n    }\n}\n</code></pre><p>上面这个例子在x86机器上运行是没有问题的，但是在Arm机器就有概率打印出Error。原因是第一个线程t1对变量x和y的写操作的顺序是不能保证顺序的，同时，第二个线程t2读取x和y的时候也不保证顺序。这一点我们在<a href=\"https://time.geekbang.org/column/article/461801\">第15节课</a>和<a href=\"https://time.geekbang.org/column/article/462113\">第16节课</a> 已经分析过了。</p><p>为了解决这个问题，Java语言在规范中做出了明确的规定，也就是在JSR 133文档中规定了Java内存模型。<strong>内存模型是用来描述编程语言在支持多线程编程中，对共享内存访问的顺序</strong>。所以显然，在上面例子中，线程间变量共享的情况，就可以借此来解决。</p><p>在JSR133文档中，这个内存模型有一个专门的名字，叫<strong>Happens-before</strong>，它规定了一些同步动作的先后顺序。当然，这个规范也不是一蹴而就的，它也是经过了几次讨论和更新之后才最终定稿。所以，在早期的JVM实现中仍然存在一些弱内存相关的问题。这些问题我们很难称其为bug，因为标准里的规定就有问题，虚拟机实现只是遵从了标准而已。</p><p>接下来，我们探寻一下Happens-before模型究竟会带来什么样的问题，这样你就能深刻体会到volatile存在的意义了。</p><h2>Happens-before模型</h2><p>Java内存模型（Java Memory Model， JMM）是通过各种操作来定义的，包括对变量的读写操作、加锁和释放锁，以及线程的启动和等待操作。JMM为程序中的动作定义了一种先后关系，这些关系被称为Happens-Before关系。<strong>要想保证操作B可以看到操作A的结果，A和B就必须满足Happens-Before关系</strong>，这个结论与A和B是否在同一线程中执行无关。</p><p>我们先来看Happens-Before的规则，然后再对它的特点进行分析。我们要明确Happens-Before模型所讨论的都是同步动作，包括加锁、解锁、读写volatile变量、线程启动和线程完成等。下面这几条规则中所说的操作都是指同步动作。见下面的表格：</p><p><img src=\"https://static001.geekbang.org/resource/image/12/96/12122e04d343b260a6a0636e2be7e396.jpg?wh=2284x1555\" alt=\"\"></p><p>Happens-Before模型强调的是同步动作的先后关系，对于非同步动作，就没有任何的限制了。这节课的第一个例子，它里面的读写操作都是非同步动作，所以它在不同的体系结构上运行，会得到不同的结果。但这并不违反JMM的规定。</p><p>你要牢记一点的是，<strong>JMM是一种理论上的内存模型，并不是真实存在的。它是以具体的CPU的内存模型为基础的</strong>。可能我这么说，你还是觉得比较抽象，现在，我们来看JSR 133文档中的两个令人费解的例子，你就能理解了。</p><p>第一个例子是控制流依赖，例子中包括了两个线程且变量x和y的初值都是0。第一个线程的代码是：</p><pre><code>// Thread1\nr1 = x;\nif (r1 != 0)\n  y = 1;\n</code></pre><p>第二个线程的代码是：</p><pre><code>// Thread 2\nr2 = y;\nif (r2 != 0)\n  x = 1;\n</code></pre><p>由于存在控制流依赖，这两段代码中，第4行都不能提前到第2行之前执行。换句话说，到目前为止，所有的主流的CPU中，上面两段代码都会按照代码顺序执行。你可以推演一下，最终的运行结果一定是r1和r2的值都是0。</p><p><strong>但是Happens-before是一种理论模型</strong>，如果线程1中，y=1先于r1=x执行，同时线程2中，x=1先于r2=y执行，最后的结果，存在r1和r2的值都是1的可能性。理论上确实可能存在一种CPU，当它进行分支预测投机执行的时候，投机的结果被其他CPU观察到。当然，实际中绝对不可能出现这样的CPU，因为这意味着厂家花费了更多的精力为软件开发者带来了一个巨大的麻烦，而且由于核间同步通讯的要求，CPU的性能还会下降。</p><p>第二个例子是数据流依赖。假设x和y的初值是0，而r1和r2的初值是42。线程1的代码是：</p><pre><code>r1 = x;\ny = r1;\n</code></pre><p>线程2的代码是：</p><pre><code>r2 = y;\nx = r2;\n</code></pre><p>因为每个线程内部的第2行和第1行之间都存在数据依赖，所以这里是无法产生乱序执行的，所以无论你以怎样的顺序对这两个线程进行调度，都不可能出现r1=r2=42的情况的。但是r1=r2=42在Happens-before模型中却是合理的，因为它没有对数据流依赖进行规定。</p><p>也就是说，<strong>普通的变量读写在JMM是允许乱序的</strong>，如果真的有人造出这么愚蠢的CPU，运行出这种结果却是符合Happens-before的规定的。</p><p><strong>但是这两个问题在现实中并不存在</strong>。我这里特别想讲这两个例子的原因，是因为JSR 133文档花费了大量的篇幅在介绍本不应该存在的两个问题，这导致这个文档极其晦涩难懂。</p><p>从实用的角度，我建议你<strong>在理解JMM时，一定要结合具体的CPU体系结构</strong>。大体上讲，JMM加上每一种体系结构都有的控制流依赖和数据流依赖，才是一种比较实用的内存模型。纯粹的JMM本身的实用性并不强。</p><p>JMM是一种标准规定，它并不管实现者是如何实现它的。具体到Java语言虚拟机的实现，当Java并发库的核心开发者Doug Lea将JMM简化之后，就变得更容易理解一些。我们来看Doug Lea的描述。</p><h2>JVM的具体实现</h2><p>Doug Lea为了方便虚拟机开发人员理解Java内存模型，编写了一个名为<a href=\"http://gee.cs.oswego.edu/dl/jmm/cookbook.html\">《Java内存模型Cookbook》</a>的小册子。在这个小册子中，他给出一个表格，<strong>现代的JVM基本都是按照这个表格来实现的</strong>。</p><p><img src=\"https://static001.geekbang.org/resource/image/f0/83/f0a8bc71606dc1d9c2c1735f9e933a83.jpg?wh=2284x1340\" alt=\"\"></p><p>这个表格描述了连续的两个读写动作，JVM应该如何处理。表格的最左列代表了第一个动作，第一行代表了第二个动作。表格中的内容使用了LoadLoad、LoadStore、StoreStore、StoreLoad四种内存屏障，分别表示第一个动作和第二个动作之间应该插入什么类型的内存屏障。</p><p>在上一节课中，我们知道了，在不同的体系结构上，这四类barrier的实际含义并不相同。因为x86采用了<strong>TSO模型</strong>，所以它根本没有定义LoadLoad、LoadStore和StoreStore这三种barrier，x86上只有StoreLoad barrier是有意义的。</p><p>而Arm上，由于存在单向的barrier，所以LoadLoad和LoadStore barrier就可以使用acquire load代替，LoadStore和StoreStore barrier也可以使用release store代替，而StoreLoad barrier就只能使用dmb代替了。</p><p>我们可以看到，表格的第三行刚好就对应了arm的acquire load barrier，所以我们就知道在arm上，JVM在实现volatile读的时候就必然会使用acquire load来代替。表格的第四列则刚好对应arm的release store barrier，同时，arm上的JVM在实现volatile写的时候，就可以使用release store来代替。</p><p>回到这节课开始时的例子，可见，只要将变量 y 改成 volatile，就相当于在第8行和第9行之前增加了StoreStore barrier，同时在第15行和第16行处增加了LoadLoad barrier，那么这段Java代码在Arm上的效果就与上一节课所分析的内存屏障的示例代码逻辑是一致的了。</p><p>只要JVM遵守了JMM的规定，那么不管在什么平台上，最后的运行结果都是一样。在这节课刚开始的那个例子中，只要把变量 y 修改成volatile修饰的，就不会再出现在x86上不会打印Error，而在Arm有机率打印Error的情况了。<strong>所有平台运行结果的一致性是由JVM遵守JMM来保证的</strong>。</p><p>到这里，我们就知道了，Happens-before模型是一种理论模型，它没有规定控制流依赖和数据流依赖。但是在实际的CPU中，这两种依赖都是存在的，这是JVM实现的基础。所以在JVM的实现中，主要是参考了Doug Lea所写的Cookbook中的建议。<strong>从实用的角度，Java程序员就可以从Doug Lea所给出的表格去理解volatile的意义，而不必再去参考JSR 133文档。</strong></p><p>接下来，我们通过两个例子来进一步加深对Java内存模型的理解，看看Java内存模型在实际场景中是如何应用的。</p><h2>JMM应用举例一：AQS</h2><p>与这节课第一个例子相似，JDK的源代码中有很多使用volatile变量的读写来保证代码执行顺序的例子，我们以CountDownLatch来举例，它有一个内部类是Sync，它的定义如下所示：</p><pre><code>private static final class Sync extends AbstractQueuedSynchronizer {\n    Sync(int count) {\n        setState(count);\n    }\n\n    int getCount() {\n        return getState();\n    }\n\n    protected int tryAcquireShared(int acquires) {\n        return (getState() == 0) ? 1 : -1;\n    }\n\n    protected boolean tryReleaseShared(int releases) {\n        // Decrement count; signal when transition to zero\n        for (;;) {\n            int c = getState();\n            if (c == 0)\n                return false;\n            int nextc = c-1;\n            if (compareAndSetState(c, nextc))\n                return nextc == 0;\n        }\n    }\n}\n</code></pre><p>我们看到代码里的tryAcquireShared代表这个方法具有acquire语义，而tryReleaseShared则代表了这个方法具有release语义。从tryAcquireShared的代码里，我们可以推测getState里面应该会有acquire语义，我们继续看AbstractQueuedSynchronizer的代码。</p><pre><code>public abstract class AbstractQueuedSynchronizer\n    extends AbstractOwnableSynchronizer\n    implements java.io.Serializable {\n\n    /**\n     * The synchronization state.\n     */\n    private volatile int state;\n\n    /**\n     * Returns the current value of synchronization state.\n     * This operation has memory semantics of a {@code volatile} read.\n     * @return current state value\n     */\n    protected final int getState() {\n        return state;\n    }\n\n    /**\n     * Sets the value of synchronization state.\n     * This operation has memory semantics of a {@code volatile} write.\n     * @param newState the new state value\n     */\n    protected final void setState(int newState) {\n        state = newState;\n    }\n\n    /**\n     * Atomically sets synchronization state to the given updated\n     * value if the current state value equals the expected value.\n     * This operation has memory semantics of a {@code volatile} read\n     * and write.\n     *\n     * @param expect the expected value\n     * @param update the new value\n     * @return {@code true} if successful. False return indicates that the actual\n     *         value was not equal to the expected value.\n     */\n    protected final boolean compareAndSetState(int expect, int update) {\n        // See below for intrinsics setup to support this\n        return unsafe.compareAndSwapInt(this, stateOffset, expect, update);\n    }\n</code></pre><p>从上面的代码中可以看到，state是一个volatile变量，根据JMM模型，我们可以知道getState方法是一种带有acquire语义的读。</p><p>在为state变量赋值的时候，AbstractQueuedSynchronizer(AQS)提供了两个方法，一个是setState，另一个是compareAndSetState。其中setState是一个带有release语义的写。那为什么还要提供comareAndSet方法呢？</p><p>这是因为compareAndSetState，不仅是强调release语义，它还有原子性语义。这个操作中包含了<strong>取值，比较和赋值三个动作</strong>，如果比较操作不成功，则赋值操作不会发生。</p><p>通过这个例子，我们就可以得到一个结论，<strong>内存屏障与原子操作是两个不同的概念。内存屏障强调的是可见性，而原子操作则是强调多个步骤要么都完成，要么都不做。也就是说一个操作中的多个步骤是不能存在有些完成了，有些没完成的状态的。</strong></p><p>接下来，我们再举一个与内存模型相关的并发例子。这是一道十几年来经久不衰的面试题，也是Java面试官最喜欢问的。这道题是：如何高效地实现线程安全的单例模式？</p><h2>JMM应用举例二：线程安全的单例模式</h2><p>我们知道单例模式是设计模式的一种，它的主要特点是全局只能生成唯一的对象。如何才能写出线程安全的单例模式代码呢？</p><p>我们从单线程最基本的单例模式开始讲起，它的代码是这样的：</p><pre><code>class Singleton {\n    private static Singleton instance;\n    \n    public int a;\n    \n    private Singleton() {\n      a = 1;\n    }\n    \n    public static Singleton getInstance() {\n        if (instance == null)\n            instance = new Singleton();\n        return instance;\n    }\n}\n</code></pre><p>这个类的特点是，<strong>构造函数是私有的</strong>。这意味着，除了在getInstance这个静态方法里，可以使用“new Singleton”的方式进行对象的创建，整个工程中的其他任意位置都不能再使用这种方法进行创建。</p><p>要想得到Singleton的实例就只能使用getInstance这个静态方法。而这个方法每一次都会返回同一个对象。所以这就保证了全局只能产生一个Singleton实例。</p><p>这个单例模式看上去写得很正确，但是面试题中的要求是写出线程安全的单例模式。上面的写法显然不是线程安全的。为什么我这么说呢？</p><p>假设第一个线程调用getInstance时，看到instance变量的值为null，它就会创建一个新的Singleton对象，然后将其赋值给instance变量。当第二个线程随后调用getInstance时，它仍然有可能看到instance变量的值为null，然后也创建一个新的Singleton对象。更具体的过程希望你可以自己进行分析，因为这是并发程序的相关内容，不是我们这节课的重点，所以我就不啰嗦了。</p><p>为了解决这个问题，我们可以将getInstance方法改为同步方法，这样就为调用这个方法加上了锁，从而可以保证线程安全：</p><pre><code>class Singleton {\n    private static Singleton instance;\n    public int a;\n    private Singleton() {\n        a = 1;\n    }\n    public synchronized static Singleton getInstance() {\n        if (instance == null)\n            instance = new Singleton();\n        return instance;\n    }\n}\n</code></pre><p>显然，上面的代码是线程安全的，我们之前分析过，在线程1还未执行完getInstance，线程2就开始执行的情况，在加锁以后就不会再出现了。但是这样会带来新的问题：<strong>访问加锁的方法是非常低效的</strong>。</p><p>所以，又有另外一种实现方式被提出：</p><pre><code>class Singleton {\n    private static Singleton instance = new Singleton();\n    \n    public int a;\n    private Singleton() {\n        a = 1;\n    }\n    \n    public static Singleton getInstance() {\n        return instance;\n    }\n}\n</code></pre><p>上面代码的第二行是在类加载的时候执行的，而类加载过程是线程安全的，所以不管有多少线程调用getInstance方法，它的返回值都是第二行所创建的对象。</p><p>这种创建方式有别于第一种。<strong>第一种单例模式的实现是在第一次调用getInstance时，它是在不得不创建的时候才去创建新的对象，所以这种方式被称为懒汉式；第二种实现则是在类加载时就将对象创建好了，所以这种方式被称为饿汉式</strong>。</p><p>还有的人既想使用懒汉式进行创建，又希望程序的效率比较好，所以提出了双重检查(Double Check)，它的具体实现方案如下：</p><pre><code>class Singleton {\n    // 非核心代码略\n    \n    public static Singleton getInstance() {\n        if (instance == null) {\n            synchronized (Singleton.class) {\n                if (instance == null)\n                    instance = new Singleton();\n            }\n        }\n        return instance;\n    }\n}\n</code></pre><p>大多数情况下，instance变量的值都不为null，所以这个方法大多数时候都不会走到加锁的分支里。如果instance变量值为null，则通过在Singleton.class对象上进行加锁，来保证对象创建的正确性，看上去这个实现非常好。</p><p>但是经过我们这节课的讲解，你就能理解这个写法在多核体系结构上还是会出现问题的。假设线程1执行到第8行，在创建Singleton变量的时候，由于没有Happens-Before的约束，所以instance变量和instance.a变量的赋值的先后顺序就不能保证了。</p><p>如果这时线程2调用了getInstance，它可能看到instance的值不是null，但是instance.a的值仍然是一个未初始化的值，也就是说线程2看到了instance和instance.a这两个变量的赋值乱序执行了。</p><p>这显然是一个写后写的乱序执行，所以修改的办法很简单：<strong>只需要将instance变量加上volatile关键字，即可把这个变量的读变成acquire读，写变成release写</strong>。这样，我们才真正地正确实现了饿汉式和懒汉式的单例模式。</p><h2>总结</h2><p>好啦，今天这节课就结束啦，这节课我们学习了Java内存模型。从这节课中，我们了解到JSR 133中所描述的Java内存模型是一种理论模型，它的规则非常少，以至于连控制流依赖和数据流依赖都没有规定，这导致JSR133文档讨论了很多在现实中根本不存在的情况。</p><p>而我们在讨论JMM的实现时，必然会与具体的CPU相联系。Doug Lea将JMM做了简单的翻译，使用软件程序员可以看懂的语言重新阐释了JSR 133文档。</p><p>到这里，这节课开始处所讲的volatile的机制，其答案也就明晰了。它的作用是为变量的读写增加happens before关系，结合具体的CPU实现，就相当于是为变量的读增加acquire语义，为变量的写增加release语义。</p><p>接下来，我们用两个具体的例子来解释可见性、原子性和volatile的用法。</p><p>第一个例子是Java并发库中的核心数据结构AbstractQueuedSynchronizer(AQS)，它通过使用volatile变量和原子操作来维护对象的状态。</p><p>第二个例子是实现线程安全的单例模式。我们梳理了单例模式的各种实现方式，并详细介绍了double check实现方式的问题，以及如何使用volatile来修复这个问题。</p><h2>思考题</h2><p>请你思考一下，volatile能替代锁（或者CAS操作）的能力吗？比如，下面这个例子的写法，sum的最终结果一定是80000吗？如果不是的话，应该怎么做才能保证呢？欢迎你在留言区分享你的想法和收获，我在留言区等你。</p><pre><code>class AddThread extends Thread {\n    public void run() {\n        for (int i = 0; i &lt; 40000; i++)\n            Main.sum += 1;\n    }\n}\n\nclass Main {\n    public static volatile int sum = 0;\n    public static void main(String[ ] args) throws Exception {\n        AddThread t1 = new AddThread();\n        AddThread t2 = new AddThread();\n        t1.start();\n        t2.start();\n        t1.join();\n        t2.join();\n\n        System.out.println(sum);\n    }\n}\n</code></pre><p><img src=\"https://static001.geekbang.org/resource/image/9e/4e/9ea947633ce6ca2395e8878b3708124e.jpg?wh=2284x1780\" alt=\"\"></p><p>好啦，这节课到这就结束啦。欢迎你把这节课分享给更多对计算机内存感兴趣的朋友。我是海纳，我们下节课再见！</p>","float_qrcode":"https://static001.geekbang.org/resource/image/83/1c/83e244537b714e899fd2e7896e22371c.png","article_cover_hidden":false,"is_required":true,"score":"51638892800","like_count":4,"article_subtitle":"","audio_download_url":"https://static001.geekbang.org/resource/audio/90/96/9032a86b424869ed82b78f48866a6296.mp3","had_viewed":false,"article_title":"18 | Java内存模型：Java中的volatile有什么用？","column_bgcolor":"#F6F7FB","offline_package":"{}","audio_title":"修改 12.6海纳 18_volatile.MP3_R_01.mp3","audio_time_arr":{"m":"19","s":"27","h":"00"},"text_read_percent":0,"cid":450,"article_cshort":"<p>你好，我是海纳。</p><p>随着这节课的开始，我们将进入到专栏的最后一个模块：<strong>自动内存管理篇</strong>。在这个模块，你将会了解到，以Java为代表的托管型语言是如何自动进行内存管理和回收整理的，这将提高你使用Java、Python、 Go等托管型语言的能力。</p><p>为什么我要把自动内存管理篇放到最后才讲呢？因为要理解这一篇的内容，需要软件篇和硬件篇的知识做铺垫。比如说，在面试时，有一个问题面试官问到的频率非常高，但几乎没有人能回答正确，因为它需要的前置知识太多。这个问题是：Java中的volatile有什么用？如何正确地使用它？</p><p>这个问题之所以会频繁出现在面试中，是因为Java并发库中大量使用了volatile变量，在JVM的研发历史上，它在各种不同的体系结构下产生了很多典型的问题。那么，在开发并发程序的时候，深刻地理解它的作用是非常有必要的。</p><p>幸运的是，前面硬件篇的知识已经帮我们打好了足够的基础，今天我们就可以深入讨论这个问题了。由于在这个问题中，volatile的语义是由Java内存模型定义的，我们就先从Java内存模型这个话题聊起。</p><h2>Java内存模型</h2><p>我们知道在不同的架构上，缓存一致性问题是不同的，例如x86采用了TSO模型，它的<strong>写后写（StoreStore）和读后读（LoadLoad）</strong>完全不需要软件程序员操心，但是Arm的弱内存模型就要求我们自己在合适的位置添加StoreStore barrier和LoadLoad barrier。例如下面这个例子：</p>","video_width":0,"column_could_sub":true,"video_id":"","sku":"100094901","video_cover":"","author_name":"海纳","column_is_onboard":true,"audio_url":"https://res001.geekbang.org/media/audio/90/96/9032a86b424869ed82b78f48866a6296/ld/ld.m3u8","chapter_id":"2404","column_had_sub":true,"column_cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg","neighbors":{"left":{"article_title":"17 | NUMA：非均匀访存带来了哪些提升与挑战？","id":464090},"right":{"article_title":"19 | 垃圾回收：如何避免内存泄露？","id":465516}},"rate_percent":0,"footer_cover_data":{"img_url":"https://static001.geekbang.org/resource/image/f0/fe/f0c47daf3262a8df6cf352d89896abfe.png","link_url":"https://time.geekbang.org/article/427012","mp_url":""},"float_app_qrcode":"","column_is_experience":false,"rate":{"1":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"2":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"3":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0}},"product_id":100094901,"had_liked":false,"id":464954,"free_get":false,"is_video_preview":false,"article_summary":"Java中的volatile有什么用？如何正确地使用它？","column_sale_type":0,"float_qrcode_jump":"https://time.geekbang.org/serv/v4/misc/jump?uri=https%3A%2F%2Ftime.geekbang.org%2Fhybrid%2Fmp%2Fjump%3Furl%3Dhttps%253A%252F%252Fstatic001.geekbang.org%252Fresource%252Fimage%252Fee%252Fb2%252Feedd4090668aabfac91b2b445d27cbb2.png","column_id":450,"article_sharetitle":"Java内存模型：Java中的volatile有什么用？","article_poster_wxlite":"https://static001.geekbang.org/render/screen/3f/ed/3f034cee35b3d7c4bec611b4e701e1ed.jpeg","article_features":0,"comment_count":7,"audio_md5":"9032a86b424869ed82b78f48866a6296","offline":{"size":19068741,"file_name":"dae8529d1dcc4d87632592b4ccd01f64","download_url":"https://static-acl-001.geekbang.org/resource/zip/article/464954/dae8529d1dcc4d87632592b4ccd01f64.zip?auth_key=1641482333-53f2862de1424b93933c98311691b823-0-dc4329fea8d97bdff41434b1de352d62"},"video_size":0,"hls_videos":[],"video_time":"","article_could_preview":false,"article_ctime":1638892800,"share":{"content":"学好内存，掌握系统开发诀窍","title":"海纳 · 编程高手必学的内存知识","poster":"https://static001.geekbang.org/resource/image/bc/c5/bcaf3ff69e538f8749970edf2301bbc5.jpg","cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg"}},"465516":{"text_read_version":0,"audio_size":16480015,"article_cover":"https://static001.geekbang.org/resource/image/1f/e1/1fa3b250a9c4d0db51be13cd433ff9e1.jpg","subtitles":[],"product_type":"c1","audio_dubber":"海纳","is_finished":false,"like":{"had_done":false,"count":1},"audio_time":"00:17:09","video_height":0,"article_content":"<p>你好，我是海纳。</p><p>从这节课开始，我们进入一个新的主题，那就是垃圾回收。对于C/C++程序员来说，内存错误是一个非常头疼的问题，常见的错误有内存泄露、悬空指针等。这使得程序员在写程序时必须很小心地申请内存，并且在适当的时候释放。但即便是很优秀的程序员，也很难保证每次申请、释放内存的操作和时机都是正确的。</p><p>为了使得程序员将注意力集中在业务逻辑而不是内存管理，于是自动内存管理技术就诞生了，它也被称为<strong>垃圾回收技术（Garbage Collection，GC）</strong>。</p><p>垃圾回收技术降低了程序员的心智负担，将程序员从繁重的内存管理工作中解放出来，这才使得淘宝这样的大型应用成为可能。但是随着业务越来越复杂，GC算法中固有的停顿造成业务卡顿等问题也变得越来越严重。在一些时延敏感型业务中，业务响应时间和GC停顿的矛盾就更加突出。所以，理解GC算法的基本原理并对其加以优化，是现代Java程序员的一项必备技能。</p><p>接下来的几节课，我们就来学习各种具体的GC算法。这节课我会先介绍GC算法中的基本概念，在这个基础上，我再带你深入了解一类重要的GC算法：引用计数法。通过这节课的学习，你将掌握垃圾回收这个重要话题相关的基本概念，了解GC算法的简单分类和算法评价标准。</p><!-- [[[read_end]]] --><p>我们先从垃圾回收的基本概念讲起吧。</p><h2>垃圾回收的基本概念和定义</h2><p>在GC算法的学习过程中，一个比较大的挑战是概念和术语比较多。在这个专栏里，我会根据需要把专用概念逐个加以介绍，我们先从最简单的开始。</p><p>GC算法中最重要的两个角色就是<strong>Mutator和Collector</strong>。</p><ul>\n<li><strong>Mutator</strong></li>\n</ul><p>Mutator的本意是改变者。这个词所表达的是通过程序来改变对象之间的引用关系。<strong>因为我们所写的所有Java程序，都有可能改变对象的存活和死亡状态，以及它们之间的引用关系，那么这些Java程序就是Mutator</strong>。因为Java程序运行所在的这些线程，我们也称为业务线程，所以在某些情况下，Mutator和业务线程这两个术语是可以混用的。在这节课中，我们使用Mutator这个术语。</p><ul>\n<li><strong>Collector</strong></li>\n</ul><p>Collector用于回收空间中的垃圾，所以叫做收集者。根据不同的GC算法，Collector不仅仅是收集，例如在Mark-Sweep中，它还负责标记存活对象、识别垃圾对象。执行Collector的线程，我们一般称为Collector线程或者GC线程。在某些GC算法中，业务线程也有可能帮助做垃圾回收的工作。所以，<strong>Mutator和Collector只是一种相对的说法，而不是精确的概念</strong>。</p><p>然后，我们还需要理解Java堆的结构，以及如何描述堆中的对象，以便于设计GC算法。</p><p>在软件篇，我们已经详细讨论过Java进程的内存布局了。在这里，我重点强调一下“Java堆”和软件篇中讲的“进程堆”的区别。<strong>进程堆是指进程中可以使用malloc和free进行分配和释放的一块用户态内存区域。而Java堆则专指创建普通Java对象的地方，这一段内存是由虚拟机所管理的</strong>。</p><p>在<a href=\"https://time.geekbang.org/column/article/454080\">软件篇答疑</a>里，我们也讲过，进程的堆的概念比Java堆的概念要大。进程堆还包括了Metaspace、JIT代码段等部分。明确了Java堆的概念以后，我们再来看看堆里的对象该如何进行描述呢？</p><p>假设有两个对象A和B，A引用了B对象，我们就从A向B绘制一条有向边。再把堆中的对象看成是结点，那么，堆里的对象和它的引用关系就可以使用图来表达了。这里有一个例子，可以帮助你理解，如下所示：</p><pre><code>public class Main {\n    public static void main(String args[]){\n        try {\n            A a = new A();\n            a.b = new B();\n            a.c = new C();\n            a.b.a = a;\n            a.b.c = a.c;\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n}\nclass A {\n    public B b;\n    public C c;\n}\nclass B {\n    public A a;\n    public C c;\n}\nclass C {\n    Runnable r;\n}\n</code></pre><p>上面代码的main函数中，创建了三个对象。我们按照上面所描述的办法，使用有向边把具有引用关系的对象表示出来，那么上面的程序就可以用下面的示意图来表示：</p><p><img src=\"https://static001.geekbang.org/resource/image/1e/38/1eyyd8f3d0633677fac471189325c838.jpg?wh=2284x1323\" alt=\"\"></p><p>这样，我们就得到了一个有向图。接下来我们就可以借助图论的各种知识来处理自动内存管理问题了。如果我们在上述main方法的最后，再添加一行代码：</p><pre><code>   a.b = new B();\n</code></pre><p>那么，此时各个对象之间的引用关系就变成下图所示的样子：</p><p><img src=\"https://static001.geekbang.org/resource/image/27/eb/27505138b641257c39ae2abb698344eb.jpg?wh=2284x1223\" alt=\"\"></p><p>可以看到，原来a对象引用着b对象，现在这个引用就指向了新的对象，我们记为b’对象。b对象指向a和c的引用并没有发生变化。</p><p>从这个图中，我们可以清楚地看到此时已经没有任何对象引用b对象了。在执行完这一句之后，我们在代码里也已经没有任何办法可以再次访问到b对象了。那么此时，b对象就变成了<strong>垃圾对象</strong>。而a对象还能使用栈上的一个变量继续访问它，所以它是一个<strong>活跃对象</strong>。</p><p>将对象之间的引用关系抽象成图这种数据结构以后，我们就可以使用图算法来研究垃圾回收问题了。比如说，我们想找出所有活跃的对象，只需要从确定的活跃对象出发，对整个有向图进行遍历，遍历到的就是活跃对象，遍历不到的就是垃圾对象。</p><p>这里我们要先明确什么是“确定的活跃对象”。<strong>所有不在堆中，而指向堆里的引用都是根引用，根引用所指向的对象就是“确定的活跃对象”，这些对象是根对象。根引用的集合就是根集合</strong>。根集合是很多基于可达性分析的GC算法遍历的起点。例如：</p><pre><code>public class Main {\n    public static void main(String args[]){\n        buildA();\n        A objInMain = new A();\n    }\n    public static void buildA() {\n        A objA = new A();\n        B objB = new B();\n        C objC = new C();\n        objA.b = objB;\n        objB.a = objA;\n        objA.c = objC;\n    }\n}\n</code></pre><p>在执行到 buildA 的时候，内存里的真实情况如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/72/4d/72dcabe609ec8f43f252yy039265124d.jpg?wh=2284x1167\" alt=\"\"></p><p>buildA的栈帧里有三个变量objA、objB、objC，它们都有引用指向堆里，所以这三个引用都是根引用。当buildA的栈帧还存在的时候，这些引用就存在，那么堆里的这三个对象就是活跃对象。一旦buildA方法执行完了，它所对应的栈帧就会被销毁，上图中的三个引用就会消失，同时堆里的三个对象也就变成了垃圾对象。</p><p>我们定义了GC算法相关概念和定义后，我们再来看看GC算法的评价标准。</p><h2>GC算法的评价标准</h2><p>各种常用的GC算法，它们的特点各不相同，分别适用于不同的场景。要想搞清楚在什么情况下采用什么算法，我们就要先分析清楚GC算法的特点。具体来讲，常用的评价GC算法的标准有以下几条：</p><ol>\n<li><strong>分配的效率</strong>：主要考察在创建对象时，申请空闲内存的效率；</li>\n<li><strong>回收的效率</strong>：它是指回收垃圾时的效率；</li>\n<li><strong>是否产生内存碎片</strong>：在讲解malloc的时候，我们讲过内存碎片。碎片是指活跃对象之间存在空闲内存，但这一部分内存又不能被有效利用。比如内存里有两块不连续的16字节空闲空间，此时分配器要申请一块32字节的空间，虽然总的空闲空间也是32字节，但由于它们不连续，不能满足分配器的这次申请。这就是碎片空间；</li>\n<li><strong>空间利用率</strong>：这里主要是衡量堆空间是否能被有效利用。比如基于复制的算法无论何时都会保持一部分内存是空闲的，那么它的空间利用率就无法达到100%，这是由算法本身决定的；</li>\n<li><strong>是否停顿</strong>：Collector在整理内存的时候会存在搬移对象的情况，因为修改指针是一种非常敏感的操作，有时候它会要求Mutator停止工作。是否需要Mutator停顿，以及停顿时长是多少，是否会影响业务的正常响应等。停顿时长在某些情况下是一个关键性指标；</li>\n<li><strong>实现的复杂度</strong>：有些算法虽然看上去很美妙，但因为其实现起来太复杂，代码难以维护，所以无法真正地商用落地。这也会影响到GC算法的选择。</li>\n</ol><p>接下来的课程中，我们就具体地研究各种GC算法，并用上面的6条评价准则对它们进行详细地分析。</p><p>GC算法大致上可以分为<strong>引用计数</strong>和<strong>基于可达性分析</strong>的算法两大类。其中，可达性分析算法是当前GC算法研究的热点，也是我们自动内存管理篇的重点。在下一节课中，我们会对它进行详细地讲解。这节课，我们先简要介绍引用计数算法。</p><p>引用计数法是非常简单高效的，它依然被Python和Swift等语言所使用。而且，引用计数法也不仅仅用在垃圾回收这一个场景。在COM组件编程、Linux内核管理物理页面、C++智能指针等场景都能看到它的身影。虽然现在引用计数法已经不是工业界和学术界研究的重点，但是掌握它仍然具有重要的意义。</p><h2>引用计数算法</h2><p>GC算法一个重要的功能是要识别出内存中的哪些对象是垃圾，也就是不会再被使用到的对象。这里就涉及到了垃圾的定义，就是不再被其他对象所引用的对象。从这个定义出发，我们可以想办法统计一个对象是否被引用。如果它被引用的次数大于0，那它就是一个活跃对象；如果它被引用的次数为0，那它就是一个垃圾对象。</p><p>为了记录一个对象有没有被其他对象引用，我们可以在每个对象的头上添加一个叫“计数器”的东西，用来记录有多少其他对象引用了它。Mutator在执行的时候会改变这个计数值，比如以下代码：</p><pre><code>A objA = new A();\nB objB = new B();\nobjA.ref = objB;\n</code></pre><p>执行到第三行的时候，它的引用关系如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/94/a8/94b0c7a2ca01ebc9f76198a7c9cabaa8.jpg?wh=2284x881\" alt=\"\"></p><p>在这个代码中，第1行创建了一个对象，不妨记为对象a，objA这个变量指向对象a，所以这个对象的引用计数就是1；第2行创建了一个B对象，记为对象b，objB变量指向对象b，此时，它的引用计数为1；第3行objA的ref属性也指向对象b，所以对象b的引用计数最终会为2。</p><p>Mutator在运行中还会不断地修改对象之间的引用关系，我们知道，这种引用关系的变化都是发生在赋值的时候。例如，接着上面的例子，我们再执行这样一行代码：</p><pre><code>objA.ref = null\n</code></pre><p>那么从objA到objB的引用就消失了，也就是上图中，那个从A的ref指向B的箭头就消失了。</p><p>对于例子中的赋值语句，如果是Java语言，最终会被翻译成putfield指令，那么JVM在处理putfield指令的时候，就可以加上对引用计数的处理。描述这个过程的伪代码如下所示：</p><pre><code>void do_oop_store(Value * obj, Value value) {\n    inc_ref(&amp;value);\n    dec_ref(obj);\n    obj = &amp;value;\n}\nvoid inc_ref(Value * ptr) {\n    ptr-&gt;ref_cnt++;\n}\nvoid dec_ref(Value * ptr) {\n    ptr-&gt;ref_cnt--;\n    if (ptr-&gt;ref_cnt == 0) {\n        collect(ptr);\n        for (Value * ref = ptr-&gt;first_ref; ref != null; ref=ref-&gt;next)\n            dec_ref(ref);\n    }\n}\n</code></pre><p>大多数语言虚拟机（例如Java虚拟机hotspot、CPython虚拟机等）都是使用C/C++来编写的，所以，我们这里的伪代码也使用C语言来描述。</p><p>上面的代码展示了在把 value 赋值给 obj 这个指针之前，我们必须先改变两个对象的引用计数。一个是obj指针原来指向的对象，它的引用计数要减一，另一个是value对象，它的引用计数加一。如果某个对象的引用计数为0，就把这个对象回收掉（collect方法负责回收内存，根据GC算法的不同，collect的实现会有所不同，所以这里我们只要知道它的功能即可，不必在意它的实现），然后把这个对象所引用的所有对象的引用计数减1。</p><p>我们在写一个对象的域的时候做了一些工作，就好比在更新对象域的时候，对这个动作进行了拦截。所以，GC中对这种特殊的操作起了一个比较形象的名字叫<strong>write barrier</strong>。这里的barrier是屏障，拦截的意思，中文翻译就是写屏障。你要注意与<a href=\"https://time.geekbang.org/column/article/461801\">第15节课</a>我们所讲的内存屏障加以区别。</p><p>那在do_oop_store里，可不可以先做减，后做加呢？就是说第2行和第3行的先后顺序换过来有没有影响呢？答案是不行。因为当obj和value是同一个对象的时候，如果先减后加的话，这个对象就会被回收，内存有可能会被破坏。从而，这个对象就有可能发生数据错误。</p><p>到这里，引用计数算法的基本原理就讲完了。接下来，我们就可以使用GC算法的评价标准来分析一下这种GC算法有什么样的优缺点，这样，我们就能知道它适合用在什么场景中了。</p><h2>引用计数算法的优缺点</h2><p>从算法描述中容易推知，引用计数具备以下优点：</p><ol>\n<li><strong>可以立即回收垃圾</strong>。因为每个对象在被引用次数为0的时候，是立即就可以知道的，所以一旦一个对象成为垃圾，它将立即被释放；</li>\n<li><strong>没有暂停时间</strong>。对象的回收根本不需要另外的GC线程专门去做，业务线程自己就搞定了，所以引用计数算法不需要停顿时间。</li>\n</ol><p>同时，引用计数也存在以下缺点：</p><ol>\n<li><strong>在每次赋值操作的时候都要做额外的计算</strong>。在多线程的情况下，为了正确地维护引用计数，需要同步和互斥操作，这往往需要通过锁来实现，这会对多线程程序性能带来比较大的损失；</li>\n<li><strong>会有链式回收的情况</strong>。比如多个对象对链表形式串在一起，它们的引用计数都为1，当链表头被回收时，整个链表都会回收，这可能会导致一次回收所使用的时间过长；</li>\n<li><strong>循环引用</strong>。如果 objA引用了objB，objB也引用了objA，但是除此之外，再没有其他的地方引用这两个对象了，这两个对象的引用计数就都是1。这种情况下，这两个对象是不能被回收的。如果说上面两条缺陷还可以克服的话，那么循环引用就是比较致命的。</li>\n</ol><p>在使用引用计数算法进行内存管理的语言中，比如<strong>Python和Swift，都会存在循环引用的问题</strong>。Python在引用计数之外，另外引入了三色标记算法，保证了在出现循环引用的情况下，垃圾对象也能被正常回收。关于三色标记算法，我们会在第21节课进行重点讲解。</p><p>而Swift则要求程序员自己解开循环引用，也就是将objA对objB的引用设为NULL，这样objA对objB的引用就消失了，objB的引用计数变为1，接下来就可以正常地将两个对象都回收掉了。</p><p>综上所述，<strong>引用计数实现方便，又可以做到对无用的资源进行立即回收，但他无法应对高并发、高吞吐的场景</strong>。</p><h2>总结</h2><p>好啦，这节课到这里就结束啦，我们一起来回顾下这节课的重点内容吧。在这节课里，我们介绍了GC算法的基本概念和定义，然后又讨论了GC算法的评价标准，以便于后面对各种算法进行对比和选择，最后，我们又讲解了引用计数法这种最直接最简单的GC算法。</p><p>GC算法中有两种重要的角色，分别是<strong>Mutator和Collector</strong>。<strong>Mutator也可以理解为业务线程，collector可以理解为GC线程</strong>。</p><p>Java对象都是在Java堆中创建的，它们之间的相互引用关系可以使用图结构来表示。找出堆中活跃对象的过程就是在堆中对Java对象进行遍历的过程。<strong>遍历算法的起点是根集合</strong>。</p><p>所有不在堆中，而指向堆里的引用都是根引用，<strong>根引用的集合就是根集合</strong>。根集合是很多基于可达性分析的GC算法遍历的起点。</p><p>然后，我们讨论了GC算法的评价标准，总结了以下六点，分别是：<strong>分配的效率、回收的效率、是否产生内存碎片、空间利用率、是否停顿，以及实现的难度</strong>。这六个维度是一般化的评价标准，并不是所有的算法都适用全部的评价标准。比如引用计数算法就不必关心分配的效率和内存碎片问题。</p><p>在这节课的最后，我们一起探讨了引用计数算法。引用计数的特点是在对象的引用在修改的过程中，可以统计对象被引用的次数，当一个对象的被引用次数为0，则对象可以被释放。在修改引用的过程中加一些动作，这种做法被称为write barrier，这是非常重要的一个机制。我们后面会反复遇到各种类型的barrier。</p><h2>思考题</h2><p>除了从栈上出发的引用是根引用，你还知道哪些根引用呢？欢迎在留言区分享你的想法，我在留言区等你。</p><p><img src=\"https://static001.geekbang.org/resource/image/b0/3f/b0e4890b55d0eebc7312bb5c9e62e53f.jpg?wh=2284x1321\" alt=\"\"></p><p>好啦，这节课到这就结束啦。欢迎你把这节课分享给更多对计算机内存感兴趣的朋友。我是海纳，我们下节课再见！</p>","float_qrcode":"https://static001.geekbang.org/resource/image/83/1c/83e244537b714e899fd2e7896e22371c.png","article_cover_hidden":false,"is_required":true,"score":"51639065600","like_count":4,"article_subtitle":"","audio_download_url":"https://static001.geekbang.org/resource/audio/46/df/46cff6fe64baf01f6fb3217b9529a0df.mp3","had_viewed":false,"article_title":"19 | 垃圾回收：如何避免内存泄露？","column_bgcolor":"#F6F7FB","offline_package":"{}","audio_title":"12.8 海纳19_gc_intro.MP3_L.mp3","audio_time_arr":{"m":"17","s":"09","h":"00"},"text_read_percent":0,"cid":450,"article_cshort":"<p>你好，我是海纳。</p><p>从这节课开始，我们进入一个新的主题，那就是垃圾回收。对于C/C++程序员来说，内存错误是一个非常头疼的问题，常见的错误有内存泄露、悬空指针等。这使得程序员在写程序时必须很小心地申请内存，并且在适当的时候释放。但即便是很优秀的程序员，也很难保证每次申请、释放内存的操作和时机都是正确的。</p><p>为了使得程序员将注意力集中在业务逻辑而不是内存管理，于是自动内存管理技术就诞生了，它也被称为<strong>垃圾回收技术（Garbage Collection，GC）</strong>。</p><p>垃圾回收技术降低了程序员的心智负担，将程序员从繁重的内存管理工作中解放出来，这才使得淘宝这样的大型应用成为可能。但是随着业务越来越复杂，GC算法中固有的停顿造成业务卡顿等问题也变得越来越严重。在一些时延敏感型业务中，业务响应时间和GC停顿的矛盾就更加突出。所以，理解GC算法的基本原理并对其加以优化，是现代Java程序员的一项必备技能。</p><p>接下来的几节课，我们就来学习各种具体的GC算法。这节课我会先介绍GC算法中的基本概念，在这个基础上，我再带你深入了解一类重要的GC算法：引用计数法。通过这节课的学习，你将掌握垃圾回收这个重要话题相关的基本概念，了解GC算法的简单分类和算法评价标准。</p>","video_width":0,"column_could_sub":true,"video_id":"","sku":"100094901","video_cover":"","author_name":"海纳","column_is_onboard":true,"audio_url":"https://res001.geekbang.org/media/audio/46/df/46cff6fe64baf01f6fb3217b9529a0df/ld/ld.m3u8","chapter_id":"2404","column_had_sub":true,"column_cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg","neighbors":{"left":{"article_title":"18 | Java内存模型：Java中的volatile有什么用？","id":464954},"right":{"article_title":"20 | Scavenge：基于copy的垃圾回收算法","id":467174}},"rate_percent":0,"footer_cover_data":{"img_url":"https://static001.geekbang.org/resource/image/f0/fe/f0c47daf3262a8df6cf352d89896abfe.png","link_url":"https://time.geekbang.org/article/427012","mp_url":""},"float_app_qrcode":"","column_is_experience":false,"rate":{"1":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"2":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"3":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0}},"product_id":100094901,"had_liked":false,"id":465516,"free_get":false,"is_video_preview":false,"article_summary":"对于C++/C程序员来说，内存错误是一个非常头疼的问题，常见的错误有内存泄露、悬空指针等。","column_sale_type":0,"float_qrcode_jump":"https://time.geekbang.org/serv/v4/misc/jump?uri=https%3A%2F%2Ftime.geekbang.org%2Fhybrid%2Fmp%2Fjump%3Furl%3Dhttps%253A%252F%252Fstatic001.geekbang.org%252Fresource%252Fimage%252Fee%252Fb2%252Feedd4090668aabfac91b2b445d27cbb2.png","column_id":450,"article_sharetitle":"19 | 垃圾回收：如何避免内存泄露？","article_poster_wxlite":"https://static001.geekbang.org/render/screen/f8/e1/f8c83b1d6c1c4fb5cca1c87fd70621e1.jpeg","article_features":0,"comment_count":4,"audio_md5":"46cff6fe64baf01f6fb3217b9529a0df","offline":{"size":16725197,"file_name":"d6c82023c4b0bce783c1a81bf08763a1","download_url":"https://static-acl-001.geekbang.org/resource/zip/article/465516/d6c82023c4b0bce783c1a81bf08763a1.zip?auth_key=1641482349-3e0069931d8d4f97a83ba73300a8374a-0-ac6500ae069022cca155531213c54015"},"video_size":0,"hls_videos":[],"video_time":"","article_could_preview":true,"article_ctime":1639065600,"share":{"content":"学好内存，掌握系统开发诀窍","title":"海纳 · 编程高手必学的内存知识","poster":"https://static001.geekbang.org/resource/image/bc/c5/bcaf3ff69e538f8749970edf2301bbc5.jpg","cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg"}},"467174":{"text_read_version":0,"audio_size":19213038,"article_cover":"https://static001.geekbang.org/resource/image/86/73/8695b38a2da9f8327a95a4fb154ab573.jpg","subtitles":[],"product_type":"c1","audio_dubber":"海纳","is_finished":false,"like":{"had_done":false,"count":1},"audio_time":"00:20:00","video_height":0,"article_content":"<p>你好，我是海纳。</p><p>上一节课中，我们讲到GC算法大致可以分为两大类：引用计数法和基于可达性分析的算法。在基于可达性分析的GC算法中，最基础、最重要的一类算法是基于copy的GC算法（后面简称copy算法）。</p><p>Copy算法是最简单实用的一种模型，也是我们学习GC算法的基础。而且，它被广泛地使用在各类语言虚拟机中，例如JVM中的Scavenge算法就是以它为基础的改良版本。所以，掌握copy算法对我们后面的学习是至关重要的。</p><p>这一节课，我们就从copy算法的基本原理开始讲起，再逐步拓展到GC算法的具体实现。这些知识将帮助你对JVM中Scavenge的实现有深入的理解，并且让你正确地掌握Scavenge GC算法的参数调优。</p><h2>最简单的copy算法</h2><p>基于copy的GC算法最早是在1963年，由Marvin Minsky提出来的。这个算法的基本思想是把某个空间里的活跃对象复制到其他空间，把原来的空间全部清空，这就相当于是把活跃的对象从一个空间搬到新的空间。因为这种复制具有方向性，所以我们把原空间称为From空间，把新的目标空间称为To空间。</p><p>分配新的对象都是在From空间中，所以From空间也被称为<strong>分配空间（Allocation Space）</strong>，而To空间则相应地被称为<strong>幸存者空间（Survivor Sapce）</strong>。在JVM代码中，这两套命名方式都会出现，所以搞清楚这点比较有好处。我们这节课为了强调拷贝进行的方向，选择使用From空间和To空间来分别指代两个空间，而尽量不使用分配空间和幸存者空间的说法。</p><!-- [[[read_end]]] --><p>最基础的copy算法，就是把程序运行的堆分成大小相同的两半，一半称为From空间，一半称为To空间。当创建新的对象时，都是在From空间里进行内存的分配。等From空间满了以后，垃圾回收器就会把活跃对象复制到To空间，把原来的From空间全部清空。然后再把这两个空间交换，也就是说To空间变成下一轮的From空间，现在的From空间变成To空间。具体过程如图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/34/f0/348990cc1ce583a35ce07966a15cfff0.jpg?wh=2284x131\" alt=\"\" title=\"图1\"></p><p>可以看到，上图中的from空间已经满了，这时候，如果想再创建一个新的对象是无法满足的。此时就会执行GC算法将活跃对象都拷贝到新的空间中去。</p><p>假设A对象作为根对象是存活的，而A引用了B和D，所以B和D是活跃的；D又引用了F，所以F也是活跃的。这时候，要是已经没有任何地方引用C对象和E对象，那么C和E就是垃圾了。当GC算法开始执行以后，就会把A、B、D、F都拷贝到To空间中去。拷贝完成后，From空间就清空了，并且From空间与To空间相互交换。</p><p>此时，top指针指向了新的From空间，并且是可用内存的开始处。如果需要再分配新的对象的话，就会从top指针开始进行内存分配。</p><p>我们知道，GC算法包括<strong>对象分配和回收</strong>两个方面。下面，我们分别从这两个方面对copy算法加以考察。先来看copy算法的内存分配是怎么做的。</p><h2>对象分配</h2><p>从上面的介绍里我们知道，在From空间里，所有的对象都是从头向后紧密排列的，也就是说对象与对象之间是没有空隙的。而所有的可用内存全部在From空间的尾部，也就是上图中top指针所指向的位置之后。</p><p>那么，当我们需要在堆里创建一个新的对象时，就非常容易了，只需要将top指针向后移动即可。top指针始终指向最后分配的对象的末尾。每当新分配一个新对象时，只需要移动一次指针即可，这种分配效率非常高。</p><p>如果按这种方式进行新对象的创建，那么对象与对象之间可以保证没有任何空隙，因为后一个对象是顶着前一个对象分配的，所以，这种方式也叫做<strong>碰撞指针（Bump-pointer）</strong>。</p><p>了解了copy算法的内存分配过程后，我们再来看回收的过程。</p><h2>Java对象的内存布局</h2><p>在进行垃圾回收之前，我们首先要识别出哪些对象是垃圾对象。上一节课我们讲过，如果一个对象永远不会再被使用到，那么我们就可以认为这个对象就是垃圾。识别一个对象是否是垃圾的主要方法有两种：引用计数和基于可达性的方法。上一节课我们讲了引用计数，这节课，我们主要来看基于可达性分析，或者称为追踪（Tracing）的方法。</p><p>要想识别一个对象是不是垃圾，<strong>Tracing首先需要找到“根”引用集合</strong>。所谓根引用指的是不在堆中，但指向堆中的引用。根引用包括了<strong>栈上的引用、全局变量、JVM中的Handler、synchronized对象等</strong>。它的基本思想是把对象之间的引用关系构成一张图，这样我们就可以从根出发，开始对图进行遍历。能够遍历到的对象，是存在被引用的情况的对象，就是活跃对象；不能被访问到的，就是垃圾对象。</p><p>那怎么把对象之间的引用关系抽象成图呢？这就涉及到了Java对象的内存布局，我们先来看一下Java对象在内存中是什么样子的。</p><p>在JVM中，一个对象由对象头和对象体构成。其中，<strong>对象头（Mark Word）<strong>在不同运行条件下会有不同的含义，例如对象锁标识、对象年龄、偏向锁指向的线程、对象是否可以被回收等等。而对象体则包含了这个对象的</strong>字段（field），包括值字段和引用字段</strong>。</p><p><img src=\"https://static001.geekbang.org/resource/image/e7/eb/e7aa5ea1e3e037442f34047cfe23a1eb.jpg?wh=2284x1004\" alt=\"\" title=\"图2\"></p><p>每一个Java对象都有一个字段记录该对象的类型。我们把描述Java类型的结构称为Klass。Klass中记录了该类型的每一个字段是值类型，还是对象类型。因此，我们可以根据对象所关联的Klass来快速知道，对象体的哪个位置存放的是值字段还是引用字段。</p><p>如果是引用字段，并且该引用不是NULL，那么就说明当前对象引用了其他对象。这样从根引用出发，就可以构建出一张图了。进一步地，我们通过图的遍历算法，就可以找到所有被引用的活对象。很显然，没有遍历到的对象就是垃圾。</p><p>通常来说，对图进行遍历有两种算法，分别是<strong>深度优先遍历（Depth First Search， DFS）<strong>和</strong>广度优先遍历（Breadth First Search，BFS）</strong>。接下来，我们一起看看这两种算法是如何完成图的遍历的。</p><h2>深度优先搜索算法的实现</h2><p>复制GC算法，最核心的就是如何实现复制。根据上面的描述，我们自己就可以很容易地写出一个基于深度优先搜索的算法，它的伪代码如下所示：</p><pre><code>void copy_gc() {\n    for (obj in roots) {\n        *obj = copy(obj);\n    }          \n}\nobj * copy(obj) {\n    new_obj = to_space.allocate(obj.size);\n    copy_data(new_obj, obj, size);\n    for (child in obj) {\n        *child = copy(child);\n    }\n    return new_obj;\n}\n</code></pre><p>可以看到，算法的开始是从roots的遍历开始的，然后对每一个roots中的对象都执行copy方法（第2~ 4行）。copy方法会在To空间中申请一块新的地址（第7行），然后将对象拷贝到To空间（第8行），再对这个对象所引用到的对象进行递归的拷贝（第9~11行），最后返回新空间的地址（第12行）。</p><p>在<a href=\"https://time.geekbang.org/column/article/433530\">第4节课</a>我们讲解栈的递归特性时，曾经对深度优先搜索的递归写法做过深入分析。拿上面的代码和第4课中的代码进行比较，我们会发现，上面的代码中缺少了对重复访问对象的判断。</p><p>考虑到有两个对象A和B，它们都引用了对象C，而且它们都是活跃对象，现在我们对这个图进行深度优先遍历。</p><p><img src=\"https://static001.geekbang.org/resource/image/a9/d2/a9f00f4cb5489137f33b45523980ebd2.jpg?wh=2284x788\" alt=\"\" title=\"图3\"></p><p>在遍历过程中，A先拷到to space，然后C又拷过去，这时候，空间里的引用是这种状态：</p><p><img src=\"https://static001.geekbang.org/resource/image/75/26/75fba6c4e5cfa6aa856040e4af431b26.jpg?wh=2284x745\" alt=\"\" title=\"图4\"></p><p>A和C都拷到新的空间里了，原来的引用关系还是正确的。但我们的算法在拷贝B对象的时候，先完成B的拷贝，然后你就会发现，此时我们还会把C再拷贝一次。这样，在To空间里就会有两个C对象了，这显然是错的。我们必须要想办法解决这个问题。</p><p>通常来说，在一般的深度优先搜索算法中，我们只需要为每个结点增加一个标志位visited，以表示这个结点是否被访问过。但这只能解决重复访问的问题，还有一件事情我们没有做到：新空间中B对象对C对象的引用没有修改。这是因为我们在对B进行拷贝的时候，并不知道它所引用的对象在新空间中的地址。</p><p>解决这个问题的办法是<strong>使用forwarding指针。也就是说每个对象的头部引入一个新的域（field），叫做forwarding</strong>。正常状态下，它的值是NULL，如果一个对象被拷贝到新的空间里以后，就把它的新地址设到旧空间对象的forwarding指针里。</p><p>当我们访问完B以后，对于它所引用的C，我们并不能知道C是否被拷贝，更不知道它被拷贝到哪里去了。此时，我们就可以在C上留下一个地址，告诉后来的人，这个地址已经变化了，你要找的对象已经搬到新地方了，请沿着这个新地址去寻找目标对象。这就是forwarding指针的作用。下面的图展示了上面描述的过程：</p><p><img src=\"https://static001.geekbang.org/resource/image/91/87/910cf356edd7e10352f5f69a55f5b487.jpg?wh=2284x1095\" alt=\"\" title=\"图5\"></p><p>如果你还不太明白，我再给你举一个形象点儿的例子：你拿到一张画，上面写着武穆遗书在临安皇宫花园里。等你去花园里找到一个盒子，却发现里面的武穆遗书已经不在了，里面留了另一幅画，告诉你它在铁掌峰第二指节。显然，有人移动过武穆遗书，并把新的地址告诉你了，等你第二次访问，到达临安的时候，根据新的地址就能找到真正的武穆遗书了。</p><p>到这里，我们就可以将copy gc的算法彻底完成了，完整的算法伪代码如下所示：</p><pre><code>void copy_gc() {\n    for (obj in roots) {\n        *obj = copy(obj);\n    }          \n}\nobj * copy(obj) {\n    if (!obj.visited) {\n        new_obj = to_space.allocate(obj.size);\n        copy_data(new_obj, obj, size);\n        obj.visited = true;\n        obj.forwarding = new_obj;\n        for (child in obj) {\n            *child = copy(child);\n        }\n    }\n    return obj.forwarding;\n}\n</code></pre><p>这样一来，我们就借助深度优先搜索算法完成了一次图的遍历。</p><p>我们说，除了深度优先搜索外，广度优先搜索也可以实现图的遍历。那这两种算法有什么区别呢？我们进一步来看。</p><h2>深度优先对比广度优先</h2><p>我们已经详细描述了基于深度优先的copy算法，为了对比它与广度优先的copy算法，我们使用一个例子来进行说明。我把这个例子所使用到的对象定义列在下面：</p><pre><code>class A {\n    public B b1;\n    public B b2;\n    public A() {\n        b1 = new B();\n        b2 = new B();\n    }\n}\nclass B {\n    public C c1;\n    public C c2;\n    public B() {\n        c1 = new C();\n        c2 = new C();\n    }\n}\n\nclass C {\n}\n</code></pre><p>假设，在From空间中对象的内存布局如下所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/4c/a5/4c38d5ee000558967ebb6f468fb264a5.jpg?wh=2284x961\" alt=\"\" title=\"图6\"></p><p>请你注意，图中的空白部分是我为了让图更容易查看而故意加的，真实的情况是每个对象之间的空白是不存在的，它们是紧紧地挨在一起的。</p><p>接下来，我们从A对象开始深度优化遍历，那么第一个被拷贝进To空间的就是A对象，如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/7b/99/7bb29da48b54fed4f3c3d898a2876799.jpg?wh=2284x1173\" alt=\"\" title=\"图7\"></p><p>然后对A进行扩展，先访问它属性b1所引用的对象，把b1所指向的对象拷贝到To空间，这一步完成以后，空间中对象的情况如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/9c/6f/9cdd56a62d722f1faf98047dec53yy6f.jpg?wh=2284x1047\" alt=\"\" title=\"图8\"></p><p><strong>接下来的这一步，是一个关键步骤，由于我们的算法是深度优先遍历，所以接下来会拷贝c1所引用的C对象，而不b2所引用的B对象。</strong>因为C的对象不包含指向其他对象的引用，所以，搜索算法拷贝完C对象以后就开始退栈。算法退到上一栈以后，就会继续搜索B对象中，c2所引用的那个C对象。经过这两步操作以后，堆空间的情况如下所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/bf/4f/bffe76b45c298229bec1247b8d1c7e4f.jpg?wh=2284x1198\" alt=\"\" title=\"图9\"></p><p>这样b1所引用的B对象就搜索完了，算法会继续退栈，继续搜索b2所引用的B对象，当b2所引用的对象也全部搜索完成以后，再把To空间和From空间对调。我们就完成了一次Copy GC的全部过程。算法完成以后的堆空间的情况如下所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/c2/df/c2c80164f1e718c60585441a5b7b28df.jpg?wh=2284x972\" alt=\"\" title=\"图10\"></p><p>从上面的图片中可以观察到一个特点：<strong>To空间中的对象排列顺序和From空间中的对象排列顺序发生了变化</strong>。从图5和图9的箭头的样子可以看得出来，图5中的箭头是比较混乱的，而图9中的箭头则简约很多。因为箭头代表了引用关系，这就说明具有引用关系的对象在新空间中距离更近了。</p><p>我们在<a href=\"https://time.geekbang.org/column/article/460545\">第14节课</a>介绍过，因为CPU有缓存机制，所以在读取某个对象的时候，有很大概率会把它后面的对象也一起读进来。通常情况下，我们在写Java代码时，经常访问一个变量后，马上就会访问它的属性。如果在读A对象的时候，把它后面的B和C对象都能加载进缓存，那么，a.b1.c1这种写法就可以立即命中缓存。</p><p>这是深度优先搜索的copy算法的最大的优点，同时，从代码里也能分析出它的缺点，那就是采用递归的写法，效率比较差。如果你熟悉数据结构的话，应该都知道，深度优先遍历也有非递归实现，它需要额外的辅助数据结构，也就是说需要手工维护一个栈结构。非递归的写法，可以使用以下伪代码表示：</p><pre><code>void copy_gc() {\n\tfor (obj in roots) {\n\t\tstack.push(obj);\n\t}\n\twhile (!stack.empty()) {\n\t\tobj = stack.pop();\n\t\t*obj = copy(obj);\n\t\tfor (child in obj) {\n\t\t\tstack.push(child);\n\t\t}\n\t}\n}\n</code></pre><p>与深度优先搜索相对应的是广度优先搜索。它的优缺点刚好与深度优先搜索相反。如果使用广度优先算法将对象从From空间拷贝到To空间，那么有引用关系的对象之间的距离就会比较远，这将不利于业务线程运行期的缓存命中。它的优点则在于<strong>可以节省GC算法执行过程中的空间，提升拷贝过程的效率</strong>。这部分内容请你作为练习，自己推导一遍广度优先搜索以后的堆空间，你就能掌握的更好了。</p><p>广度优先算法节省空间的原理是：<strong>使用scanned指针将非递归的广度优先遍历所需的队列，巧妙地隐藏在了To空间中</strong>。我使用伪代码写出来，你就能理解了：</p><pre><code>void copy_gc() {\n\tfor (obj in roots) {\n\t\t*obj = copy(obj);\n\t}\n\twhile (to.scanned &lt; to.top) {\n\t\tfor (child in obj(scanned)) {\n\t\t\t*child = copy(child)\n\t\t}\n\t\tto.scanned += obj(scanned).size();\n\t}\n}\n</code></pre><p>其中，obj（scanned）代表把scanned所指向的对象强制转换为一个obj。</p><p>综上所述，<strong>深度优先搜索的非递归写法需要占用额外的空间，但有利于提高业务线程运行期的缓存命中率</strong>。<strong>而广度优先搜索则与其相反，它不利于运行期的缓存命中，但算法的执行效率更高</strong>。所以JDK6以前的JVM使用了广度优先的非递归遍历，而在JDK8以后，已经把广度优先算法改为深度优先了，尽管这样做需要额外引用一个独立的栈。</p><p>到这里，基于copy算法的原理，我们就全部讲完了。总体来看，基于copy的算法要将堆空间分成两部分：<strong>一部分是From空间，一部分是To空间。不管什么时刻，总有一半空间是空闲的</strong>。所以，它总体的空间利用率并不高。为了提升空间的利用率，Hotspot对copy算法进行了改造，并把它称为Scavenge算法。那它是怎么实现的呢？</p><h2>Scavenge算法</h2><p>我们知道，每次回收中能存活下来的对象占总体的比例都比较小。那么，我们就可以结合这个特点，把To空间设置得小一点，来提升空间的利用率。</p><p>Hotspot在实现copy算法时做了一些改进。它将From空间称为Eden空间，To空间在算法实现中则被分成S0和S1两部分，这样To空间的浪费就可以减少了。Java堆的空间关系如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/e2/9b/e25aa4090d108890bf27b6cca1d4709b.jpg?wh=2284x890\" alt=\"\" title=\"图11\"></p><p>在这张图里，Hotspot的内存管理器在Eden空间中分配新的对象，每次GC时，如果将S0做为To空间，则S1与Eden合起来成为From空间。也就是说To空间这个空闲区域就大大减小了，这样可以提升空间的总体利用率。</p><p>Scavenge算法是简单copy算法的一种改进。在这种算法中，人们习惯于把S0和S1称为<strong>幸存者空间（Survivor Space）</strong>。配置Survivor空间的大小是JVM GC中的重要参数，例如：-XX:SurvivorRatio=8，代表Eden:S0:S1=8:1:1。</p><p>讲到这，我们清楚地了解了Scavenge算法的原理和来龙去脉。由此我们也容易推知，基于Copy的GC算法有以下特点：</p><ol>\n<li>对象之间紧密排列，中间没有空隙，也就是没有内存碎片；</li>\n<li>分配内存的效率非常高。因为每次分配对象都是把指针简单后移即可，操作非常少，所以效率高；</li>\n<li>回收的效率取决于存活对象的多少，如果存活对象比较多，那么回收的效率就差，如果存活的对象少，则回收效率高。如果对象的生命周期比较短，也就是说存活的时候比较短，那么在进行GC的时候，存活的对象就会比较少，这种情况下采用基于copy的GC算法是比较高效的；</li>\n<li>内存利用率并不高。因为在任一时刻总有一部分空间是无非被使用的，Scavenge算法也只能缓解这个问题，而不能彻底解决，这是由算法的设计所决定的；</li>\n<li>copy算法需要搬移对象，所以需要业务线程暂停。</li>\n</ol><h2>总结</h2><p>这节课我们重点学习了基于copy的GC算法的基本原理和它的具体实现。</p><p>因为copy算法每一次都会搬移对象，在搬移的过程中就已经完成了内存的整理，所以对象与对象之间是没有空隙的，也就是说没有内存碎片。这同时也让内存分配的实现非常简单：<strong>我们只需要记录一个头部指针，有分配空间的需求就把头部指针向后移就可以了</strong>。因为后一个对象是顶着前一个对象分配的，所以，这种方式也叫做<strong>碰撞指针</strong>。</p><p>接下来，我们重点研究了Java对象的内存布局，从这里我们知道，Java对象并不只包含用户定义的字段，还包括了对象头和Klass指针。其中Klass用于描述Java对象的类型，它还记录了Java对象的布局信息，来指示对象中哪一部分是值，哪一部分是引用。</p><p>然后就是我们这节课的重点了。使用深度优先搜索算法对活跃对象进行遍历，在遍历的同时就把活跃对象复制到To空间中去了。活跃对象有可能被重复访问，所以人们使用forwarding指针来解决这个问题。<strong>图的遍历分为深度优先搜索和广度优先搜索</strong>，我们对两种做法都加以讲解，并对比了它们的特点：</p><ul>\n<li>深度优先搜索的递归写法实现简单，但效率差，非递归写法需要额外的辅助数据结构，但它能使业务线程运行时有更好的空间局部性，有利于提高缓存命中率。</li>\n<li>广度优先搜索的实现可以借助To空间做为辅助队列，节约空间。但不利于业务线程的缓存命中率。</li>\n</ul><p>最后，我们展示了Hotspot中的真实做法，也就是Scavenge算法，并总结了copy算法的五个特点：<strong>没有内存碎片、分配效率高、回收效率取决于存活对象比例、总的内存利用率不高和需要暂停</strong>。</p><h2>思考题</h2><p>我们提到，copy算法需要暂停业务线程，那么假设业务线程很多，我们应该怎么样通知业务线程停下来呢？提示：参考加餐二中提到的信号机制和第5节课所讲的协程切换时机。欢迎在留言区分享你的想法，我在留言区等你。</p><p><img src=\"https://static001.geekbang.org/resource/image/8e/4d/8ea7e1b041d031a92275be3b8257a14d.jpg?wh=2284x1386\" alt=\"\"><br>\n好啦，这节课到这就结束啦。欢迎你把这节课分享给更多对计算机内存感兴趣的朋友。我是海纳，我们下节课再见！</p>","float_qrcode":"https://static001.geekbang.org/resource/image/83/1c/83e244537b714e899fd2e7896e22371c.png","article_cover_hidden":false,"is_required":true,"score":"51639324800","like_count":0,"article_subtitle":"","audio_download_url":"https://static001.geekbang.org/resource/audio/4c/c3/4cccdbe8f86b3a5760a9696204e5d9c3.mp3","had_viewed":false,"article_title":"20 | Scavenge：基于copy的垃圾回收算法","column_bgcolor":"#F6F7FB","offline_package":"{}","audio_title":"12.11 海纳 20_copy_gc_L.mp3","audio_time_arr":{"m":"20","s":"00","h":"00"},"text_read_percent":0,"cid":450,"article_cshort":"<p>你好，我是海纳。</p><p>上一节课中，我们讲到GC算法大致可以分为两大类：引用计数法和基于可达性分析的算法。在基于可达性分析的GC算法中，最基础、最重要的一类算法是基于copy的GC算法（后面简称copy算法）。</p><p>Copy算法是最简单实用的一种模型，也是我们学习GC算法的基础。而且，它被广泛地使用在各类语言虚拟机中，例如JVM中的Scavenge算法就是以它为基础的改良版本。所以，掌握copy算法对我们后面的学习是至关重要的。</p><p>这一节课，我们就从copy算法的基本原理开始讲起，再逐步拓展到GC算法的具体实现。这些知识将帮助你对JVM中Scavenge的实现有深入的理解，并且让你正确地掌握Scavenge GC算法的参数调优。</p><h2>最简单的copy算法</h2><p>基于copy的GC算法最早是在1963年，由Marvin Minsky提出来的。这个算法的基本思想是把某个空间里的活跃对象复制到其他空间，把原来的空间全部清空，这就相当于是把活跃的对象从一个空间搬到新的空间。因为这种复制具有方向性，所以我们把原空间称为From空间，把新的目标空间称为To空间。</p><p>分配新的对象都是在From空间中，所以From空间也被称为<strong>分配空间（Allocation Space）</strong>，而To空间则相应地被称为<strong>幸存者空间（Survivor Sapce）</strong>。在JVM代码中，这两套命名方式都会出现，所以搞清楚这点比较有好处。我们这节课为了强调拷贝进行的方向，选择使用From空间和To空间来分别指代两个空间，而尽量不使用分配空间和幸存者空间的说法。</p>","video_width":0,"column_could_sub":true,"video_id":"","sku":"100094901","video_cover":"","author_name":"海纳","column_is_onboard":true,"audio_url":"https://res001.geekbang.org/media/audio/4c/c3/4cccdbe8f86b3a5760a9696204e5d9c3/ld/ld.m3u8","chapter_id":"2404","column_had_sub":true,"column_cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg","neighbors":{"left":{"article_title":"19 | 垃圾回收：如何避免内存泄露？","id":465516},"right":{"article_title":"21 | 分代算法：基于生命周期的内存管理","id":468157}},"rate_percent":0,"footer_cover_data":{"img_url":"https://static001.geekbang.org/resource/image/f0/fe/f0c47daf3262a8df6cf352d89896abfe.png","link_url":"https://time.geekbang.org/article/427012","mp_url":""},"float_app_qrcode":"","column_is_experience":false,"rate":{"1":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"2":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"3":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0}},"product_id":100094901,"had_liked":false,"id":467174,"free_get":false,"is_video_preview":false,"article_summary":"基于copy的算法要将堆空间分成两部分：一部分是From空间，一部分是To空间。不管什么时刻，总有一半空间是空闲的。","column_sale_type":0,"float_qrcode_jump":"https://time.geekbang.org/serv/v4/misc/jump?uri=https%3A%2F%2Ftime.geekbang.org%2Fhybrid%2Fmp%2Fjump%3Furl%3Dhttps%253A%252F%252Fstatic001.geekbang.org%252Fresource%252Fimage%252Fee%252Fb2%252Feedd4090668aabfac91b2b445d27cbb2.png","column_id":450,"article_sharetitle":"20 | Scavenge：基于copy的垃圾回收算法","article_poster_wxlite":"https://static001.geekbang.org/render/screen/bd/aa/bdfafc8272aa6ba0dc52b32a81b950aa.jpeg","article_features":0,"comment_count":5,"audio_md5":"4cccdbe8f86b3a5760a9696204e5d9c3","offline":{"size":19179155,"file_name":"93c6eca3aee4727d0f57c3000aa5b7d1","download_url":"https://static-acl-001.geekbang.org/resource/zip/article/467174/93c6eca3aee4727d0f57c3000aa5b7d1.zip?auth_key=1641482365-1b84f4776b1e4575b04968844efcf47f-0-b32d1688768ae3375171930761cf7317"},"video_size":0,"hls_videos":[],"video_time":"","article_could_preview":false,"article_ctime":1639324800,"share":{"content":"学好内存，掌握系统开发诀窍","title":"海纳 · 编程高手必学的内存知识","poster":"https://static001.geekbang.org/resource/image/bc/c5/bcaf3ff69e538f8749970edf2301bbc5.jpg","cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg"}},"468157":{"text_read_version":0,"audio_size":22607403,"article_cover":"https://static001.geekbang.org/resource/image/ca/36/cab793435e74bc69c57835eed9f28936.jpg","subtitles":[],"product_type":"c1","audio_dubber":"海纳","is_finished":false,"like":{"had_done":false,"count":0},"audio_time":"00:23:32","video_height":0,"article_content":"<p>你好，我是海纳。</p><p>上节课，我们讲过了可达性分析中基于copy的垃圾回收算法，它比较适合管理短生命周期对象。那什么算法适合管理长生命周期对象呢？它就是可达性分析的GC算法中的另一大类：<strong>Mark-Sweep算法</strong>。</p><p>为了发挥两种算法的优点，GC的开发者就基于对象的生命周期引入了分代垃圾回收算法，它将堆空间划分为年轻代和老年代。其中年轻代使用Copy GC来管理，老年代则使用Mark-Sweep来管理。</p><p>所以这节课我们将先介绍传统Mark-Sweep算法的特点，在此基础上再引入分代垃圾回收。年轻代算法的原理你可以去上节课看看，这节课我们就重点介绍老年代的管理算法，并通过Hotspot中的具体实现来进行举例。</p><p>通过这节课，你将从根本上把握住分代垃圾回收和老年代管理工作原理，在此基础上，你才能理解分代垃圾回收中比较晦涩的几个参数，从而可以对GC调优做到知其然且知其所以然。另外，CMS算法是你以后掌握G1 GC和Pauseless GC的基础，尽管CMS在现实场景中应用得越来越少，但它的基本原理却仍然是学习GC的重要步骤。</p><p>好啦，不啰嗦了，我们先从朴素的Mark-Sweep算法开始讲起。</p><h2>Mark-Sweep算法</h2><p>简单来讲，Mark-Sweep算法由<strong>Mark和Sweep两个阶段组成</strong>。在Mark阶段，垃圾回收器遍历活跃对象，将它们标记为存活。在Sweep阶段，回收器遍历整个堆，然后将未被标记的区域回收。</p><!-- [[[read_end]]] --><p>当传统的Mark-Sweep算法在分配新的对象时，它的做法与<a href=\"https://time.geekbang.org/column/article/440452\">第9节课</a>所讲的malloc分配算法是一样的，但在具体实现上还是有一些细微的差别。</p><p>Mark-Sweep算法和malloc相似的地方是，都是用一个链表将所有的空闲空间维护起来，这个链表就是<strong>空闲链表（freelist）</strong>。当内存管理器需要申请内存空间时，便向这个链表查询，如果找到了合适大小的空闲块，就把空闲块分配出去，同时将它从空闲链表中移除。如果空闲块比较大，就把空闲块进行分割，一部分用于分配，剩余的部分重新加到空闲链表中。</p><p>Hotspot虚拟机的Mark-Sweep算法与malloc实现的不同之处在于：在Hotspot里，当一个空闲块分配给一个新对象后，如果剩余空间大于24字节，便将剩余的空间加入到空闲链表，当剩余空间不足24字节的话，就做为碎片空间填充一些无效值。而malloc则会分成多个空闲链表进行管理，更具体的实现可以参考<a href=\"https://time.geekbang.org/column/article/440452\">第9节课</a>。</p><p>Hotspot中的Mark-Sweep的算法运行过程示意图如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/18/2b/18ca325ee2db6bdedd94485e2e1c782b.jpg?wh=2284x1188\" alt=\"\"></p><p>图中的A、C、D、F是空间中的活跃对象，B、 E、G三块区域是空闲区域，假设B的大小是20，E是36，G是60。它们由空闲链表统一管理。</p><p>假设现在有两个分配请求，都要分配大小为30的空间，按照上面内容中的算法描述，具体的分配过程如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/3b/8f/3b6ecfda2971d6462541259909194d8f.jpg?wh=2284x1113\" alt=\"\"></p><p>E的大小为36，可以满足分配，但是E所剩下的区域已经比较小了，分配器不再将它加回到空闲链表，E和F之间就产生了一块内存碎片。第二个请求，则会在G区域进行分配，在分配完以后，G区域剩下的空闲区域还比较大，所以分配器会把分割后的空闲区域，也就是H区域再挂回空闲链表。上图中显示了分配完以后，堆里的最终结果。</p><p>介绍完空闲链表结构以后，我们来学习算法的原理。Mark-Sweep算法主要包含Mark和Sweep两个阶段。按照先后顺序，我们先从第一阶段，也就是Mark阶段开始讲起。</p><p>Mark阶段的核心任务是<strong>从根引用出发，根据引用关系进行遍历，所有可以遍历到的对象就是活跃对象</strong>。我们需要一边遍历，一边将哪些对象是活跃的记录下来。当遍历完成以后，我们就找到了所有的活跃对象。遍历的方法可以采用深度优先和广度优先两种策略。这一点和上一节课所讲的对象遍历的方法是一样的。</p><p>那如何对活跃对象进行标记呢？一般来说，常见的方法有两种。<strong>一种是在每个对象前面增加一个机器字，采用其中的某一位作为“标记位”</strong>。如果该位置位，就表示这个对象是活跃对象；如果该位未置位，那么表示这个对象是要回收的对象。</p><p><strong>另一种方法是采用标记位图，将每一个机器字映射成位图中的一个比特</strong>。在真正的实现中，往往会采用两个位图，其中一个标记活跃对象的起始位置，另一个标记活跃对象的结束位置。</p><p>Sweep阶段要做的事情就是把非活跃对象，也就是垃圾对象的空间回收起来，重新将它们放回空闲链表中。具体做法就是按照空间顺序，从头至尾扫描整个空间里的所有对象，如果一个对象没有被标记，这个对象所占用内存就会被添加回空闲链表。这个阶段的操作是比较简单的，只涉及了链表的添加操作，而且它和malloc的回收过程是一致的，所以我就不再啰嗦了。</p><p>Mark-Sweep算法回收的是垃圾对象，如果垃圾对象比较少，回收阶段所做的事情就比较少。所以它适合于存活对象多，垃圾对象少的情况。而我们上节课讲的基于copy的垃圾回收算法Scavenge，搬移的是活跃对象，所以它更适合存活对象少，垃圾对象多的情况。既然如此，那我们能不能把这两种算法的优点结合起来，用不同的算法管理不同类型的对象呢？</p><h2>分代垃圾回收算法</h2><p>自然是可以的，对于存活时间比较短的对象，我们可以用Scavenge算法回收；对于存活时间比较长的对象，就可以使用Mark-Sweep算法。这就是分代垃圾回收算法产生的动机。</p><p>我们知道，Java中的函数局部对象和临时对象也会在堆里进行分配，这就导致Java中的对象的生命周期都不长。所以，使用Scavenge对这些对象进行管理是合适的。可以说，<strong>Scavenge所管理的空间是“年轻代”</strong>。</p><p>那怎么区分那些存活时间长的对象呢？我们可以在对象的头部记录一个名为age的变量，对象头部就是<a href=\"https://time.geekbang.org/column/article/465516\">第19节课</a>所介绍的Mark word。age变量不需要占据整个Mark word空间，只需要其中的几个比特就可以了。Scavenge GC每做一次，就是把存活的对象往Survivor空间中复制一次，我们就相应把这个对象的age加一，以此来代表它的生命周期比较长。</p><p>当对象的age值到达一个上限以后，就可以将它搬移到由Mark-Sweep算法所管理的空间了。与“年轻代”相对应，我们称这个空间为“老年代”。而对象从年轻代到老年代的搬移过程，就称为<strong>晋升(Promotion)。</strong></p><h3>记录集：维护跨代引用</h3><p>可以想象，把对象放到两个空间，肯定会有跨空间的对象引用。如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/d6/94/d66a5bcd75599a42cc85221f8de1a794.jpg?wh=2284x934\" alt=\"\"></p><p>上图中左边代表年轻代在GC之前，在幸存者空间S0中一共有6个对象，其中4个对象是活跃对象，两个白色框代表非活跃对象。假如此时发生年轻代GC，垃圾回收器就会把A、B、D、F四个对象尝试向另外一个幸存者空间，也就是S1进行搬移。</p><p>在搬移的过程还会将对象的age加一，如果刚好A对象和F对象的age大于晋升阈值，那么这两个对象就会被搬到老年代中。如上图中的右侧所示，在年轻代的幸存者空间中，对象都是紧密排列的，所以B和D会靠在一起。而在老年代空间，由于对象是从空闲区域中分配的，所以A和F不一定是靠在一起的。</p><p>显然，这就会带来一个问题，在以后年轻代GC执行时，我们就要考虑从老年代到年轻代的引用了，也就是图中，A指向B和D对象的引用。反过来说，当老年代GC执行时，也同样要考虑从年轻代到老年代的引用，也就是图中的D指向F的引用。</p><p>在进行年轻代垃圾回收时，为了找出从老年代到年轻代的引用，可以考虑对老年代对象进行遍历。但如果这么做的话，年轻代GC执行时，就会对全部对象进行遍历，分代就没意义了。</p><p>为了解决这个问题，我们可以想办法把这种跨代引用记录下来，<strong>记录跨代引用的集合就是记录集（Remember Set，RS）。</strong></p><p>最直观的思路是，记录集里记录被引用到的新生代对象，如下图中的左侧所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/86/0d/8611a2d63be51e24b6d421544a9a3b0d.jpg?wh=2284x1179\" alt=\"\"></p><p>记录集中记下了被跨代引用的B和D两个对象，那么当发生年轻代GC时，就能正确地找到B和D对象是根对象。但是这样做的问题是，当B和D再经过一轮年轻代GC，位置发生变化以后，A对象对它们的引用却无法正确维护。</p><p>为了解决这个问题，我们可以再进一步将A放到记录集中，也就是上图中右侧所示，记录集中指向了A对象。这样，在做年轻代GC时，我们对A进行遍历，就可以访问到B和D了。因为B和D在年轻代中，它们的位置会发生变化，变化的地址也能正确地更新到对象A中。A在老年代，所以它不会被搬移，我们只是借用它去访问B和D。</p><p>搞清楚了记录集的作用，我来接着看如何维护记录集呢？</p><h3>写屏障：维护记录集</h3><p>维护记录集的手段，在<a href=\"https://time.geekbang.org/column/article/465516\">第19节课</a>我们已经接触过了，那就是写屏障（write barrier）。在<a href=\"https://time.geekbang.org/column/article/465516\">第19节课</a>，我们在对象的引用发生变化的时候，来维护对象的引用计数。在分代式垃圾回收算法中，我们可以使用同样的思路来维护跨代引用。</p><p>当对象的属性进行写操作时，跨代引用就有可能出现。所以，我们在这个时候可以检查是否存在跨代引用。写屏障的伪代码如下所示：</p><pre><code>void do_oop_store(Value obj, Value* field, Value value) {\n    if (in_young_space(value) &amp;&amp; in_old_space(obj) &amp;&amp;\n        !rs.contains(obj)) {\n      rs.add(obj);\n    }\n    *field = &amp;value;\n}\n</code></pre><p>上面代码中，obj代表引用者对象；value代表被引用的对象；field代表obj对象中被修改的那个域，它是一个指针，代表了地址，这意味着我们要修改地址处存放的值，而不是修改指针本身。</p><p>在进行对象的域赋值时，我们要先做以下三个判断（第2行）：</p><ol>\n<li><strong>被引用的对象是否在年轻代</strong>；</li>\n<li><strong>发出引用的对象，也就是引用者，是否在老年代；以上两点都满足，就说明产生了跨代引用</strong>；</li>\n<li><strong>检查记录集中是否已经包含了obj</strong>。</li>\n</ol><p>如果以上三点都满足，就将obj添加到记录集中（第4行）。</p><p><strong>还有一种情况可能产生跨代引用，那就是晋升</strong>。假设A对象引用B对象，它们都在年轻代里，经过一次年轻代GC，A对象晋升到老年代，那这也会产生跨代引用，所以在晋升的时候，我们也要对这种情况加以处理。晋升的完整伪代码，如下所示：</p><pre><code>void promote(obj) {\n  new_obj = allocate_and_copy_in_old(obj);\n  obj.forwarding = new_obj;\n  \n  for (oop in oop_map(new_obj)) {\n    if (in_young_space(oop)) {\n      rs.add(new_obj);\n      return;\n    }\n  }\n}\n</code></pre><p>在上面的代码里，我们先在老年代中分配一块空间，把待晋升对象复制到老年代。然后把新地址new_obj记录到原来对象的forwarding指针。接着遍历从这个对象出发的所有引用，如果这个对象有引用年轻代对象，就把这个晋升后的对象添加到记录集中。</p><p>在上面所讲的写屏障实现里，一个对象写操作中要进行三个判断，如果条件成立，还要再执行一次记录集的添加对象操作，效率是比较差的。为了提升效率，又有人提出了<strong>Card table这种实现方式来提升写屏障的效率</strong>。</p><h3>Card table：提升写屏障效率</h3><p>随着对象的增多，记录集会变得很大，而且每次对老年代做GC，正确地维护记录集也是一件复杂的事情。另外，写屏障的效率也不高，为了解决这个问题，可以借鉴位图的思路，这就是Card table。</p><p>Hotspot的分代垃圾回收将老年代空间的512bytes映射为一个byte，当该byte置位，则代表这512bytes的堆空间中包含指向年轻代的引用；如果未置位，就代表不包含。这个byte其实就和位图中标记位的概念很相似，人们称它为card。多个card组成的表就是Card table。一个card对应512字节，压缩比达到五百分之一，Card table的空间消耗还是可以接受的。</p><p><img src=\"https://static001.geekbang.org/resource/image/68/5e/6885cac6d70539140c63a5a514f6c05e.jpg?wh=2284x1217\" alt=\"\"></p><p>如上图所示，图的下方就是Card table，因为A对象包含指向年轻代的引用，所以A对象所对应的card就被置位，而F对象不存在指向年轻代的引用，所以它所对应的card就未被置位。</p><p>在明白了分代式垃圾回收是如何维护跨代引用的以后，我们就可以转向研究Hotspot中分代式垃圾回收的典型算法：<strong>并发标记清除算法（Concurrent Mark Sweep，CMS）</strong>。标记清除的概念我们已经介绍过了，并发又是什么意思呢？接下来，我们一起来研究一下。</p><h2>并发标记算法</h2><p>在并发标记算法中，当垃圾回收器在标记活跃对象的时候，我们肯定不希望业务线程同时还会修改对象之间的引用关系。所以，早期的Mark-Sweep算法会让业务线程都停下来，以便于垃圾回收器可以对活跃对象进行标记，这就是GC停顿产生的原因。由于业务线程停顿的时候，整个Java进程都不能再响应请求，<strong>人们把这种情况形象地称为“世界停止”（Stop The World，STW）</strong>。</p><p>为了减少GC停顿，我们可以在做GC Mark的时候，让业务线程不要停下来。这意味着GC Mark和业务线程在同时工作，这就是<strong>并发</strong>（Concurrent）的GC算法。</p><p>这里你要注意区别并发（Concurrent）和并行（Parallel）的区别。并发GC是指GC线程和业务线程同时工作，并行是指多个GC线程同时工作。</p><p>Hotspot中在实现Mark-Sweep算法的时候，采用了并发的方式，这就是并发标记清除，简称为CMS。它的特点是GC线程在标记存活对象的过程中，业务线程是不必停下来的。直觉上就是一边对图进行遍历，一边修改图中的边，这样肯定会产生问题。接下来，我们就详细地分析一下这么做到底会有什么问题。</p><p>为了方便描述，我们引入三种颜色来辅助算法的讲解。我们知道，图的遍历过程，就是不断地对结点进行搜索和扩展的过程。以标记算法来说，如果采用广度优先遍历，那么搜索就是对结点进行标记，扩展就是将结点所引用的其他对象都添加到辅助队列中。根据这个定义，我们可以将结点分为三类：</p><ul>\n<li><strong>白色：还未搜索的对象；</strong></li>\n<li><strong>灰色：已经搜索，但尚未扩展的对象；</strong></li>\n<li><strong>黑色：已经搜索，也完成扩展的对象。</strong></li>\n</ul><p>这里要注意，三色标记并不意味着对象真的要为自己增加一个color属性，它只是一种抽象的概念，在不同的GC算法中，它代表不同的状态。</p><p>我们以广度优先搜索的标记算法为例，白色对象就是未被标记的对象；灰色则是已经被标记，但还没有完成扩展的对象，也就是还在队列中的对象；黑色则是已经扩展完的对象，即从队列中出队的对象。</p><p>你要注意的是，并发标记中最严重的问题就是<strong>漏标</strong>。如果一个对象是活跃对象，但它没有被标记，这就是漏标。这就会出现活跃对象被回收的情况。例如下图中所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/9f/68/9fcc7712d1429e4f41cf89866433ef68.jpg?wh=2284x1020\" alt=\"\"></p><p>在上图中的最左边，标号（a）的子图中，一切都还是正常的，B尚未扩展，在B扩展的时候，C自然可以被标记。在（b）中，A出发的引用指向了C，这时由于B指向C的引用还存在，仍然没有什么问题。但在（c）中，B指向C的引用消失了，因为A已经变成黑色，不会再被扩展了，所以C就没有机会再被标记了。这就产生了漏标。</p><p>总的来说，黑色对象引用了白色对象，而白色对象又没有其他机会再被访问到，所以白色对象就被漏标了。</p><p>漏标问题的解决方案主要有三种，我们这节课会介绍两种，第三种我们会在下一节课再详细介绍。第一种解决方案是<strong>往前进</strong>的方案，如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/0c/de/0c8ac88d114ef4b944161859f69310de.jpg?wh=2284x1012\" alt=\"\"></p><p>解决漏标问题，还是要从写屏障入手。荷兰计算机科学家Dijkstra提出一种写屏障：<strong>当黑色对象引用白色对象时，把白色对象直接标灰，也就是说将C对象直接标记，然后放入队列中待扩展</strong>。这个过程的伪代码我建议你自己写一下，作为练习。</p><p>往前走的方案是比较符合直觉的，但是假如在C对象被标记以后，A对C的引用又消失了，C实际上是被多标了。多标并不会带来严重的后果，它只会导致原本应该被回收的对象没有被及时回收，这种对象被称为<strong>浮动垃圾</strong>。</p><p>为了解决浮动垃圾这个问题，我们可以使用第二种方案，那就是<strong>往后退一步</strong>：把黑色对象重新扩展一次，也就是说黑色结点变成灰色。如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/13/7f/130e8301018f99a1fc1808e47147677f.jpg?wh=2284x1087\" alt=\"\"></p><p>如果对象修改比较频繁，那么后退一步的方案会比前进一步的方案更好，因为它不会产生浮动垃圾。Lua虚拟机中就采用了方案二，这是因为Lua中的Table结构的修改是比较频繁的。</p><p>在Hotspot中，CMS的实现和上面的两种思路有关系，但又不完全一样，接下来，我们全面地分析一下在Hotspot中，CMS是如何实现的。</p><h2>Hotspot中CMS的实现</h2><p>在Hotspot中，CMS是由多个阶段组成的，主要包括初始标记、并发标记、重标记、并发清除，以及最终清理等。其中：</p><ul>\n<li><strong>初始标记阶段</strong>，标记老年代中的根对象，因为根对象中包含从栈上出发的引用等比较敏感的数据，并发控制难以实现，所以这一阶段一般都采用Stop The World的做法。这里一般不遍历年轻代对象，也就是不关注从年轻代指向老年代的引用。</li>\n<li><strong>并发标记阶段</strong>，这一阶段就是在上面内容中讲到的三色标记算法中做了一些改动，我们会在后面的内容中详细分析这一阶段的实现。</li>\n<li><strong>重标记阶段</strong>，这一阶段会把年轻代对象也进行一次遍历，找出年轻代对老年代的引用，并且为并发标记阶段扫尾。</li>\n<li><strong>并发清除阶段</strong>，这一阶段会把垃圾对象归还给freelist，只要注意好freelist的并发访问，实现垃圾回收线程和业务线程并发执行是简单的。</li>\n<li><strong>最终清理阶段</strong>，清理垃圾回收所使用的资源。为下一次GC执行做准备。</li>\n</ul><p>CMS中最复杂的还是并发标记阶段。如果在并发标记的过程中，业务线程修改了对象之间的引用关系，CMS采用的办法是：<strong>在write barrier中，只要一个对象A引用了另外一个对象B，不管A和B是什么颜色的，都将A所对应的card置位。</strong></p><p>由于不必检查颜色，这个置位的过程就非常快，你可以自己思考一下，card置位的具体实现。当一轮标记完成以后，如果还有置位的card，那么垃圾回收器就会开启新一轮并发标记。新的一轮标记，会从上一轮置位的card所对应的对象开始进行遍历，遍历完成后再把card全部清零，所以这样的一轮并发标记也被称为<strong>预清理（preclean）</strong>。</p><p>如果恰好在某一轮并发标记的过程中，业务线程不再修改对象之间的引用关系了，那么就不会再产生card置位的情况了。这时就可以结束并发标记阶段了。</p><p>但是如果每一轮都有card置位，应该怎么办呢？CMS也会在预清理达到一定次数以后停止，进入重标记阶段。重标记的作用是遍历新生代和card置位的对象，对老年代对象做一次重新标记。这一次是需要停顿的，因为这一次垃圾回收器将不允许card再被置位了。</p><p>一般来说，新生代指向老年代的引用不会太多，但是偶尔也会发生这种引用很多的情况，如果出现了这种情况，可以在重标记之前，进行一次年轻代GC，这样可以减少年轻代中的对象数量，减少重标记的停顿时间。这个功能可以使用参数-XX:+CMSScavengeBeforeReMark来打开。</p><p>这个过程的示意图如下所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/2c/09/2cc6a95f48e5c54b0c2f3685da57d109.jpg?wh=2284x1026\" alt=\"\"></p><p>因为card table有两个功能：维护跨代引用和标记灰色结点，所以，Hotspot又引入了另外一个数据结构mod union table（MUT）来用于维护跨代引用。在并发标记开始之前，card table中的内容就会被复制到MUT里，如果在Concurrent Mark阶段，发生了年轻代的垃圾回收，则可以使用MUT来进行跨代扫描。</p><p>上图中展示了，A对象在引用B对象时，A对象所对应的card被置位。因为D对象所对应的card和A对象所对应的card是同一个card，所以，GC在清理card的过程中仍然会把D对象进行标记。所以D就是一个被多标记了的垃圾对象，也就是浮动垃圾。</p><p>到这时，Hotspot中的具体实现我们就搞清楚了。</p><h2>总结</h2><p>好啦，这节课到这里就结束啦，我们一起来回顾一下这节课的重点内容吧。这节课，我们先介绍了最基本的<strong>Mark-Sweep算法</strong>，它使用空闲链表来组织空闲空间，它的分配过程与<a href=\"https://time.geekbang.org/column/article/440452\">第9节课</a>介绍的malloc的方法很相似。在标记阶段，垃圾回收器通过图遍历算法来标记对象，在活跃对象被标记以后，不活跃的区域会被集中地归还到空闲链表中。</p><p><strong>Mark-Sweep算法最大的特点是不移动活跃对象，只回收不活跃的空间</strong>。所以它更适合管理生命周期比较长的对象。它的特点与上节课所讲的基于copy的算法刚好互补，所以人们就把这两者结合起来，使用copy算法管理短生命周期对象，也就是<strong>年轻代</strong>；使用Mark-Sweep算法管理长生命周期对象，也就是<strong>老年代</strong>。</p><p>在分代式垃圾回收算法里，最大的挑战是如何维护跨代引用，以加速单独的某个代的垃圾回收的过程。我们介绍了<strong>记录集</strong>和<strong>Card table</strong>两种方式，并且介绍了如何使用write barrier对它们进行维护。</p><p>在分代垃圾回收算法里，<strong>用于管理老年代的是CMS算法</strong>。这种算法的主要挑战来自于垃圾回收线程在标记的过程中，业务线程还在不断地修改对象之间的引用关系。我们介绍了三色标记算法，并且介绍了两种解决漏标问题的手段：<strong>“往前走”和“往后退一步”</strong>。</p><h2>思考题</h2><p>我们已经知道标记的过程就是图遍历的过程，那你觉得在Mark-Sweep算法中，应该采用广度优先还是深度优先进行遍历呢？为什么呢？考虑到对象不必搬移，你还能不能想到更省空间的做法？欢迎在留言区分享你的想法，我在留言区等你。</p><p><img src=\"https://static001.geekbang.org/resource/image/b9/64/b9d7165fba7871ef37af60999de84564.jpg?wh=2284x1551\" alt=\"\"></p><p>好啦，这节课到这就结束啦。欢迎你把这节课分享给更多对计算机内存感兴趣的朋友。我是海纳，我们下节课再见！</p>","float_qrcode":"https://static001.geekbang.org/resource/image/83/1c/83e244537b714e899fd2e7896e22371c.png","article_cover_hidden":false,"is_required":true,"score":"51639497600","like_count":1,"article_subtitle":"","audio_download_url":"https://static001.geekbang.org/resource/audio/62/64/62e70b6122a5bba813326f324d0cbf64.mp3","had_viewed":false,"article_title":"21 | 分代算法：基于生命周期的内存管理","column_bgcolor":"#F6F7FB","offline_package":"{}","audio_title":"12.14海纳 21_cms.MP3_L.mp3","audio_time_arr":{"m":"23","s":"32","h":"00"},"text_read_percent":0,"cid":450,"article_cshort":"<p>你好，我是海纳。</p><p>上节课，我们讲过了可达性分析中基于copy的垃圾回收算法，它比较适合管理短生命周期对象。那什么算法适合管理长生命周期对象呢？它就是可达性分析的GC算法中的另一大类：<strong>Mark-Sweep算法</strong>。</p><p>为了发挥两种算法的优点，GC的开发者就基于对象的生命周期引入了分代垃圾回收算法，它将堆空间划分为年轻代和老年代。其中年轻代使用Copy GC来管理，老年代则使用Mark-Sweep来管理。</p><p>所以这节课我们将先介绍传统Mark-Sweep算法的特点，在此基础上再引入分代垃圾回收。年轻代算法的原理你可以去上节课看看，这节课我们就重点介绍老年代的管理算法，并通过Hotspot中的具体实现来进行举例。</p><p>通过这节课，你将从根本上把握住分代垃圾回收和老年代管理工作原理，在此基础上，你才能理解分代垃圾回收中比较晦涩的几个参数，从而可以对GC调优做到知其然且知其所以然。另外，CMS算法是你以后掌握G1 GC和Pauseless GC的基础，尽管CMS在现实场景中应用得越来越少，但它的基本原理却仍然是学习GC的重要步骤。</p><p>好啦，不啰嗦了，我们先从朴素的Mark-Sweep算法开始讲起。</p><h2>Mark-Sweep算法</h2><p>简单来讲，Mark-Sweep算法由<strong>Mark和Sweep两个阶段组成</strong>。在Mark阶段，垃圾回收器遍历活跃对象，将它们标记为存活。在Sweep阶段，回收器遍历整个堆，然后将未被标记的区域回收。</p>","video_width":0,"column_could_sub":true,"video_id":"","sku":"100094901","video_cover":"","author_name":"海纳","column_is_onboard":true,"audio_url":"https://res001.geekbang.org/media/audio/62/64/62e70b6122a5bba813326f324d0cbf64/ld/ld.m3u8","chapter_id":"2404","column_had_sub":true,"column_cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg","neighbors":{"left":{"article_title":"20 | Scavenge：基于copy的垃圾回收算法","id":467174},"right":{"article_title":"22 | G1 GC：分区回收算法说的是什么？","id":468883}},"rate_percent":0,"footer_cover_data":{"img_url":"https://static001.geekbang.org/resource/image/f0/fe/f0c47daf3262a8df6cf352d89896abfe.png","link_url":"https://time.geekbang.org/article/427012","mp_url":""},"float_app_qrcode":"","column_is_experience":false,"rate":{"1":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"2":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"3":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0}},"product_id":100094901,"had_liked":false,"id":468157,"free_get":false,"is_video_preview":false,"article_summary":"由于业务线程停顿的时候，整个Java进程都不能再响应请求，人们把这种情况形象地称为“世界停止”（Stop The World，STW）。","column_sale_type":0,"float_qrcode_jump":"https://time.geekbang.org/serv/v4/misc/jump?uri=https%3A%2F%2Ftime.geekbang.org%2Fhybrid%2Fmp%2Fjump%3Furl%3Dhttps%253A%252F%252Fstatic001.geekbang.org%252Fresource%252Fimage%252Fee%252Fb2%252Feedd4090668aabfac91b2b445d27cbb2.png","column_id":450,"article_sharetitle":"21 | 分代算法：基于生命周期的内存管理","article_poster_wxlite":"https://static001.geekbang.org/render/screen/60/21/60b013233728434c608d9475d742c021.jpeg","article_features":0,"comment_count":4,"audio_md5":"62e70b6122a5bba813326f324d0cbf64","offline":{"size":23112243,"file_name":"9a571181ff1951d8ac0f872c4676a407","download_url":"https://static-acl-001.geekbang.org/resource/zip/article/468157/9a571181ff1951d8ac0f872c4676a407.zip?auth_key=1641482381-be13042a07da4ee8ab3698e0863c8777-0-7cd314f206ea682e7ef555e4f4994667"},"video_size":0,"hls_videos":[],"video_time":"","article_could_preview":false,"article_ctime":1639497600,"share":{"content":"学好内存，掌握系统开发诀窍","title":"海纳 · 编程高手必学的内存知识","poster":"https://static001.geekbang.org/resource/image/bc/c5/bcaf3ff69e538f8749970edf2301bbc5.jpg","cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg"}},"468883":{"text_read_version":0,"audio_size":19916511,"article_cover":"https://static001.geekbang.org/resource/image/2e/b3/2eda4152d735ec26239a57ff732304b3.jpg","subtitles":[],"product_type":"c1","audio_dubber":"海纳","is_finished":false,"like":{"had_done":false,"count":2},"audio_time":"00:20:44","video_height":0,"article_content":"<p>你好，我是海纳。</p><p>在上一节课，我们介绍了分代式垃圾回收算法。把对象分代以后，可以大大减轻垃圾回收的压力，进而就减少了停顿时长。在这种思路的启发下，人们进一步想，如果把对象分到更多的空间中，根据内存使用的情况，每一次只选择其中一部分空间进行回收不就好了吗？根据这个思路，GC开发者设计了<strong>分区回收算法</strong>。</p><p>它在实际场景中应用非常广泛，比如说Hotspot中的G1 GC就是分区回收算法的一种具体实现，Android上的art虚拟机也采用了分区回收算法。而且从JDK9开始，G1 GC就是JDK的默认垃圾回收算法了，所以在将来很长时间里，对G1 GC进行合理的调优，将是Java程序员要重点掌握的知识。</p><p>那么这节课，我们就来深入地讲解分区回收算法的基本原理，掌握G1 GC的若干重要参数，从而对G1 GC进行合理的参数调优。</p><p>要想理解分区垃圾回收的原理，还得从它的结构讲起。</p><h2>分区算法的堆结构</h2><p>首先，我们来了解一下分区回收算法的堆空间是如何划分的。下图是G1 GC的堆结构：</p><p><img src=\"https://static001.geekbang.org/resource/image/aa/bf/aaf7f24a480ca0d292ef267f2bcacdbf.jpg?wh=2284x1161\" alt=\"\"></p><p>G1也是一个分代的垃圾回收算法，不过，和之前介绍的CMS、Scavenge算法不同的是：<strong>G1的老年代和年轻代不再是一块连续的空间，整个堆被划分成若干个大小相同的Region，也就是区</strong>。Region的类型有<strong>Eden、Survivor、Old、Humongous</strong>四种，而且每个Region都可以单独进行管理。</p><!-- [[[read_end]]] --><p>Humongous是用来存放大对象的，如果一个对象的大小大于一个Region的50%（默认值），那么我们就认为这个对象是一个大对象。为了防止大对象的频繁拷贝，我们可以将大对象直接放到Humongous中。</p><p>而Eden、Survivor、Old三种区域和我们前面课程中介绍的Eden分区、Survivor分区以及老年代的作用是类似的。也就是说，对象会在Eden Regions中分配，当进行年轻代GC时，会将活跃对象拷贝到Survivor Regions；当对象年龄超过晋升阈值时，就把活跃对象复制进Old Regions。如果你不清楚，可以看看<a href=\"https://time.geekbang.org/column/article/465516\">第19节课</a>和<a href=\"https://time.geekbang.org/column/article/467174\">第20节课</a>，这里我就不再啰嗦了。</p><p>在了解了G1的堆空间划分之后，我们就可以开始学习G1算法的回收原理了。实际上，分区垃圾回收算法最大的特点是<strong>维护跨分区引用</strong>，这也是它实现起来最难的地方。下面我们就以此为切入点，来探寻G1算法的原理。</p><h2>写屏障</h2><p>维护跨分区引用，其中的关键就是写屏障。我们在上节课讲CMS时提到，写屏障是对象在修改引用关系时，额外做一些操作来维护相关信息。在CMS中，写屏障主要有两个作用：</p><ol>\n<li><strong>在并发标记阶段解决活跃对象漏标问题；</strong></li>\n<li><strong>在写屏障里使用card table维护跨代引用。</strong></li>\n</ol><p>我们先来看第一个作用，也就是解决活跃对象漏标的问题。上一节课介绍了解决漏标问题的两种方法，分别是<strong>“往前走”和“往后退一步”</strong>。今天这节课，我们就来介绍第三种解法，这个解法呢，是由日本学者汤浅太一提出的，具体的算法如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/9f/19/9ff38194b9663fa85f9b5148d6179e19.jpg?wh=2284x1171\" alt=\"\"></p><p>在这张图中，我们可以看到，当对象B对C的引用关系消失以后，再将C标记为灰色，即便将来A对C的引用消失了，也会在当前GC周期内被视为活跃对象。也就是说，C有可能变成浮动垃圾。我们把这种在删除引用的时候进行维护的屏障叫做<strong>deletion barrier</strong>。</p><p>G1中采用的就是这种做法。这种做法的特点是，在GC标记开始的一瞬间，活跃的对象无论在标记期间发生怎样的变化，都会被认为是活跃的对象。</p><p>我们知道，当一个对象的全部引用被删除时，才会被当做垃圾。而如果使用我们前面讲到的deletion barrier，在并发标记阶段，即便对象的全部引用被删除，也会被当做活跃对象来处理。就好像在GC开始的瞬间，内存管理器为所有活跃对象做了一个快照一样，所以人们给了这种技术一个很形象的名字：<strong>开始时快照（Snapshot At The Beginning，SATB）</strong>。</p><p>你要注意的是，有些文章对SATB的解释是：在GC开始时将堆做一个内存快照，存放到磁盘上。这种说法就是望文生义了。因为快照这个词在计算机领域通常是指压缩，索引等技术，所以就有人把这里的快照理解成了对堆对象的一种压缩。由此，我们就知道这种错误的说法是怎么来的了。</p><p>言归正传，我们在理解SATB的含义之后，再来看看SATB具体的工作原理吧。</p><p>我们在讲写屏障时提到，当B对象对C对象的引用消失时，C对象将会被标记为灰色。这个动作的效率是比较低的，如果都放在写屏障中做，会极大地影响程序性能。<strong>因为写屏障的逻辑是由业务线程执行的。</strong></p><p>为了解决这个问题，GC开发者将“C对象标记为灰色”这件事情往后推迟了。业务线程只需要把C对象记录到一个本地队列中就可以了。每个业务线程都有一个这样的线程本地队列，它的名字是<strong>SATB队列</strong>。</p><p>当业务线程发现对象C的引用被删除之后，直接将C放到SATB队列中，并不去做标记，真正做标记的工作交给GC线程去做，这样就减少了写屏障的开销。</p><p><img src=\"https://static001.geekbang.org/resource/image/c6/cf/c68666025933acfa0c621a01b7bf68cf.jpg?wh=2284x1243\" alt=\"\"></p><p>如上图所示，每个线程有自己的本地SATB队列，当本地队列满了之后，就把它交给SATB队列集合，然后再领取一个空队列当做线程的本地SATB队列。GC线程则会将SATB队列集合中的对象标记为灰色，至于什么时候标记，并不需要业务线程关心。</p><p>在学习了SATB相关知识后，我们继续来定义G1的两种垃圾回收模式，以方便后面详细地介绍算法的执行过程。</p><h2>垃圾回收模式</h2><p>G1的垃圾回收模式有两种：分别是<strong>young GC和mixed GC</strong>。</p><ul>\n<li>young GC：只回收年轻代的Region。</li>\n<li>mixed GC：回收全部的年轻代Region，并回收部分老年代的Region。</li>\n</ul><p>我要告诉你的是，无论是young GC还是mixed GC，都会回收全部的年轻代，mixed回收的老年代Region是需要进行决策的（Humongous在回收时也是当做老年代的Region处理的）。那么决定老年代Region是否被回收的因素具体有哪些呢？</p><p>我们把mixed GC中选取的老年代对象Region的集合称之为<strong>回收集合（Collection Set，CSet）</strong>。CSet的选取要素有以下两点：</p><ol>\n<li>该Region的垃圾占比。垃圾占比越高的Region，被放入CSet的优先级就越高，这就是<strong>垃圾优先策略（Garbage First），也是G1 GC名称的由来</strong>。</li>\n<li>建议的暂停时间。建议的暂停时间由-XX:MaxGCPauseMillis指定，G1会根据这个值来选择合适数量的老年代Region。</li>\n</ol><p>MaxGCPauseMillis 默认是200ms，一般不需要进行调整，如果需要停顿时间更短可以对它进行设置，不过需要注意的是，MaxGCPauseMillis设置的越小，选取的老年代Region就会越少，如果GC压力居高不下，就会触发G1的Full GC。</p><p>触发G1的Full GC代价是很高的。最早的实现是一个单线程的Mark-Compact GC，停顿时间非常长，虽然后来也改进成多线程，但还是需要尽量避免触发G1的Full GC。如果一个应用会频繁触发G1 GC的Full GC，那么说明这个应用的GC参数配置是不合理的，理想情况下G1是没有Full GC的。在这节课的最后，我会介绍几个常用的G1参数，方便你在实践中对G1进行调参。</p><p>在学习了G1的垃圾回收模式之后，我们需要解决的问题还有不少，首先就是跨区引用的问题。</p><h2>维护跨区引用</h2><p>在上面的内容中，我们提到了写屏障的两个功能，第二个功能就是<strong>维护跨区引用</strong>。在<a href=\"https://time.geekbang.org/column/article/468157\">第21节课</a>中，我们已经学习了CMS的跨代引用，实际上，CMS的跨代引用和G1的跨区引用的原理是相同的。不同的是，CMS的跨代引用它的回收空间是固定的，例如young GC只回收年轻代，Concurrent Mark Sweep只回收老年代，这样只需要维护一张卡表就可以了。</p><p>但是像G1这种分区回收算法，有些Region可能被选入CSet，有些则不会。所以，我们需要知道当一个Region需要被回收时，有哪些其他的Region引用了自己。相应地，为了加快定位速度，分区回收算法为每个Region都引入了<strong>记录集（Remembered Set，RSet）</strong>，每个Region都有自己的专属RSet。</p><p>和Card table 不同的是，RSet记录谁引用了我，这种记录集被人们称为<strong>point-in型</strong>的，而Card table则记录我引用了谁，这种记录集被称为<strong>point-out型</strong>。</p><p><img src=\"https://static001.geekbang.org/resource/image/e1/89/e128f9d4b811a9dbbc0ddb1a4b5de789.jpg?wh=2284x976\" alt=\"\"></p><p>如图所示，图中的左侧展示了一个维护跨区引用的通用记录集，而右侧则展示了只对应于一个Region的专属记录集。</p><p>接下来我们继续分析RSet的维护策略，也就是说哪些引用关系需要加入到RSet：</p><ol>\n<li>如果是同一个Region的对象，它们之间相互引用是不必维护的，这个很好理解，因为不存在跨Region的问题；</li>\n<li>由年轻代Region出发到其他Region的，无论目标是年轻代还是老年代，这一类引用也都不用维护。因为结合young GC和mixed GC的策略可以知道，无论是什么回收模式，年轻代的全部Region都会被清理，这就意味着一定会对年轻代的所有对象进行遍历；</li>\n<li>从CSet集合的Region出发指向其他Region的，也不需要维护，理由和第2点是一样的。</li>\n</ol><p>总的来说，RSet 需要维护的引用关系只有两种，<strong>非CSet 老年代Region 到年轻代Region的引用，和非CSet 老年代Region到CSet老年代Region的引用。</strong></p><p>那么，RSet具体是何时被记录的呢？答案也是写屏障，写屏障的这个作用，我们在上面的内容中已经提到过。如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/ef/53/efd323267e5f1256e272a9fc54244453.jpg?wh=2284x967\" alt=\"\"></p><p>G1在RSet中记录的也是card。比如Region1中的对象A引用了Region2的对象B，那么对象A所对应的card就会被记录在Region2的RSet中（注意！不是Region1的RSet）。</p><p>在G1中，我们把这种card称为dirty card。和SATB相似，业务线程也不是直接将dirty card放到RSet中的。而是在业务线程中引入一个叫做<strong>dirty card queue（DCQ）</strong>的队列，在写屏障中，业务线程只需要将dirty card放入DCQ中，而不做非常细致的检查。</p><p>接下来，GC线程中，有一类特殊的线程，它们会从DCQ中找到这种dirty card，然后再去做更精细的检查，只有确实不属于上面所描述的三种情况的跨区引用，才真正放到专属RSet中去。这一类特殊的线程就是G1 GC中的<strong>Refine线程</strong>。</p><p>下面我们再来继续剖析RSet存放的形式是怎样的。考虑某个Region的RSet，它可能会因为引用关系比较多，而变得很大。根据另一个Region对这个Region的引用数量，可以分为少、中、多三种情况。针对这三种情况，RSet准备了三种不同的数据结构来应对，分别是<strong>稀疏表、细粒度表和粗粒度表</strong>。三种表之间的关系是不断粗化的，如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/69/yy/69b112a235d36b564ac825a04ca653yy.jpg?wh=2284x985\" alt=\"\"></p><p>从上图中，我们可以看到：</p><ol>\n<li>稀疏表是一个哈希表，当Region A对Region B的引用很少时，就可以将相关的card放到稀疏表里；</li>\n<li>细粒度表则是一个真正的card table，当Region之间的引用比较多时，就可以直接使用位图来代替哈希表，因为这能加快查找的速度（使用位操作代替哈希表的查找）；</li>\n<li>粗粒度表则是一个区的位图，因为相对来说，区是比较少的，所以粗粒度表的大小也很小。当Region A对Region B的引用非常多时，就不用再使用card table来进行管理了，在回收Region B时，直接将Region A的全部对象都遍历一次就可以了。</li>\n</ol><p>总之，随着其他Region对本Region的引用关系越多，RSet存放引用关系使用的表粒度就越粗，这样做主要是为了减少RSet记录数，提高定位效率。</p><p>在解决了跨区引用的问题之后，接下来我们就可以学习 G1 的垃圾清理过程了，这是垃圾回收器真正回收内存的过程，所以它的重要性不言而喻。</p><h2>垃圾回收的过程</h2><p>G1的垃圾清理是通过把活跃的对象，从一个Region拷贝到另一个空白Region，这个空白Region隶属于Survivor空间。这个过程在G1 GC中被命名为<strong>转移（Evacuation）</strong>。它和之前讲到的基于copy的GC的最大区别是：<strong>它可以充分利用concurrent mark的结果快速定位到哪些对象需要被拷贝。</strong></p><p>接下来让我们通过一个例子，来看看G1 Evacuation的具体过程吧。</p><p><img src=\"https://static001.geekbang.org/resource/image/8a/26/8a623b380c0af2110b016e8ea5b86826.jpg?wh=2284x947\" alt=\"\"></p><p>在上图中，Region2是一个待回收的Region，隶属于CSet。在它的专属RSet中记录了Region1 的第二个card和Region3的第一个card，说明Region1和Region3有对Region2的对象引用，Region4 是一个被选为Survivor的空白Region。</p><p>假如Region1和Region3都经过了并发标记，识别出A对象是垃圾对象，而E对象是活跃对象。那么，我们就可以从活跃对象E开始进行遍历。注意，这一次遍历的目标是把Region2中的对象搬移到Region4。</p><p>Region1中的A是垃圾对象，这在并发标记阶段就已经发现了，所以在转移阶段就不会再起作用了。进而，Region2中的B、C也不会被标记到，最终只有对象D被拷贝到了Region4，与此同时，原始Region2的RSet也会被维护到Region4。</p><p>因为Evacuation发生的时机是不确定的，在并发标记阶段也可能发生。所以并发标记要使用一个BitMap来记录活跃对象，而Evacuation也需要使用一个BitMap来将活跃的对象进行搬移。这就产生了读和写的冲突：<strong>并发标记需要写BitMap，而Evacuation需要读BitMap</strong>。</p><p>为了解决这个问题，G1维护了两个BitMap，一个名为nextBitMap，一个名为prevBitMap。其中，<strong>prevBitMap是用于搬移活跃对象</strong>，而<strong>nextBitMap则用于并发标记记录活跃对象</strong>。</p><p>当并发标记开始以后，新的对象仍然有可能会被继续分配。内存管理器把这些对象全部认为是活跃对象。我们来看下面的这个示意图：</p><p><img src=\"https://static001.geekbang.org/resource/image/d5/54/d5f0763d397f4097d9633e24281d5d54.png?wh=2284x2362\" alt=\"\"></p><p>在上图中，TAMS指针，是Top At Mark Start的缩写。初始时，prevTAMS，nextTAMS和top指针都指向一个分区的开始位置。</p><p>随着业务线程的执行，top指针不断向后移动。并发标记开始时（图1），nextTAMS记录下当前的top指针，并且针对nextTAMS之前的对象进行活跃性扫描，扫描的结果就存放在nextBitMap中（图2）。</p><p>当并发标记结束以后，nextTAMS的值就记录在prevTAMS中，并且nextBitMap也赋值给prevBitMap。如果此时发生了Evacuation，则prevBitMap已经可用了。如果没有发生Evacuation，那么nextBitMap就会清空，为下一轮并发标记做准备。这样就可以保证，在任意时刻开启Evacuation的话，prevBitMap总是可用的（图3）。</p><p>在并发标记开始以后，再创建的对象，其实就是nextTAMS指针到top指针之间的对象，这些对象全部认为是活跃的（注意观察图中紫色部分）。</p><p>我们再从对象活跃性的角度理解两个TAMS指针和top的关系。当并发标记开始时，nextTAMS就固定了，但是top还是可能继续向后移，所以nextTAMS和top之间的对象在这次标记过程中都被认为是活跃对象。当Evacuation开始时，它只使用prevBitMap的信息，显然prevBitMap中的信息只能覆盖到prevTAMS处，所以从prevTAMS到top的对象就都认为是活跃的。</p><p>top指针是一个Region内已分配区域和未分配区域的界限。通过TAMS和BitMap，GC线程可以清楚地知道一个Region内活跃对象的分布，不仅可以确定Evacation的范围，还可以用来计算一个Region的垃圾比例，为CSet选择提供参考。</p><p>好啦，关于G1的算法原理，我们就先介绍到这里吧，下面让我们一起看看G1有哪些常用参数吧，因为掌握G1中重要的参数的意义，才能帮助你对G1 GC进行参数调优。</p><h2>G1 常用参数</h2><p>G1的默认参数已经被调整得很好了，大多数情况下，不需要再调整。但是，也不排除特殊情况，因此我们还是需要掌握一些GC参数，具体列表如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/5a/68/5a7b129e54bed0ce79604bafa77a0768.jpg?wh=2284x1620\" alt=\"\"></p><p>这个表格中最重要、也是你平时最有可能用到的参数，就是<strong>MaxGCPauseMillis</strong>。它设置了期望的最大停顿时间。MaxGCPauseMillis设置的越小，可以控制的停顿时间就越短。但是如果设置得太短，可能会引起Full GC，代价十分昂贵。</p><p>其次，比较关键的参数是<strong>InitiatingHeapOccupancyPercent（IHOP）</strong>，它的作用是在老年代的内存空间达到一定百分比之后，启动并发标记。当然，这更进一步是为了触发mixed GC，以此来回收老年代。如果一个应用老年代对象产生速度较快，可以尝试适当调小IHOP。</p><h2>总结</h2><p>好了，今天这节课就到这里来，我们一起来回顾一下这节课的重点内容。这节课，我们首先介绍了G1特点，明确了分区的意义。然后我们讲到了G1的堆空间划分策略，G1的每个分区都可以单独管理，<strong>空闲Region可以用来当做Survivor空间，Homongous区是用来存放大对象的</strong>，在回收过程中，和老年代同等对待。</p><p>然后，我们重点分析了write barrier的两个作用，一个是<strong>维护记录集</strong>，一个是<strong>解决漏标问题</strong>。</p><p>G1的记录集是与Region一一对应的，是一种point-in类型的记录集。它仍然采用dirty card的设计，将dirty card存放在记录集中。记录集为了管理dirty card，区分了三种粒度，分别是<strong>稀疏表，细粒度表和粗粒度表。</strong></p><p>解决漏标问题则是采用了SATB的设计，保证了在GC开始的瞬间活跃的对象就始终是活跃的。</p><p>接下来，我们解释了G1的两种垃圾回收模式，分别是<strong>young GC</strong>和<strong>mixed GC</strong>。young GC只回收年轻代，mixed GC回收全部年轻代和部分老年代。</p><p>G1的垃圾清理过程与普通的copy-based不同。我们说，G1的Evacuation可能发生在并发标记阶段，为了保证Evacuation在并发标记阶段可以知道哪些对象是活的、需要被拷贝，我们介绍了两个BitMap和TAMS指针。这样一来，在Evacuation进行的过程中，管理器就有依据判断一个Region中哪些对象是应该被拷贝的。</p><p>最后，我还给你讲了在实际工作中需要掌握的几个G1参数，尤其是<strong>MaxGCPauseMillis和IHOP</strong>。MaxGCPauseMillis用来设置期望最大停顿时间，IHOP用来调整并发标记的处理时机，调整老年代回收的及时性。</p><h2>思考题</h2><p>请你思考：如何可以进一步减少垃圾回收的最大停顿时间？欢迎在留言区分享你的想法，我在留言区等你。</p><p><img src=\"https://static001.geekbang.org/resource/image/a9/c4/a9c0bf3dfb5c7d3f0c7b0c3936eba9c4.jpg?wh=2284x1386\" alt=\"\"></p><p>好啦，这节课到这就结束啦。欢迎你把这节课分享给更多对计算机内存感兴趣的朋友。我是海纳，我们下节课再见！</p>","float_qrcode":"https://static001.geekbang.org/resource/image/83/1c/83e244537b714e899fd2e7896e22371c.png","article_cover_hidden":false,"is_required":true,"score":"51639670400","like_count":1,"article_subtitle":"","audio_download_url":"https://static001.geekbang.org/resource/audio/3d/b0/3de5c0478d66be4fbaa70499179ab6b0.mp3","had_viewed":false,"article_title":"22 | G1 GC：分区回收算法说的是什么？","column_bgcolor":"#F6F7FB","offline_package":"{}","audio_title":"12.16 海纳 22_g1gc_R.mp3","audio_time_arr":{"m":"20","s":"44","h":"00"},"text_read_percent":0,"cid":450,"article_cshort":"<p>你好，我是海纳。</p><p>在上一节课，我们介绍了分代式垃圾回收算法。把对象分代以后，可以大大减轻垃圾回收的压力，进而就减少了停顿时长。在这种思路的启发下，人们进一步想，如果把对象分到更多的空间中，根据内存使用的情况，每一次只选择其中一部分空间进行回收不就好了吗？根据这个思路，GC开发者设计了<strong>分区回收算法</strong>。</p><p>它在实际场景中应用非常广泛，比如说Hotspot中的G1 GC就是分区回收算法的一种具体实现，Android上的art虚拟机也采用了分区回收算法。而且从JDK9开始，G1 GC就是JDK的默认垃圾回收算法了，所以在将来很长时间里，对G1 GC进行合理的调优，将是Java程序员要重点掌握的知识。</p><p>那么这节课，我们就来深入地讲解分区回收算法的基本原理，掌握G1 GC的若干重要参数，从而对G1 GC进行合理的参数调优。</p><p>要想理解分区垃圾回收的原理，还得从它的结构讲起。</p><h2>分区算法的堆结构</h2><p>首先，我们来了解一下分区回收算法的堆空间是如何划分的。下图是G1 GC的堆结构：</p><p><img src=\"https://static001.geekbang.org/resource/image/aa/bf/aaf7f24a480ca0d292ef267f2bcacdbf.jpg?wh=2284x1161\" alt=\"\"></p><p>G1也是一个分代的垃圾回收算法，不过，和之前介绍的CMS、Scavenge算法不同的是：<strong>G1的老年代和年轻代不再是一块连续的空间，整个堆被划分成若干个大小相同的Region，也就是区</strong>。Region的类型有<strong>Eden、Survivor、Old、Humongous</strong>四种，而且每个Region都可以单独进行管理。</p>","video_width":0,"column_could_sub":true,"video_id":"","sku":"100094901","video_cover":"","author_name":"海纳","column_is_onboard":true,"audio_url":"https://res001.geekbang.org/media/audio/3d/b0/3de5c0478d66be4fbaa70499179ab6b0/ld/ld.m3u8","chapter_id":"2404","column_had_sub":true,"column_cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg","neighbors":{"left":{"article_title":"21 | 分代算法：基于生命周期的内存管理","id":468157},"right":{"article_title":"23 | Pauseless GC：挑战无暂停的垃圾回收","id":469971}},"rate_percent":0,"footer_cover_data":{"img_url":"https://static001.geekbang.org/resource/image/f0/fe/f0c47daf3262a8df6cf352d89896abfe.png","link_url":"https://time.geekbang.org/article/427012","mp_url":""},"float_app_qrcode":"","column_is_experience":false,"rate":{"1":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"2":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"3":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0}},"product_id":100094901,"had_liked":false,"id":468883,"free_get":false,"is_video_preview":false,"article_summary":"上一节课介绍了解决漏标问题的两种方法，分别是“往前走”和“往后退一步”。今天这节课，我们就来介绍第三种解法。","column_sale_type":0,"float_qrcode_jump":"https://time.geekbang.org/serv/v4/misc/jump?uri=https%3A%2F%2Ftime.geekbang.org%2Fhybrid%2Fmp%2Fjump%3Furl%3Dhttps%253A%252F%252Fstatic001.geekbang.org%252Fresource%252Fimage%252Fee%252Fb2%252Feedd4090668aabfac91b2b445d27cbb2.png","column_id":450,"article_sharetitle":"22 | G1 GC：分区回收算法说的是什么？","article_poster_wxlite":"https://static001.geekbang.org/render/screen/09/64/09cf09600843975ee8145e0380803f64.jpeg","article_features":0,"comment_count":6,"audio_md5":"3de5c0478d66be4fbaa70499179ab6b0","offline":{"size":20346233,"file_name":"360c64f1357f9c94c7e2e0271a618a5c","download_url":"https://static-acl-001.geekbang.org/resource/zip/article/468883/360c64f1357f9c94c7e2e0271a618a5c.zip?auth_key=1641482397-70090dde65d54567b46725be02a776ac-0-09924681ce767dd174a1b5f08195eb82"},"video_size":0,"hls_videos":[],"video_time":"","article_could_preview":false,"article_ctime":1639670400,"share":{"content":"学好内存，掌握系统开发诀窍","title":"海纳 · 编程高手必学的内存知识","poster":"https://static001.geekbang.org/resource/image/bc/c5/bcaf3ff69e538f8749970edf2301bbc5.jpg","cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg"}},"469971":{"text_read_version":1,"audio_size":19304465,"article_cover":"https://static001.geekbang.org/resource/image/5a/98/5a90cyy08ec3b9cabc4b3b47e31fbf98.jpg","subtitles":[],"product_type":"c1","audio_dubber":"海纳","is_finished":false,"like":{"had_done":false,"count":1},"audio_time":"00:20:06","video_height":0,"article_content":"<p>你好，我是海纳。</p><p>在前面的几节课程中，我们学习了 CMS 、G1 等垃圾回收算法，这两类GC算法虽然一直在想办法降低GC时延，但它们仍然存在相当可观的停顿时间。</p><p>如何进一步降低GC的停顿时间，是当前垃圾回收算法领域研究的最热点话题之一。今天我们就来学习这类旨在减少GC停顿的垃圾回收算法，也就是<strong>无暂停GC</strong>（Pauseless GC）。由于Hotspot的巨大影响力和普及程度，以及它的代码最容易获得，我们这节课就以ZGC为例来深入讲解无暂停GC。</p><p>而且，ZGC对Java程序员的意义和G1是同样重要的。如果说CMS代表的是过去式，而G1是一种过渡（尽管这个过渡期会很长），那么ZGC无疑就是JVM自动内存管理器的未来。</p><p>通过这节课的学习，你就能了解到无暂停GC的基本思想和可以使用的条件，从而为未来正确地使用无暂停GC做好充分的准备。</p><p>无暂停GC这个词你可能比较陌生，让你觉得这个算法很难，我们不妨先来了解一下它的前世今生，你就能知其然，经过后面对它原理的讲解，你就能知其所以然了。</p><h2>无暂停GC简介</h2><p>JVM的核心开发者Cliff Click供职于Azul Systems公司期间，撰写了一篇很重要的论文，也就是<a href=\"https://www.usenix.org/legacy/events/vee05/full_papers/p46-click.pdf\">Pauseless GC</a>，提出了无暂停GC的想法和架构设计。同时，Azul公司也在他们的JVM产品Zing中实现了一个无暂停GC，将GC的停顿时间大大减少，这就是<a href=\"https://www.azul.com/products/components/pgc\">C4垃圾回收器</a>。</p><!-- [[[read_end]]] --><p>同时，Red hat公司的GC研究小组也开启了一款名为Shenandoah的垃圾回收器，它的工作原理与C4不同，但它在停顿时间这一项上的表现也非常出色。人们把Shenandoah GC也归为无暂停GC。</p><p>时隔多年，Oracle 公司也开发了一款面向低时延的垃圾回收器，它的基本思想和C4垃圾回收器的一致，并且也在openjdk社区开源。</p><p>了解了无暂停GC的历史后，我们再分别从功能原理和代码实现上来讨论无暂停GC。从功能原理上看，<strong>无暂停GC与CMS、Scanvenge等传统算法不同，它的停顿时间不会随着堆大小的增加而线性增加</strong>。以ZGC为例，它的最大停顿时间不超过 10ms ，注意不是平均，也不是随机，而是最大不超过 10ms 。是不是感到很震惊呢？这节课我们就一起揭开ZGC的神秘面纱，探究这极低时延背后的真相。</p><p>从代码实现上看，ZGC 很复杂，包含很多细节，整个GC周期甚至划分了十个不同的阶段。代码阅读起来也相当困难。不过不用担心，这节课重点介绍的不是ZGC的代码实现，而是ZGC 背后的原理，当我们理解它的原理之后，再去探究实现细节，才会事半功倍。我们就先从刚才提到的那个问题，也就是它为什么可以做到最大10ms的停顿时间开始吧。</p><h2>ZGC停顿时间的真相</h2><p>ZGC和G1 有很多相似的地方，它的主体思想也是采用复制活跃对象的方式来回收内存。在回收策略上，它也同样将内存分成若干个区域，回收时也会选择性地先回收部分区域。</p><p>ZGC 与G1的区别在于：<strong>它可以做到并发转移（拷贝）对象</strong>。关于并发转移的概念，这里我还是提醒你一下，并发转移指的是在对象拷贝的过程中，应用线程和 GC 线程可以同时进行，这是其他GC算法目前没有办法做到的。</p><p>前面几节课中我们介绍的垃圾回收算法，在进行对象转移时都是需要 <strong>“世界停止”</strong>（Stop The World，STW）的，而对象转移往往是垃圾回收过程最耗时的一个环节，并且随着堆的增大，这个时间也会跟着增加。ZGC则不同，<strong>在应用线程运行的同时，GC线程也可以进行对象转移，这样就相当于把整个GC最耗时的环节放在应用线程后台默默执行，不需要一个长时间的STW来等待</strong>。这也正是ZGC停顿时间很小的主要原因。</p><p>你可能会问，如何能在应用线程修改对象引用关系的同时，GC线程还能正确地转移对象，或者说GC线程将对象转移的过程中，应用线程是如何访问正在被搬移的对象呢？接下来我就带你了解并发转移的关键技术。</p><h2>并发转移关键技术</h2><p>在此之前，我们首先回顾一下并发标记算法的原理。在并发标记的过程中，应用线程可能会修改对象之间的引用关系，为了保证在对象标记的过程中活跃对象不被漏标，我们引入了<strong>三色标记算法</strong>。虽然三色标记算法会在当前回收周期内产生浮动垃圾，但是不会漏标，而且多标记的垃圾对象也会在下一个回收周期被清理。</p><p>在介绍三色标记算法时，我们还讲到了 write barrier 概念。wirte barrier主要是通过拦截写动作，在对象赋值时加入额外操作。这节课，我们就来讲解一个与write barrier对应的操作，它是无暂停GC算法中普遍采用的一个操作，那就是read barrier ，也就是在对象读取时加入额外操作。</p><h3>read barrier</h3><p>通过前面的学习，我们知道CMS算法和G1算法都使用了write barrier来保证并发标记的完整性，防止漏标现象。ZGC的并发标记也不例外，这个技术我们已经深入讨论过了，这里就不再啰嗦了。除此之外，ZGC提升效率的核心关键在于并发转移阶段使用了read barrier。</p><p>请你试想一下，当应用线程去读一个对象时，GC 线程刚好正在搬移这个对象。<strong>如果GC线程没有搬移完成，那么应用线程可以去读这个对象的旧地址；如果这个对象已经搬移完成，那么可以去读这个对象的新地址。那么判断这个对象是否搬移完成的动作就可以由read barrier来完成</strong>。</p><p><img src=\"https://static001.geekbang.org/resource/image/ed/6a/ed2443f9f2b906d07yyb51c752d8086a.jpg?wh=2284x1375\" alt=\"\"></p><p>上图中，对象a和对象b都引用了对象foo，当foo正在拷贝的过程中，应用线程A可以访问旧的对象foo得到正确的结果，当foo拷贝完成之后，应用线程B就可以通过read barrier来获取对象foo的新地址，然后直接访问对象foo的新地址。</p><p>请你思考一下，如果这里只用 write barrier是否可行？当foo正在拷贝的过程中，应用线程A如果要写这个对象，那么只能在旧的对象foo上写，因为还没有搬移完成；如果当foo拷贝完成之后，应用线程B再去写对象foo，是写到foo的新地址，还是旧地址呢？</p><p>如果写到旧地址，那么对象foo就白搬移了，如果写到新地址，那么又和线程A看到的内容不一样？所以使用write barrier是没有办法解决并发转移过程中，应用线程访问一致性问题，从而无法保证应用线程的正确性。因此，为了实现并发转移，<strong>ZGC使用了read barrier</strong>。</p><p>与此同时，我们还需要关注一个问题，就是在大多数的应用中，读操作要比写操作多一个数量级，所以<strong>read barrier对性能更加敏感</strong>（ZGC最初的设计目标之一是吞吐量不低于G1的15%），这就要求read barrier要非常高效。</p><p>为了达到这个目的，<strong>ZGC采用了用空间换时间的做法</strong>，也就是<strong>染色指针</strong>（colored pointer）技术。通过这个技术，ZGC不仅非常高效地完成了read barrier需要完成的工作，而且可以更高效的利用内存。接下来我们就看看染色指针是怎么一回事吧。</p><h3>染色指针</h3><p>我们知道，在 64 位系统下，当前Linux系统上的地址指针只用到了 48 位，寻址范围也就是 256T。但实际上，当前的应用根本就用不到256T内存，也没有哪台服务器机器上面可以一下插这么多内存条。所以， ZGC 就借用了地址的第 42 ~ 45 位作为标记位，第 0 ~ 41位共 4T 的地址空间留做堆使用。我们结合 JVM 的源码来看看ZGC中对地址具体是怎么标注的。</p><p><img src=\"https://static001.geekbang.org/resource/image/e7/be/e74043f75ec7e31026797cc3656ac1be.jpg?wh=2284x971\" alt=\"\"></p><p>通过上图我们可以看出，第 46 和 47 位是预留的，也就是说标记位可以继续向左移两位，那么可以支持的堆空间就可以扩展到 16T。当前很多资料说 ZGC只支持 4T 内存，实际上现在最新版本已经支持到了 16T，如果你特别感兴趣的话，我建议你可以下载 openJDK 的源码进行查看。</p><p>第 42-45 这 4 位是标记位，它将地址划分为 Marked0、Marked1、Remapped、Finalizable 四个地址视图（由于Finalizable与弱引用的实现有关系，我们这里只讨论前三个）。</p><p>地址视图应该怎么理解呢？其实很简单，对一个对象来说，如果它地址的第 42 位是 1，那么它就被认为是处于 Marked0 视图。依次类推，如果第 43位是1，这个对象就处于Marked1 视图；如果第 44 位是1，该对象就处于Remapped 视图。</p><p>地址视图的巧妙之处就在于，<strong>一个在物理内存上存放的对象，被映射在了三个虚拟地址上</strong>。前面我们学习地址映射的时候知道，一个物理地址可以被映射到多个虚拟地址，这个映射方式在同一个进程内同样适用。例如下面的代码：</p><pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;fcntl.h&gt;\n#include &lt;sys/mman.h&gt;\n#include &lt;unistd.h&gt;\n  \n#define PAGE_SIZE 4096\n\nint main() {\n    int fd = memfd_create(&quot;anonymous&quot;, MFD_CLOEXEC);\n    ftruncate(fd,PAGE_SIZE);\n    char* shm0 = (char*)mmap(NULL, PAGE_SIZE, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);\n    char* shm1 = (char*)mmap(NULL, PAGE_SIZE, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);\n    char* shm2 = (char*)mmap(NULL, PAGE_SIZE, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);\n    sprintf(shm0 ,&quot;hello colored pointer&quot;);\n    printf(&quot;%s\\n&quot;,shm1);\n    printf(&quot;%s\\n&quot;,shm2);\n    sprintf(shm1 ,&quot;wow!&quot;);\n    printf(&quot;%s\\n&quot;,shm0);\n    printf(&quot;%s\\n&quot;,shm2);\n    close(fd);\n    munmap(shm0,PAGE_SIZE);\n    munmap(shm1,PAGE_SIZE);\n    munmap(shm2,PAGE_SIZE);\n    return 0;\n}\n</code></pre><p>使用以下命令，编译并执行这个程序：</p><pre><code>$ gcc -Wall -D_GNU_SOURCE multi_mmap.c -o multi\n$ ./multi\n</code></pre><p>上面的例子先在内存中创建了一个匿名文件（第10行），然后将这个匿名文件映射到shm0，shm1，shm2三个虚拟地址上（第12-14行）。当我们修改shm0时，shm1和shm2的内容也会跟着变化。地址视图也是用了同样的原理，三个地址视图映射的是同一块物理内存，映射地址的差异只在第42-45位上。这样一个对象可以由三个虚拟地址访问，其访问的内容是相同的。</p><p>有了地址视图之后，我们就可以在一个对象转移之后，修改它的地址视图了，同时还可以维护一张映射表（下称forwarding table）。在这个映射表中，key 是旧地址，value 是新地址。当对象再次被访问时，通过插入的read barrier 来判断对象是否被搬移过。如果forwarding table中有这个对象，说明当前访问的对象已经转移，read barrier这时就会将对这个对象的引用直接更改为新地址。</p><p>我还是举一个例子来说明，搬移一个对象以及访问它的引用所需要的步骤，如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/a0/7d/a01b4d1202816b2bc83f595e0yy7867d.jpg?wh=2284x1349\" alt=\"\"></p><p>上图中，当foo对象发生转移之后，对象a再访问foo时就会触发read barrier。read barrier会查找forwarding table来确定对象是否发生了转移，确定foo被转移到新地址foo（new）之后，直接将这一次对foo的访问更改为foo（new）。由于整个过程是依托于read barrier自动完成的，这个过程也叫“自愈”。在介绍了 ZGC的关键技术之后，我们来重点讲下 ZGC 的回收原理。</p><h2>ZGC的回收原理</h2><p>ZGC虽然在实现上有十个左右的小步骤，但在总体思想上可以概括为三个核心步骤，我们通过<a href=\"https://www.usenix.org/legacy/events/vee05/full_papers/p46-click.pdf\">Pauseless GC原始论文</a>的内容来介绍。</p><p><img src=\"https://static001.geekbang.org/resource/image/9c/2f/9c004f0c74395e78d123bfc5f3f17b2f.jpg?wh=2284x1035\" alt=\"\"></p><p>在这张图中，你可以看到 Pauseless 的三个核心步骤分别是：<strong>Mark、Relocate和 Remap</strong>。接下来我们就简单了解下这三个核心步骤都做了哪些事情。按照步骤的先后顺序，我们先来介绍Mark。</p><h3>Mark</h3><p>事实上，ZGC 也不是完全没有 STW 的。在进行初始标记时，它也需要进行短暂的 STW。不过在这个阶段，ZGC只会扫描 root，之后的标记工作是并发的，所以整个初始标记阶段停顿时间很短。也正是因为这一点，ZGC 的最大停顿时间是可控的，也就是说<strong>停顿时间不会随着堆的增大而增加</strong>。</p><p>初始标记工作完成之后，就可以根据 root 集合进行并发标记了。前面我们提到的三个地址视图 Marked0、Marked1、Remapped 在这里就起了作用。</p><p>在 GC 开始之前，地址视图是 Remapped。那么在 Mark 阶段需要做的事情是，将遍历到的对象地址视图变成 Marked0，也就是修改地址的第 42 位为 1。前面我们讲过，三个地址视图映射的物理内存是相同的，所以修改地址视图不会影响对象的访问。</p><p>除此之外，应用线程在并发标记的过程中也会产生新的对象。类似于 G1 中的 SATB 机制，新分配的对象都认为是活的，它们地址视图也都标记为 Marked0。至此，所有标记为 Marked0 的对象都认为是活跃对象，活跃对象会被记录在一张活跃表中。</p><p>而视图仍旧是Remapped 的对象，就认为是垃圾。接下来，我们进入 Relocate 阶段，也就是转移阶段。</p><h3>Relocate</h3><p><strong>Relocate 阶段的主要任务是搬移对象</strong>，在经过 Mark 阶段之后，活跃对象的视图为 Marked0。搬移工作要做两件事情：</p><ul>\n<li><strong>选择一块区域，将其中的活跃对象搬移到另一个区域；</strong></li>\n<li><strong>将搬移的对象放到forwarding table。</strong></li>\n</ul><p>关于第一点，我们前面提到ZGC是分块的，块区域叫Page；G1也是分块的，只不过被分成的块叫Region。虽然细节上有些差异，但它们总体的思想是类似的。</p><p>至于forwarding table，我们在前面也提到过，它是一张维护对象搬移前和搬移后地址的映射表，key是对象的旧地址，value是对象的新地址。</p><p><img src=\"https://static001.geekbang.org/resource/image/66/4c/66eyy0d67fa3eb8fbf8dda5818905b4c.jpg?wh=2284x1400\" alt=\"\"></p><p>在Relocate阶段，应用线程新创建的对象地址视图标记为Remapped。如果应用线程访问到一个地址视图是Marked0的对象，说明这个对象还没有被转移，那么就需要将这个对象进行转移，转移之后再加入到forwarding table，然后再对这个对象的引用直接指向新地址，完成自愈。这些动作都是发生在read barrier中的，是由应用线程完成的。</p><p>当 GC 线程遍历到一个对象，如果对象地址视图是 Marked0，就将其转移，同时将地址视图置为 Remapped，并加入到forwarding table ；如果访问到一个对象地址视图已经是 Remapped，就说明已经被转移了，也就不做处理了。</p><p>那么关于三个地址视图我们已经用到了其中两个，你一定好奇Marked1视图什么时候使用。接下来我们就进入Remap阶段，为你揭晓Marked1视图的作用。</p><h3>Remap</h3><p><strong>Remap阶段主要是对地址视图和对象之间的引用关系做修正</strong>。因为在Relocate阶段，GC线程会将活跃对象快速搬移到新的区域，但是却不会同时修复对象之间的引用（请注意这一点，这是ZGC和以前我们遇到的所有基于copy的GC算法的最大不同）。这就导致还有大量的指针停留在Marked0视图。</p><p>这样就会导致活跃视图不统一，需要再对对象的引用关系做一次全面的调整，这个过程也是要遍历所有对象的。不过，因为Mark阶段也需要遍历所有对象，所以，可以把当前GC周期的Remap阶段和下一个GC周期的Mark阶段复用。</p><p>但是由于Remap阶段要处理上一轮的Marked0视图指针，又要同时标记下一轮的活跃对象，为了区分，可以再引入一个Mark标记，这就是Marked1标志。可以想象，Marked0和Marked1视图在每一轮GC中是交替使用的。</p><p>在Remap阶段，新分配对象的地址视图是Marked1，如果遇到对象地址视图是Marked0或者Remaped，就把地址视图置为Marked1。具体过程如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/d7/f3/d767c2f1ab6f9f7d2c570dba0443c7f3.jpg?wh=2284x1489\" alt=\"\"></p><p>这个过程结束以后，就完成了地址视图的调整，同时也完成了新一轮的Mark。可以看到，Marked0和Marked1其实是交替进行的，通过地址视图的切换，在应用线程运行的同时，默默就把活对象搬走了，把垃圾回收了。</p><p>好了，关于ZGC的回收原理我们就讲到这里。ZGC的回收过程大致分为三个主要阶段，其中<strong>Mark阶段负责标记活跃对象、Relocate阶段负责活跃对象转移、ReMap阶段负责地址视图统一</strong>。因为Remap阶段也需要进行全局对象扫描，所以Remap和Mark阶段是重叠进行的。</p><h2>总结</h2><p>好啦，这节课到这里就结束啦。这节课，我们先介绍了无暂停GC的发展历史，然后介绍了无暂停回收算法的特点，那就是能够将垃圾回收的最大停顿时间控制在10ms以内，并且停顿时间不会随着堆的增大而线性增加。</p><p>我们选取了openjdk的ZGC作为举例，详细介绍了ZGC停顿时间的真相，同时也分析了ZGC的回收原理。ZGC之所以能够做到这么低的停顿时间，是因为它的大部分工作都是并发执行的，其中也包括了垃圾回收过程中最耗时的对象转移阶段。</p><p><strong>ZGC能够做到并发转移，背后有两大关键技术，分别是read barrier和colored pointer</strong>。read barrier的作用在于应用线程可以在对象转移之后，通过forwarding table实现\"自愈\"。而colored pointer实现了地址视图，十分高效地完成了read barrier需要完成的工作，在实现并发转移的同时，保证吞吐率不出现大幅下降。</p><p>最后我们介绍了ZGC的回收原理，<strong>整个回收过程可以大致分为Mark、Relocate、Remap三个阶段</strong>，其中Mark和Remap阶段是可以重叠的。</p><p>GC开始时，地址视图为Remapped，Mark阶段的主要工作是<strong>标记活跃对象，然后将地址视图向Marked0迁移</strong>，处于Marked0的对象都被认为是活跃对象。</p><p>Relocate阶段开始时，地址视图为Marked0，该阶段主要做<strong>对象搬移工作，将地址视图向Remapped迁移</strong>。应用线程如果访问一个已经被转移的对象，就会触发read barrier，完成“自愈”，最终访问的是Remapped视图的新对象。</p><p>而Remap阶段是<strong>地址视图的修复阶段</strong>，在Remap阶段开始时，地址视图为Remapped。Remap阶段的功能是做<strong>地址视图统一</strong>，对于仍处于Marked0 和 Remaped视图的活跃对象，将其地址视图更新为Marked1。当然也可以是对于仍处于Marked1 和 Remaped视图的活跃对象，将其地址视图更新为Marked0。Remap和Mark阶段交替进行，交替操作Marked0和Marked1视图。</p><p>通过地址视图的切换以及使用read barrier完成对象<strong>“自愈”</strong>过程，使得ZGC能够高效、准确的完成并发转移，大大降低了垃圾回收过程中的停顿时间，以至于达到无暂停GC的效果。</p><p>好啦，以上就是无暂停垃圾回收算法的核心内容了。</p><h2>思考题</h2><p>请你思考一下：ZGC在对象转移之后旧对象原来占用的内存空间是否可以重复利用？请你结合colored pointer的功能思考。一点提示：可以思考一下为什么不使用forwarding指针技术，而要使用forwarding table呢？欢迎在留言区分享你的想法，我在留言区等你。</p><p><img src=\"https://static001.geekbang.org/resource/image/e7/13/e71335bdb1bf497bf731b0be9a9a1c13.jpg?wh=2284x1386\" alt=\"\"></p><p>好啦，这节课到这就结束啦。欢迎你把这节课分享给更多对计算机内存感兴趣的朋友。我是海纳，我们下节课再见！</p>","float_qrcode":"https://static001.geekbang.org/resource/image/83/1c/83e244537b714e899fd2e7896e22371c.png","article_cover_hidden":false,"is_required":true,"score":"51639929600","like_count":0,"article_subtitle":"","audio_download_url":"https://static001.geekbang.org/resource/audio/c2/1a/c2e85063ba0eef8a2d9fec624737df1a.mp3","had_viewed":true,"article_title":"23 | Pauseless GC：挑战无暂停的垃圾回收","column_bgcolor":"#F6F7FB","offline_package":"{}","audio_title":"12.19 海纳 23_zgc_01.MP3","audio_time_arr":{"m":"20","s":"06","h":"00"},"text_read_percent":108610361632,"cid":450,"article_cshort":"<p>你好，我是海纳。</p><p>在前面的几节课程中，我们学习了 CMS 、G1 等垃圾回收算法，这两类GC算法虽然一直在想办法降低GC时延，但它们仍然存在相当可观的停顿时间。</p><p>如何进一步降低GC的停顿时间，是当前垃圾回收算法领域研究的最热点话题之一。今天我们就来学习这类旨在减少GC停顿的垃圾回收算法，也就是<strong>无暂停GC</strong>（Pauseless GC）。由于Hotspot的巨大影响力和普及程度，以及它的代码最容易获得，我们这节课就以ZGC为例来深入讲解无暂停GC。</p><p>而且，ZGC对Java程序员的意义和G1是同样重要的。如果说CMS代表的是过去式，而G1是一种过渡（尽管这个过渡期会很长），那么ZGC无疑就是JVM自动内存管理器的未来。</p><p>通过这节课的学习，你就能了解到无暂停GC的基本思想和可以使用的条件，从而为未来正确地使用无暂停GC做好充分的准备。</p><p>无暂停GC这个词你可能比较陌生，让你觉得这个算法很难，我们不妨先来了解一下它的前世今生，你就能知其然，经过后面对它原理的讲解，你就能知其所以然了。</p><h2>无暂停GC简介</h2><p>JVM的核心开发者Cliff Click供职于Azul Systems公司期间，撰写了一篇很重要的论文，也就是<a href=\"https://www.usenix.org/legacy/events/vee05/full_papers/p46-click.pdf\">Pauseless GC</a>，提出了无暂停GC的想法和架构设计。同时，Azul公司也在他们的JVM产品Zing中实现了一个无暂停GC，将GC的停顿时间大大减少，这就是<a href=\"https://www.azul.com/products/components/pgc\">C4垃圾回收器</a>。</p>","video_width":0,"column_could_sub":true,"video_id":"","sku":"100094901","video_cover":"","author_name":"海纳","column_is_onboard":true,"audio_url":"https://res001.geekbang.org/media/audio/c2/1a/c2e85063ba0eef8a2d9fec624737df1a/ld/ld.m3u8","chapter_id":"2404","column_had_sub":true,"column_cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg","neighbors":{"left":{"article_title":"22 | G1 GC：分区回收算法说的是什么？","id":468883},"right":{"article_title":"24 | GC实例：Python和Go的内存管理机制是怎样的？","id":470918}},"rate_percent":42,"footer_cover_data":{"img_url":"https://static001.geekbang.org/resource/image/f0/fe/f0c47daf3262a8df6cf352d89896abfe.png","link_url":"https://time.geekbang.org/article/427012","mp_url":""},"float_app_qrcode":"","column_is_experience":false,"rate":{"1":{"cur_version":1641480156,"max_rate":36,"cur_rate":36,"is_finished":false,"total_rate":86,"learned_seconds":70},"2":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"3":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0}},"product_id":100094901,"had_liked":false,"id":469971,"free_get":false,"is_video_preview":false,"article_summary":"如果说CMS代表的是过去式，而G1是一种过渡（尽管这个过渡期会很长），那么ZGC无疑就是JVM自动内存管理器的未来。","column_sale_type":0,"float_qrcode_jump":"https://time.geekbang.org/serv/v4/misc/jump?uri=https%3A%2F%2Ftime.geekbang.org%2Fhybrid%2Fmp%2Fjump%3Furl%3Dhttps%253A%252F%252Fstatic001.geekbang.org%252Fresource%252Fimage%252Fee%252Fb2%252Feedd4090668aabfac91b2b445d27cbb2.png","column_id":450,"article_sharetitle":"23 | Pauseless GC：挑战无暂停的垃圾回收","article_poster_wxlite":"https://static001.geekbang.org/render/screen/74/be/745163fec4101d4c4c3c39b17a59adbe.jpeg","article_features":0,"comment_count":3,"audio_md5":"c2e85063ba0eef8a2d9fec624737df1a","offline":{"size":20623981,"file_name":"aaf03095fd706b216a0bcc08960081e9","download_url":"https://static-acl-001.geekbang.org/resource/zip/article/469971/aaf03095fd706b216a0bcc08960081e9.zip?auth_key=1641482413-29660440d5bb440db8ebf7603342d9d2-0-b9133ec62bab65fb61acbb9a0e0d2f76"},"video_size":0,"hls_videos":[],"video_time":"","article_could_preview":false,"article_ctime":1639929600,"share":{"content":"学好内存，掌握系统开发诀窍","title":"海纳 · 编程高手必学的内存知识","poster":"https://static001.geekbang.org/resource/image/bc/c5/bcaf3ff69e538f8749970edf2301bbc5.jpg","cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg"}},"470918":{"text_read_version":0,"audio_size":23691885,"article_cover":"https://static001.geekbang.org/resource/image/c4/b4/c4dc222c3bb1bf2d1183cb56164773b4.jpg","subtitles":[],"product_type":"c1","audio_dubber":"海纳","is_finished":false,"like":{"had_done":false,"count":2},"audio_time":"00:24:40","video_height":0,"article_content":"<p>你好，我是海纳。</p><p>我们前面几节课主要是以Java为例，介绍了JVM中垃圾回收算法的演进过程。实际上，除了JVM之外，用于运行JavaScript的V8虚拟机、Lua虚拟机、Python虚拟机和Go的虚拟机都采用了自动内存管理技术。这节课，我们就一起来分析一下它们的实现。</p><p>通过这节课，你将会看到垃圾回收算法的设计是十分灵活而且多种多样的，这会为你以后改进应用中自动或半自动的内存管理，提供很好的参考。你要注意的是，学习自动内存管理，一定要抓住核心原理，不要陷入到细节里去。另外，你可以通过查看虚拟机源代码来验证自己的猜想，但不要把源代码教条化。</p><p>接下来，我先解释一下为什么选择Python和GO这两种语言做为例子。</p><h2>静态语言和动态语言</h2><p>我先介绍两个基本概念：<strong>动态语言和静态语言</strong>。动态语言的特征是可以在运行时，为对象甚至是类添加新的属性和方法，而静态语言不能在运行期间做这样的修改。</p><p>动态语言的代表是Python和JavaScript，静态语言的代表是C++。Java本质上是一门静态语言，但它又提供了反射（reflection）的能力为动态性开了一个小口子。</p><p>从实现的层面讲，静态语言往往在编译时就能确定各个属性的偏移值，所以编译器能确定某一种类型的对象，它的大小是多少。这就方便了在分配和运行时快速定位对象属性。关于静态语言的对象内存布局，我们在<a href=\"https://time.geekbang.org/column/article/431373\">导学（三）</a>和<a href=\"https://time.geekbang.org/column/article/465516\">第19节课</a>都做了介绍，你可以去看看。</p><!-- [[[read_end]]] --><p>而动态语言往往会将对象组织成一个字典结构，例如下面这个Python的例子：</p><pre><code>&gt;&gt;&gt; class A(object):\n...  pass\n...\n&gt;&gt;&gt; a = A()\n&gt;&gt;&gt; a.b = 1\n&gt;&gt;&gt; b = A()\n&gt;&gt;&gt; b.c = 2\n&gt;&gt;&gt; b.__dict__\n{'c': 2}\n&gt;&gt;&gt; a.__dict__\n{'b': 1}\n</code></pre><p>你可以看到，类A的两个对象a和b，它们的属性是不相同的。这在Java语言中是很难想象的，但是在Python或者JavaScript中，却是司空见惯的操作。</p><p><strong>如果把上述代码中的a对象和b对象想象成一个字典的话，这段代码就不难理解了。第5行和第6行的操作不过是相当于在字典中添加了新的一个键值对而已。</strong></p><p>在了解了动态语言和静态语言的区别以后，我们就从动态语言中选择Python语言，从静态语言中选择Go语言，来对两种语言的实现加以解释。其中，Python语言的分配过程与<a href=\"https://time.geekbang.org/column/article/440452\">第9节课</a>所讲的malloc的实现非常相似，所以我们重点看它的垃圾回收过程。Go语言的垃圾回收过程就是简单的CMS，所以我们重点分析它的分配过程。下面，我们先从Python语言开始吧。</p><h2>Python 的内存管理机制</h2><p>我们先从Python的对象布局讲起，以最简单的浮点数为例，在Include/floatobject.h中，python中的浮点数是这么定义的：</p><pre><code>typedef struct {\n    PyObject_HEAD\n    double ob_fval;\n} PyFloatObject;\n</code></pre><p>我们继续查看PyObject_HEAD的定义：</p><pre><code>/* PyObject_HEAD defines the initial segment of every PyObject. */\n#define PyObject_HEAD                   \\\n    _PyObject_HEAD_EXTRA                \\\n    Py_ssize_t ob_refcnt;               \\\n    struct _typeobject *ob_type;\n</code></pre><p>这是一个宏定义，而其中的EXTRA在正常编译时是空的。所以，我们直接展开所有宏，那么PyFloatObject的定义就是这样子的：</p><pre><code>typedef struct {\n    Py_ssize_t ob_refcnt;\n    struct _typeobject *ob_type;\n    double ob_fval;\n} PyFloatObject;\n</code></pre><p>这样就很清楚了，ob_refcnt就是引用计数，而ob_fval是真正的值。例如我写一段这样的代码：</p><pre><code>a = 1000.0\na = 2000.0\n</code></pre><p>在执行第1句时，Python虚拟机真正执行的逻辑是创建一个PyFloatObject对象，然后使它的ob_fval为1000.0，同时，它的引用计数为1；当执行到第2句时，创建一个值为2000.0的PyFloatObject对象，并且使这个对象的引用计数为1，而前一个对象的引用计数就要减1，从而变成0。那么前一个对象就会被回收。</p><p>在Python中，引用计数的维护是通过这两个宏来实现的：</p><pre><code>#define Py_INCREF(op) (                         \\\n    _Py_INC_REFTOTAL  _Py_REF_DEBUG_COMMA       \\\n    ((PyObject*)(op))-&gt;ob_refcnt++)\n#define Py_DECREF(op)                                   \\\n    do {                                                \\\n        if (_Py_DEC_REFTOTAL  _Py_REF_DEBUG_COMMA       \\\n        --((PyObject*)(op))-&gt;ob_refcnt != 0)            \\\n            _Py_CHECK_REFCNT(op)                        \\\n        else                                            \\\n        _Py_Dealloc((PyObject *)(op));                  \\\n    } while (0)\n</code></pre><p>这两个宏位于Include/object.h中。这段代码里最重要的地方在于ob_refcnt增一和减一的操作。这段代码与<a href=\"https://time.geekbang.org/column/article/465516\">第19节课</a>所讲的引用计数法的伪代码十分相似，我就不重复分析了。</p><p>使用了引用计数的地方，就会存在循环引用。例如下图中的四个对象，A是根对象，它与B之间有循环引用，那么它们都不是垃圾对象。C和D之间也有循环引用，但因为没有外界的引用指向它们了，所以它们就是垃圾对象，但是循环引用导致他们都不能释放。</p><p><img src=\"https://static001.geekbang.org/resource/image/97/5f/97cc143f4d4216ba56ace93f1b8f7f5f.jpg?wh=2284x946\" alt=\"\"></p><p>Python为了解决这个问题，在虚拟机中引入了一个双向链表，把所有对象都放到这个链表里。Python的每个对象头上都有一个名为PyGC_Head的结构：</p><pre><code>/* GC information is stored BEFORE the object structure. */\ntypedef union _gc_head {\n    struct {\n        union _gc_head *gc_next;\n        union _gc_head *gc_prev;\n        Py_ssize_t gc_refs;\n    } gc;\n    long double dummy;  /* force worst-case alignment */\n} PyGC_Head;\n</code></pre><p>在这个结构里，gc_next和gc_prev的作用就是把对象关联到链表里。而gc_refs则是用于消除循环引用的。当链表中的对象达到一定数目时，Python的GC模块就会执行一次标记清除。具体来讲，一共有四步。</p><p>第一步，<strong>将ob_refcnt的值复制到gc_refs中</strong>。对于上面的例子，它们的gc_refs的值就如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/52/49/524145acfebb39dc369c22fb3c36f649.jpg?wh=2284x896\" alt=\"\"></p><p>第二步是<strong>遍历整个链表，对每个对象，将它直接引用的对象的gc_refs的值减一</strong>。比如遍历到A对象时，只把B对象的gc_refs值减一；遍历到B对象时，再把它直接引用的A对象的gc_refs值减一。经过这一步骤后，四个对象的gc_refs的值如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/7d/c9/7d29ee40420ff42a0c6a77455e2f0cc9.jpg?wh=2284x896\" alt=\"\"></p><p>第三步，<strong>将gc_refs值为0的对象，从对象链表中摘下来，放入一个名为“临时不可达”的链表中</strong>。之所以使用“临时”，是因为有循环引用的垃圾对象的gc_refs在此时一定为0，比如C和D。但gc_refs值为0的对象不一定是垃圾对象，比如B对象。此时，B、C和D对象就被放入临时不可达链表中了，示意图如下所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/38/69/3823754c6a23a3874f45547066fbd869.jpg?wh=2284x863\" alt=\"\"></p><p>最后一步，<strong>以可达对象链表中的对象为根开始深度优先搜索</strong>，<strong>将所有访问到gc_refs为0的对象，再从临时不可达链表中移回可达链表中</strong>。最后留在临时不可达链表中的对象，就是真正的垃圾对象了。</p><p>接下来就可以使用_Py_Dealloc逐个释放链表中的对象了，对于上面的例子，就是把B对象重新加回到可达对象链表中，然后将C和D分别释放。</p><p>到这里，Python内存管理的核心知识我们就介绍完了，接下来，我们来看Go语言的例子。它的特点是分配算法复杂，但清除算法简单。</p><h2>Go语言的内存管理机制</h2><p>Go语言采用的垃圾回收算法是<strong>并发标记清理</strong>（Concurrent Mark Sweep，CMS）算法。CMS算法是Tracing GC算法中非常经典且朴素的例子，我们在前边的课程中讲了基于复制的GC算法和基于分区的GC算法，它们都在不同方面比CMS GC有优势，那为什么Go语言还是选择了CMS作为其GC算法呢？这里原因主要有两点：</p><p><strong>第一点是，Go语言通过内存分配策略缓解了CMS容易产生内存碎片的缺陷。</strong></p><p>相对于CMS GC，基于压缩的GC算法和基于复制的GC算法最大的优势是，能够降低内存的碎片率。这一点我们在前边的课程里已经充分讨论了。Go的GC算法采用TCMalloc分配器的内存分配思路，虽然不能像基于copy的GC算法那样消除掉内存碎片化的问题，但也极大地降低了碎片率。</p><p>另外，基于Thread Cache的分配机制可以使得Go在分配的大部分场景下避免加锁，这使得Go在高并发场景下能够发挥巨大的性能优势。</p><p><strong>第二点是，Go语言是有值类型数据的，即struct类型</strong>。有了值类型的介入，编译器只需要关注函数内部的逃逸分析（intraprocedural escape analysis），而不用关注函数间的逃逸分析（interprocedural analysis），由此可以将生命周期很短的对象安排在栈上分配。</p><p>在前面的课程里，我们介绍了分代GC可以区分长生命周期和短生命周期对象，它的优势是能够快速回收生命周期短的对象。但由于逃逸分析的优势，Go语言中的短生命周期对象并没有那么多，所以分代GC在Go语言中收益较低。另外，由于分代GC需要额外的write barrier来维护老年代对年轻代的引用关系，也加重了GC引入的开销。</p><p>基于这两个主要原因，Go语言目前采用的GC算法是CMS算法。当然，Go语言在演进的过程中也曾采用过ROC和分代GC的算法，但后来也都放弃了。如果你对Go GC的演进过程感兴趣，可以看下<a href=\"https://go.dev/blog/ismmkeynote\">《Getting to Go: The Journey of Go’s Garbage Collector》</a>这篇文章。</p><p>在上面讲的第一点原因中，我们提到Go在CMS算法中，为了提高分配效率并且保障堆空间较低的碎片率，采用了TCMalloc的分配思想。所以，我们先来看一下TCMalloc的分配思想是怎样的，把握住TCMalloc的思想，你就能容易理解Go的分配机制了。</p><h2>TCMalloc的分配思想</h2><p>在TCMalloc中，“TC”是Thread Cache的意思，其核心思想是：<strong>TCMalloc会给每个线程分配一个Thread-Local Cache，对于每个线程的分配请求，就可以从自己的Thread-Local Cache区间来进行分配。此时因为不会涉及多线程操作，所以并不需要进行加锁，从而减少了因为锁竞争而引起的性能损耗。</strong></p><p>而当Thread-Local Cache空间不足的时候，才向下一级的内存管理器请求新的空间。TCMalloc引入了Thread cache、Central cache以及Page heap三个级别的管理器来管理内存，可以充分利用不同级别下的性能优势。这个时候你会发现，TCMalloc的多级管理机制非常类似计算机系统结构的内存多级缓存机制。</p><p><img src=\"https://static001.geekbang.org/resource/image/45/a8/45883b243e08fb71fd6453f63d489da8.jpg?wh=2284x1310\" alt=\"\"></p><p>围绕着这个核心思想，我们具体看下Go的分配器是怎么实现的。在Go的内存管理机制中，有几个重要的数据结构需要关注，分别是<strong>mspan、heapArena、mcache、mcentral以及mheap</strong>。其中，mspan和heapArena维护了Go的虚拟内存布局，而mcache、mcentral以及mheap则构成了Go的三层内存管理器。</p><h3>虚拟内存布局</h3><p>Go的内存管理基本单元是mspan，每个mspan中会维护着一块连续的虚拟内存空间，内存的起始地址由startAddr来记录。每个mspan存储的内存空间大小都是内存页的整数倍，由npages来保存。不过你需要注意的是，这里内存页并非是操作系统的物理页大小，Go的内存页大小设置的是8KB。mspan结构的部分定义如下：</p><pre><code>type mspan struct {\n    next *mspan     // next span in list, or nil if none\n    prev *mspan     // previous span in list, or nil if none\n\n    startAddr uintptr // address of first byte of span aka s.base()\n    npages    uintptr // number of pages in span\n    ...\n    spanclass   spanClass     // size class and noscan (uint8)\n    ...\n}\n</code></pre><p><img src=\"https://static001.geekbang.org/resource/image/32/eb/323bc82131d5ba1675a8bd9d9ef53ceb.jpg?wh=2284x1290\" alt=\"\"></p><p>heapArena的结构相当于Go的一个内存块，在x86-64架构下的Linux系统上，一个heapArena维护的内存空间大小是64MB。该结构中存放了ArenaSize/PageSize 长度的mspan数组，heapArena结构的spans变量，用来精确管理每一个内存页。而整个arena内存空间的基址则存放在zeroedBase中。heapArena结构的部分定义如下：</p><pre><code>type heapArena struct {\n    ...\n    spans [pagesPerArena]*mspan\n    zeroedBase uintptr\n}\n</code></pre><p>有了这两个结构，我们就可以整体看下Go的虚拟内存布局了。Go整体的虚拟内存布局是存放在mheap中的一个heapArena的二维数组。定义如下：</p><pre><code>arenas [1 &lt;&lt; arenaL1Bits]*[1 &lt;&lt; arenaL2Bits]*heapArena\n</code></pre><p>这里二维数组的大小在不同架构跟操作系统上有所不同，对于x86-64架构下的Linux系统，第一维数组长度是1，而第二维数组长度是4194304。这样每个heapArena管理的内存大小是64MB，由此可以算出Go的整个堆空间最多可以管理256TB的大小。</p><p><img src=\"https://static001.geekbang.org/resource/image/61/d6/6100a108e58e380b6635c7bb705a70d6.jpg?wh=2284x1110\" alt=\"\"></p><p>这里我们又会发现，Go通过heapArena来对虚拟内存进行管理的方式其实跟操作系统通过页表来管理物理内存是一样的。了解了Go的虚拟内存布局之后，我们再来看下Go的三级内存管理器。</p><h3>三级内存管理</h3><p>在Go的三级内存管理器中，维护的对象都是小于32KB的小对象。对于这些小对象，Go又将其按照大小分成了67个类别，称为spanClass。每一个spanClass都用来存储固定大小的对象。这67个spanClass的信息在runtime.sizeclasses.go中可以看到详细的说明。我选取了一部分注释放在了下面，你可以看看。</p><pre><code>// class  bytes/obj  bytes/span  objects  tail waste  max waste  min align\n//     1          8        8192     1024           0     87.50%          8\n//     2         16        8192      512           0     43.75%         16\n//     3         24        8192      341           8     29.24%          8\n//     4         32        8192      256           0     21.88%         32\n//    ...\n//    67      32768       32768        1           0     12.50%       8192\n</code></pre><p>对于上面的注释，我以class 3为例做个介绍。class 3是说在spanClass为3的span结构中，存储的对象的大小是24字节，整个span的大小是8192字节，也就是一个内存页的大小，可以存放的对象数目最多是341。</p><p>tail waste这里是8字节，这个8是通过8192 mod 24计算得到，意思是，当这个span填满了对象后，会有8字节大小的外部碎片。而max waste的计算方式则是$\\left[ \\left( 24-17 \\right)\\times341+8\\right]\\div8192$得到，意思是极端场景下，该span上分配的所有对象大小都是17字节，此时的内存浪费率为29.24%。</p><p>以上67个存储小对象的spanClass级别，再加上class为0时用来管理大于32KB对象的spanClass，共总是68个spanClass。这些数据都是通过在runtime.mksizeclasses.go中计算得到的。我们从上边的注释可以看出，Go在分配的时候，是通过控制每个spanClass场景下的最大浪费率，来保障堆内存在GC时的碎片率的。</p><p><img src=\"https://static001.geekbang.org/resource/image/a2/26/a2466f262yy6886a4813f99b587cee26.jpg?wh=2284x1338\" alt=\"\"></p><p>另外，spanClass的ID中还会通过最后一位来存放noscan的属性。这个标志位是用来告诉Collector该span中是否需要扫描。如果当前span中并不存放任何堆上的指针，就意味着<strong>Collector不需要扫描这段span区间</strong>。</p><pre><code>func makeSpanClass(sizeclass uint8, noscan bool) spanClass {\n    return spanClass(sizeclass&lt;&lt;1) | spanClass(bool2int(noscan))\n}\n\nfunc (sc spanClass) sizeclass() int8 {\n    return int8(sc &gt;&gt; 1)\n}\n\nfunc (sc spanClass) noscan() bool {\n    return sc&amp;1 != 0\n}\n</code></pre><p>好了，接下来我们继续看下mcache的结构。mcache是Go的线程缓存，对应于TCMalloc中的Thread cache结构。mcache会与线程绑定，每个goroutine在向mcache申请内存时，都不会与其他goroutine发生竞争。mcache中会维护上述$68\\times2$种spanClass的mspan数组，存放在mcache的alloc中，包括scan以及noscan两个队列。mcache的主要结构如下，其中tiny相关的三个field涉及到tiny对象的分配，我们稍后在对象分配机制中再进行介绍。</p><pre><code>type mcache struct {\n    ...\n    tiny       uintptr\n    tinyoffset uintptr\n    tinyAllocs uintptr\n\n    alloc [numSpanClasses]*mspan // spans to allocate from, indexed by spanClass\n    ...\n}\n</code></pre><p><img src=\"https://static001.geekbang.org/resource/image/98/87/98a5961841b744615161dde97cbbf787.jpg?wh=2284x1279\" alt=\"\"></p><p>当mcache中的内存不够需要扩容时，需要向mcentral请求，mcentral对应于TCMalloc中的Central cache结构。mcentral的主要结构如下：</p><pre><code>type mcentral struct {\n    spanclass spanClass\n    partial [2]spanSet // list of spans with a free object\n    full    [2]spanSet // list of spans with no free objects\n}\n</code></pre><p>可以看到，mcentral中也存有spanClass的ID标识符，这表示说每个mcentral维护着固定一种spanClass的mspan。spanClass下面是两个spanSet，它们是mcentral维护的mspan集合。partial里存放的是包含着空闲空间的mspan集合，full里存放的是不包含空闲空间的span集合。这里每种集合都存放两个元素，用来<strong>区分集合中mspan是否被清理过</strong>。</p><p>mcentral不同于mcache，每次请求mcentral中的mspan时，都可能发生不同线程直接的竞争。因此，在使用mcentral时需要进行加锁访问，具体来讲，就是spanSet的结构中会有一个mutex的锁的字段。</p><p><img src=\"https://static001.geekbang.org/resource/image/4a/e0/4a1633500dba8f4923d3bbba9e6b97e0.jpg?wh=2284x1333\" alt=\"\"></p><p>我们最后看下mheap的结构。mheap在Go的运行时里边是只有一个实例的全局变量。上面我们讲到，维护Go的整个虚拟内存布局的heapArena的二维数组，就存放在mheap中。mheap结构对应于TCMalloc中的Page heap结构。mheap的主要结构如下：</p><pre><code>type mheap struct {\n    lock  mutex\n\n    arenas [1 &lt;&lt; arenaL1Bits]*[1 &lt;&lt; arenaL2Bits]*heapArena\n    central [numSpanClasses]struct {\n        mcentral mcentral\n        pad      [cpu.CacheLinePadSize - unsafe.Sizeof(mcentral{})%cpu.CacheLinePadSize]byte\n    }\n}\n\nvar mheap_ mheap\n</code></pre><p>mheap中存放了$68\\times2$个不同spanClass的mcentral数组，分别区分了scan队列以及noscan队列。</p><p>到目前为止，我们就对Go的三级内存管理器有了一个整体的认识，下面这张图展示了这几个结构的关系。</p><p><img src=\"https://static001.geekbang.org/resource/image/e4/75/e4e35deaffa0504a8aa9cd7955eec375.jpg?wh=2284x1402\" alt=\"\"></p><p>学习了Go的三级内存管理机制，那么Go的对象分配逻辑也就很清晰了。</p><h3>对象分配机制</h3><p>我们在这里需要注意的一点是：Go根据对象大小分成了三个级别，分别是微小对象、小对象和大对象。<strong>微小对象</strong>，也就是指大小在0 ~ 16字节的非指针类型对象；<strong>小对象</strong>，指的是大小在16 ~ 32KB的对象以及小于16字节的指针对象；<strong>大对象</strong>，也就是上文提到的spanClass为0的类型。</p><p>对于这三种类型对象，Go的分配策略也不相同，对象分配的主要逻辑如下：</p><pre><code>if size &lt;= maxSmallSize {\n    if noscan &amp;&amp; size &lt; maxTinySize {\n      // 微小对象分配\n    } else {\n      // 小对象分配\n    }\n} else {\n  // 大对象分配\n}\n</code></pre><p>微小对象会被放到spanClass为2的mspan中，由于每个对象最大是16字节，在分配时会尽量将多个小对象放到同一个内存块内（16字节），这样可以更进一步的降低这种微小对象带来的碎片化。</p><p>我们在mcache中提到的三个字段：tiny、tinyoffset和tinyAllocs就是用来维护微小对象分配器的。tiny是指向当前在使用的16字节内存块的地址，tinyoffset则是指新分配微小对象需要的起始偏移，tinyAllocs则存放了目前这个mcache中共存放了多少微小对象。</p><p><img src=\"https://static001.geekbang.org/resource/image/4b/c3/4b04b0918e4963e1065f6913f73737c3.jpg?wh=2284x1387\" alt=\"\"></p><p>对于小对象的分配，整体逻辑跟TCMalloc是保持一致的，就是依次向三级内存管理器请求内存，一旦内存请求成功则返回，这里我就不详细展开了。</p><p>对于大对象的分配，Go并不会走上述的三次内存管理器，而是直接通过调用mcache.allocLarge来分配大内存。allocLarge会以内存页的倍数大小来通过mheap_.alloc申请对应大小内存，并构建起spanClass为0的mspan对象返回。</p><p>好，接下来我们具体看下Go的内存回收器是怎么实现的。</p><h2>内存回收机制</h2><p>前面我们讲到，Go的GC算法采用的是经典的CMS算法，在并发标记的时候使用的是三色标记清除算法。而在write barrier上则采用了<strong>Dijkstra的插入barrier</strong>，以及汤浅太一提出的删除<strong>barrier混合的barrier算法</strong>。</p><p>另外，为了降低GC在回收时STW的最长时间，Go在内存回收时并不是一次性回收完全部的垃圾，而是采用增量回收的策略，将整个垃圾回收的过程切分成多个小的回收cycle。具体来讲，Go的内存回收主要分为这几个阶段：</p><ol>\n<li><strong>清除终止阶段；</strong></li>\n</ol><p>这个阶段会进行STW，让所有的线程都进入安全点。如果此次GC是被强制触发的话，那么这个阶段还需要清除上次GC尚未清除的mspan。</p><ol start=\"2\">\n<li><strong>标记阶段；</strong></li>\n</ol><p>在进行标记阶段之前，需要先做一些准备工作，也就是将gcphase状态从_GCoff切换到_GCmark，并打开write barrier和mutator assists，然后将根对象压入扫描队列中。此时，所有的mutator还处于STW状态。</p><p>准备就绪后重启mutator的运行，此时后台中的标记线程和mutator assists可以共同帮助GC进行标记。在标记过程中，write barrier会把所有被覆盖的指针以及新指针都标记为灰色，而新分配的对象指针则直接标记为黑色。</p><p>标记线程开始进行根对象扫描，包括所有的栈对象、全局变量的对象，还有所有堆外的运行时数据结构。这里你要注意的是，当对栈进行扫描的时候需要暂停当前的线程。扫描完根对象后，标记线程会继续扫描灰色队列中的对象，将对象标记为黑色并依次将其引用的对象标灰入队。</p><p>由于Go中每个线程都有一个Thread Local的cache，GC采用的是分布式终止算法来检查各个mcache中是否标记完成。</p><ol start=\"3\">\n<li><strong>标记终止阶段；</strong></li>\n</ol><p>在标记终止阶段会进行STW，暂停所有的线程。STW之后将gcphase的状态切换到_GCmarktermination，并关闭标记进程和mutator assists。此外还会进行一些清理工作，刷新mcache。</p><ol start=\"4\">\n<li><strong>清除阶段。</strong></li>\n</ol><p>在开始清除阶段之前，GC会先将gcphase状态切换到_GCoff，并关闭write barrier。接着再恢复用户线程执行，此时新分配的对象只会是白色状态。并且清除工作是增量是进行的，所以在分配时，也会在必要的情况下先进行内存的清理。这个阶段的内存清理工作会在后台并发完成。</p><h2>总结</h2><p>这节课做为自动内存管理的最后一节，同样也是我们专栏的最后一篇文章，我们通过两个具体的例子来展示了，实际场景中语言虚拟机是如何进行内存管理的。</p><p>首先，我们介绍了动态语言和静态语言的区别，并在动态语言中选择Python做为实例，在静态语言中选择Go做为实例进行介绍。</p><p>它们的数据结构和算法设计其实并没有超出我们之前课程所介绍的理论。只是在代码实现上，在细节上做了一些调整。</p><p>我们讲解了动态类型语言的内存布局，接着又分析了Python中的内存回收的具体实现。Python主要使用引用计数法进行内存管理。为了解决循环引用的问题，又引入了一种特殊的标记算法来释放垃圾对象。</p><p>在Go语言的内存管理机制中，我们详细学习了Go中内存分配的实现。Go语言通过采用TCMalloc的分配器思路，以及对内存对象类别大小的精确控制，保障了程序在运行过程中能够有高速的分配效率，以及维持较低的碎片率，这样回收器就可以采用相对简单的CMS算法。</p><p>到这里，我们就把自动内存管理的核心内容全部介绍完了。</p><h2>思考题</h2><p>如果你对Go语言比较熟悉的话，可以思考一下，Go语言中保留了值类型，有哪些优点和缺点呢？欢迎在留言区分享你的想法，我在留言区等你。</p><p><img src=\"https://static001.geekbang.org/resource/image/f6/cb/f6c74dc0fcf052f32d46d8c952f939cb.jpg?wh=2284x1386\" alt=\"\"></p><p>好啦，这节课到这就结束啦。欢迎你把这节课分享给更多对计算机内存感兴趣的朋友。我是海纳，我们下节课再见！</p>","float_qrcode":"https://static001.geekbang.org/resource/image/83/1c/83e244537b714e899fd2e7896e22371c.png","article_cover_hidden":false,"is_required":true,"score":"51640102400","like_count":4,"article_subtitle":"","audio_download_url":"https://static001.geekbang.org/resource/audio/5f/f1/5ffefaf7b1c3404743f5858cc2200af1.mp3","had_viewed":false,"article_title":"24 | GC实例：Python和Go的内存管理机制是怎样的？","column_bgcolor":"#F6F7FB","offline_package":"{}","audio_title":"12.21 海纳 24_pygo_01.MP3","audio_time_arr":{"m":"24","s":"40","h":"00"},"text_read_percent":0,"cid":450,"article_cshort":"<p>你好，我是海纳。</p><p>我们前面几节课主要是以Java为例，介绍了JVM中垃圾回收算法的演进过程。实际上，除了JVM之外，用于运行JavaScript的V8虚拟机、Lua虚拟机、Python虚拟机和Go的虚拟机都采用了自动内存管理技术。这节课，我们就一起来分析一下它们的实现。</p><p>通过这节课，你将会看到垃圾回收算法的设计是十分灵活而且多种多样的，这会为你以后改进应用中自动或半自动的内存管理，提供很好的参考。你要注意的是，学习自动内存管理，一定要抓住核心原理，不要陷入到细节里去。另外，你可以通过查看虚拟机源代码来验证自己的猜想，但不要把源代码教条化。</p><p>接下来，我先解释一下为什么选择Python和GO这两种语言做为例子。</p><h2>静态语言和动态语言</h2><p>我先介绍两个基本概念：<strong>动态语言和静态语言</strong>。动态语言的特征是可以在运行时，为对象甚至是类添加新的属性和方法，而静态语言不能在运行期间做这样的修改。</p><p>动态语言的代表是Python和JavaScript，静态语言的代表是C++。Java本质上是一门静态语言，但它又提供了反射（reflection）的能力为动态性开了一个小口子。</p><p>从实现的层面讲，静态语言往往在编译时就能确定各个属性的偏移值，所以编译器能确定某一种类型的对象，它的大小是多少。这就方便了在分配和运行时快速定位对象属性。关于静态语言的对象内存布局，我们在<a href=\"https://time.geekbang.org/column/article/431373\">导学（三）</a>和<a href=\"https://time.geekbang.org/column/article/465516\">第19节课</a>都做了介绍，你可以去看看。</p>","video_width":0,"column_could_sub":true,"video_id":"","sku":"100094901","video_cover":"","author_name":"海纳","column_is_onboard":true,"audio_url":"https://res001.geekbang.org/media/audio/5f/f1/5ffefaf7b1c3404743f5858cc2200af1/ld/ld.m3u8","chapter_id":"2404","column_had_sub":true,"column_cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg","neighbors":{"left":{"article_title":"23 | Pauseless GC：挑战无暂停的垃圾回收","id":469971},"right":{"article_title":"不定期福利第一期 | 海纳：我是如何学习计算机知识的？","id":447556}},"rate_percent":0,"footer_cover_data":{"img_url":"https://static001.geekbang.org/resource/image/f0/fe/f0c47daf3262a8df6cf352d89896abfe.png","link_url":"https://time.geekbang.org/article/427012","mp_url":""},"float_app_qrcode":"","column_is_experience":false,"rate":{"1":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"2":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"3":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0}},"product_id":100094901,"had_liked":false,"id":470918,"free_get":false,"is_video_preview":false,"article_summary":"Python语言的分配过程与第9节课所讲的malloc的实现非常相似，所以我们重点看它的垃圾回收过程。Go语言的垃圾回收过程就是简单的CMS，所以我们重点分析它的分配过程。","column_sale_type":0,"float_qrcode_jump":"https://time.geekbang.org/serv/v4/misc/jump?uri=https%3A%2F%2Ftime.geekbang.org%2Fhybrid%2Fmp%2Fjump%3Furl%3Dhttps%253A%252F%252Fstatic001.geekbang.org%252Fresource%252Fimage%252Fee%252Fb2%252Feedd4090668aabfac91b2b445d27cbb2.png","column_id":450,"article_sharetitle":"24 | GC实例：Python和Go的内存管理机制是怎样的？","article_poster_wxlite":"https://static001.geekbang.org/render/screen/08/b0/08cb51a8b0d049afc81658cdd33830b0.jpeg","article_features":0,"comment_count":3,"audio_md5":"5ffefaf7b1c3404743f5858cc2200af1","offline":{"size":25386842,"file_name":"5aff7dafebaca2d06ab52cd76e83ab19","download_url":"https://static-acl-001.geekbang.org/resource/zip/article/470918/5aff7dafebaca2d06ab52cd76e83ab19.zip?auth_key=1641482429-b11f072fba204bf4a94adfffe3e0c02c-0-30e47298c568e8bc549b1e5302201393"},"video_size":0,"hls_videos":[],"video_time":"","article_could_preview":false,"article_ctime":1640102400,"share":{"content":"学好内存，掌握系统开发诀窍","title":"海纳 · 编程高手必学的内存知识","poster":"https://static001.geekbang.org/resource/image/bc/c5/bcaf3ff69e538f8749970edf2301bbc5.jpg","cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg"}},"471878":{"text_read_version":0,"audio_size":559644,"article_cover":"https://static001.geekbang.org/resource/image/b2/99/b2464347af002d1363125c145e927799.jpg","subtitles":[],"product_type":"c1","audio_dubber":"海纳","is_finished":false,"like":{"had_done":false,"count":1},"audio_time":"00:00:34","video_height":0,"article_content":"<p>你好，我是海纳。</p><p>到这里，《编程高手必学的内存知识》这门课的正文内容就全部结束了。我特别给你准备了一套期末测试题，来帮助你检验自己的学习效果。</p><p>这套测试题共有20道题目，包括8道单选题，12道多选题，满分100分，提交试卷后，系统会自动评分。还等什么，点击下面的按钮开始答题吧!</p><p><a href=\"http://time.geekbang.org/quiz/intro?act_id=1281&exam_id=3392\"><img src=\"https://static001.geekbang.org/resource/image/28/a4/28d1be62669b4f3cc01c36466bf811a4.png?wh=1142*201\" alt=\"\"></a></p><p>最后，我很希望听听你学习这个专栏的感受。这里我为你准备了一份<a href=\"https://jinshuju.net/f/xuZpdo\">毕业问卷</a>，题目不多，希望你可以花两分钟填一下。</p><p><a href=\"https://jinshuju.net/f/xuZpdo\"><img src=\"https://static001.geekbang.org/resource/image/3f/a0/3f48e5fb2346b2290fa10e7043710ba0.jpg?wh=1142x801\" alt=\"\"></a></p><!-- [[[read_end]]] -->","float_qrcode":"https://static001.geekbang.org/resource/image/83/1c/83e244537b714e899fd2e7896e22371c.png","article_cover_hidden":false,"is_required":false,"score":"71640275200","like_count":0,"article_subtitle":"","audio_download_url":"https://static001.geekbang.org/resource/audio/44/57/4440d8b910f6e5621a493b8621ae4657.mp3","had_viewed":false,"article_title":"期末测试 | 来赴一场满分之约吧！","column_bgcolor":"#F6F7FB","offline_package":"{}","audio_title":"12.26 海纳 26_test_01.MP3","audio_time_arr":{"m":"00","s":"34","h":"00"},"text_read_percent":0,"cid":450,"article_cshort":"<p>你好，我是海纳。</p><p>到这里，《编程高手必学的内存知识》这门课的正文内容就全部结束了。我特别给你准备了一套期末测试题，来帮助你检验自己的学习效果。</p><p>这套测试题共有20道题目，包括8道单选题，12道多选题，满分100分，提交试卷后，系统会自动评分。还等什么，点击下面的按钮开始答题吧!</p><p><a href=\"http://time.geekbang.org/quiz/intro?act_id=1281&exam_id=3392\"><img src=\"https://static001.geekbang.org/resource/image/28/a4/28d1be62669b4f3cc01c36466bf811a4.png?wh=1142*201\" alt=\"\"></a></p><p>最后，我很希望听听你学习这个专栏的感受。这里我为你准备了一份<a href=\"https://jinshuju.net/f/xuZpdo\">毕业问卷</a>，题目不多，希望你可以花两分钟填一下。</p><p><a href=\"https://jinshuju.net/f/xuZpdo\"><img src=\"https://static001.geekbang.org/resource/image/3f/a0/3f48e5fb2346b2290fa10e7043710ba0.jpg?wh=1142x801\" alt=\"\"></a></p>","video_width":0,"column_could_sub":true,"video_id":"","sku":"100094901","video_cover":"","author_name":"海纳","column_is_onboard":true,"audio_url":"https://res001.geekbang.org/media/audio/44/57/4440d8b910f6e5621a493b8621ae4657/ld/ld.m3u8","chapter_id":"2478","column_had_sub":true,"column_cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg","neighbors":{"left":{"article_title":"不定期福利第二期 | 软件篇答疑","id":454080},"right":{"article_title":"结束语 | 自主基础软件开发的那片星辰大海","id":472403}},"rate_percent":0,"footer_cover_data":{"img_url":"https://static001.geekbang.org/resource/image/f0/fe/f0c47daf3262a8df6cf352d89896abfe.png","link_url":"https://time.geekbang.org/article/427012","mp_url":""},"float_app_qrcode":"","column_is_experience":false,"rate":{"1":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"2":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"3":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0}},"product_id":100094901,"had_liked":false,"id":471878,"free_get":false,"is_video_preview":false,"article_summary":"为了帮助你检验自己的学习效果，我特别给你准备了一套结课测试题，快来挑战一下吧。","column_sale_type":0,"float_qrcode_jump":"https://time.geekbang.org/serv/v4/misc/jump?uri=https%3A%2F%2Ftime.geekbang.org%2Fhybrid%2Fmp%2Fjump%3Furl%3Dhttps%253A%252F%252Fstatic001.geekbang.org%252Fresource%252Fimage%252Fee%252Fb2%252Feedd4090668aabfac91b2b445d27cbb2.png","column_id":450,"article_sharetitle":"来赴一场满分之约吧！","article_poster_wxlite":"https://static001.geekbang.org/render/screen/a7/a5/a7817e60bf1c2a4664f9b1f0e2794da5.jpeg","article_features":0,"comment_count":0,"audio_md5":"4440d8b910f6e5621a493b8621ae4657","offline":{"size":1530969,"file_name":"0d395e567fab01bb394de9f1d647f99d","download_url":"https://static-acl-001.geekbang.org/resource/zip/article/471878/0d395e567fab01bb394de9f1d647f99d.zip?auth_key=1641482476-726d5dae621f4a55bfd2d6d4ba6aae5c-0-90ffe3d67b21550a5153e7c1cd27d4ad"},"video_size":0,"hls_videos":[],"video_time":"","article_could_preview":false,"article_ctime":1640275200,"share":{"content":"学好内存，掌握系统开发诀窍","title":"海纳 · 编程高手必学的内存知识","poster":"https://static001.geekbang.org/resource/image/bc/c5/bcaf3ff69e538f8749970edf2301bbc5.jpg","cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg"}},"472403":{"text_read_version":0,"audio_size":11384130,"article_cover":"https://static001.geekbang.org/resource/image/70/52/70c25ac05a911a1889562a7942981652.jpg","subtitles":[],"product_type":"c1","audio_dubber":"海纳","is_finished":false,"like":{"had_done":false,"count":6},"audio_time":"00:11:51","video_height":0,"article_content":"<p>你好，我是海纳。</p><p>送君千里，终有一别，我们的专栏到这里就结束了。<strong>我希望这个专栏对你来说不是一个终点，而是让你学习基础软件的起点。</strong></p><p>这个专栏虽然不可能让你通过短短的十五万字和一百来幅图，就搞明白操作系统、CPU和编译器的每一个细节。<strong>但我觉得它仍然可以帮你建立起一个大的框架，让你对计算机内存的各种工作原理有一个全面的认识</strong>。今天，我想以一种既轻松又严肃的心情，和你聊聊自主基础软件开发的那片星辰大海。</p><h2>星辰大海</h2><p>不知道你有没有听说过“程序员的三大浪漫”？有的人说这三大浪漫是指基础软件中的操作系统、编译器和数据库技术；当然也有人觉得三大浪漫是指操作系统、编译器和图形学。我个人觉得<strong>这四门学科不管是在过去、现在，还是未来，都会是计算机工业的基石</strong>。</p><p>十五年前流行的ASP、JSP技术已经被扫入历史的垃圾堆里，十年前的Struts、Hibernate也逐渐销声匿迹。一代代的应用技术就是这样不断崛起、不断消亡，但是基础软件领域却一直保持着自己的节奏正常演进。</p><p>有同学问我，下一个十年的热点技术会是什么？我说我不知道，也许是物联网？也许是增强现实、虚拟现实？甚至也可能是区块链加元宇宙，或者是单人轨道交通和自动驾驶？这些我都无从得知。</p><!-- [[[read_end]]] --><p>虽然我不能给你一个确定的答案，但我知道，<strong>对操作系统来说，未来硬件无论怎么变革，对硬件进行管理和抽象、为软件提供合适的底座的诉求永远不会消失；对编译器来说，人们对新的编程语言和编程范式的追求一定不会消失；对数据库来说，数据量在未来一定会越来越大，所以高效处理数据的需求也永远不会消失；而对于图形学来说，创造更加方便的人机交互方式和更加逼真的虚拟世界，这个追求也肯定不会消失</strong>。</p><p>这四门技术可以说是含金量很高的了，但在2010年以前，在国内做这四个方向是很难找到工作的，只有Intel、AMD、IBM等外企才会提供为数不多的工作岗位，国内只能看到零星的一些企业还在坚持走国产基础软件的道路。可想而知，当时的市场环境有多艰难。企业不赚钱，程序员的待遇当然也不会高，“三大浪漫害死人”的说法就是这一段时间兴起的。</p><p>不过，让人意想不到的是，接下来的十年，数据库技术爆发了。在这一段时间里，我们看到了阿里OceanBase、腾讯TDSQL和华为GuassDB等ICT企业大力投入到数据库领域，由此带来的技术成果也支撑了各自企业的业务飞速发展，数据库相关从业人员的个人发展也赢来了巨大的转机。</p><p>你可能会想，为什么其他三个领域就没有爆发呢？我想从三个方面来谈一下个人的看法，这三个方面分别是<strong>生态、平台和技术竞争力</strong>。而且，我觉得这三个方面不光可以总结基础软件的产业形态，还可以帮你对未来的技术方向做一个基本的判断。</p><p>我们就先从生态开始聊起吧。</p><h2>生态</h2><p>软件生态就像一个自然界真实的生态系统一样，它由各种各样的角色组成，角色之间有能量流动和物质流动。以Android生态为例，在这个生态里有芯片设计者、芯片制造商；有操作系统开发者（比如说Google）；也有像微信、支付宝这样的应用开发商；此外还有消费者。这里的消费者还可以进一步细分为手机购买者和应用使用者，不同的用户，他们的需求可能并不完全重叠。</p><p>当移动互联网刚刚兴起之时，其实并不是Android一统天下，当时还有塞班、Windows，甚至黑莓和三星的自研系统也占据了一席之地。当时不管哪一家都很难称得上是生态。大浪淘沙，慢慢地，其他的OS都掉队了。最后除苹果之外，只剩下Android一家独大了。怎么回事呢？</p><p>因为消费者逐渐发现Android手机比其他手机更好用，于是购买Android手机的人越来越多。开发者发现Android手机的市占率最高，都纷纷为它开发应用，Android手机的功能由此完善起来，消费者的购买力也逐步上升。这就是生态系统的正循环。</p><p><strong>一旦一个系统进入正循环，其他系统就很难再去抗衡了</strong>。因为对于应用开发者而言，他们会有很强的动力要求开发平台归一。只针对一个系统进行开发，无疑会极大地降低开发者的成本。</p><p>在PC市场也是一样的道理。二十年前就有很多人喊着要用国产系统干掉Windows，实际上根本不可行。一个新的OS，如果没有配套的Office、浏览器，各种音乐软件和流行的游戏也与它不兼容，试问有多少人愿意使用它呢？</p><p>我举的这两个例子，是想让你体会什么是生态的威力。<strong>对于我们个人而言，做事情一定要顺应生态成长的趋势，不要逆势而行。</strong></p><p>当然，这里面也有不一样的声音，一些人会问我：“海纳，照你这个说法，我们国内企业都没有机会了？”当然不是这样，中国经过了长期的发展，在操作系统、编程语言和编译器领域都有了不错的积累。当下一次硬件平台发生革命性变化的时候，中国的基础软件领域就能实现弯道超车。</p><p>接下来，我再来谈谈我对另外一个方面“平台”的看法。</p><h2>平台</h2><p>我们这一代人经历了两种平台，分别是个人电脑和移动端。在这两个平台各自的发展时期都诞生了一些伟大的公司，流传着可歌可泣的事迹。但最激动人心、最波澜壮阔的场景还是发生在平台切换期间，也就是2010~2015这一段时间。这时候的新思想、新技术都层出不穷。</p><p>例如虚拟DOM的思想彻底变革了移动平台前端的开发逻辑，React、Vue，乃至Flutter都能看到虚拟DOM的影响；Swift、Dart中采用的声明式UI为移动端开发提供了更多选择；移动终端的暴发也催生了微服务、容器化、边缘计算等新的服务形态，服务端从IaaS、PaaS到FaaS，开发者的心智负担越来越轻，开发效率越来越高……这样的例子还有很多，我就不再列举了。</p><p>不过有点遗憾的是，这一次移动互联网的变革并不是由中国人引领的，所以在操作系统、编程语言等领域，我们并没有拿到最大的那块蛋糕。但好的一方面是，在移动互联网时代，国内与国外的技术差距在肉眼可见地缩小。</p><p>未来，人们对信息处理的便捷性和对各行业信息化的要求还会更高。在交通、家居、制造、物流、运输和能源等领域都会出现更多的数据处理和自动化调度运维，甚至是出现人工智能的辅助。</p><p>这些需求会极大地促进计算平台的迁移。而这一次迁移将是中国基础软件行业千载难逢的机遇。我不知道未来的计算平台是车载，还是家居，或者是工业机器人，但我知道，<strong>风一定会来</strong>。</p><p>作为一个普通人，在风起之前最应该做的事情是，把自己的翅膀锻炼好，而不是去猜风来的方向。<strong>现在互联网周期逐渐见顶，行业增长明显放缓，不少企业都表示凛冬将至。但我不这样看，毕竟春天都是从寒冬中孕育。当前正是有抱负的程序员们逆周期加强学习的时机。</strong></p><p><strong>如果你选择相信，你就能看见</strong>，只有将个人命运与国家民族的命运绑定在一起，形成共振，才能最大限度地实现自我价值。而中国的科技突破已经处于黎明之前，这是一种无可置疑的必然。</p><p>那么，<strong>当我们在做选择的时候，还是选择有益于生态的方向，不能与生态对抗；要抓住平台切换的机遇，这是弯道超车的机会；还要考虑一个行业在技术上是否已经到顶。</strong></p><h2>技术竞争力</h2><p><strong>一个行业是否有前途，要看它是否有可能在未来做出更有竞争力的产品</strong>。这个道理被很多人忽略了。比如飞机制造业，虽然空客和波音的技术水平很高，但飞机制造却是一个实打实的夕阳产业。因为后来者很难在技术上对它们进行反超。在大型客机这个领域，需求已经到顶了，速度更快的客机早就出现了，但综合舒适度和经济性，现在时速800公里的客机就是最好的。所以这个行业就成为了夕阳产业。</p><p>正是基于这样的判断，我认为“三大浪漫”还有很大的空间，上面提到的四门学科离天花板还有很远，也就是说从技术上超越现有技术的可能性还非常大。它们都是大有希望的朝阳产业，所以技术竞争力是要考虑的第三个方面。</p><p>实事求是地说，<strong>科学技术的竞争，归根到底还是人才的竞争</strong>。在计算机领域，我们看到了超级程序员所起到的作用，比如Linus、Chris Lattner、James Gosling等天才程序员，他们创造的丰功伟绩让后来者不断赞叹。</p><p>中国的程序员虽然号称有百万之众，但是真正有影响力的天才程序员还是屈指可数。一方面是我们起步晚，技术积累不足，另一方面是国内的计算机教育太落后。在基础软件领域，我们即使有好的idea，也经常因为人的能力不足而无法实现。</p><p>国内外计算机教育的差距到底有多大呢？Berkeley的本科毕业生在上世纪70年代，就能在本科设计中做出PostgreSQL这种级别的作品，而我们的本科生有很多连图的深度优先遍历都未必能正确完整地写下来。而且我们国内的教材和PPT几十年也不更新，现在有很多高校在讲汇编的时候还在使用实模式汇编，讲计算机原理时还在使用8259A讲解中断。</p><p>每次和一些朋友聊起这个现状的时候，我们都感到深深的忧虑，这也就促使我想把学校里过时的知识指出来，让你少走弯路，同时把学校里没讲到的最有价值、最有用的东西分享给你。所以，未来我肯定还会坚持写作，以不同的形式呈现给你。<strong>我坚信，学习计算机知识没有捷径，但却有一条正确的路</strong>。</p><p>在专栏的更新过程中，我看到有不少同学说课程学起来很难、很枯燥，但我还是固执地用这样的风格完成了这个专栏。<strong>因为内存知识是我们学习操作系统的基本功，一旦基本功学通了，那之后学习各种计算机底层的软硬件知识就很容易</strong>。这样的课注定不会比速成课招人喜欢，但我认为从长期来看，它反而更实用。</p><p>中国的程序员并不少，但是高级程序员还远远不够。相比起现在的世界第一美国，我们的人才培养还有很长的路要走。就像我开头说的，希望你不要把这个专栏当做一个终点，而是要去探索基础软件的那片星辰大海，希望你成为我国信息技术的核心骨干，甚至是世界一流的超级程序员，希望你承载着我们这一辈的梦想继续前行……</p><p>“<strong>天不老，情难绝，心似双丝网，中有千千结</strong>”，这是我在学生时代写在日记本上的一句话，此刻这句话不断地浮现在我的心上，虽然很不想说再见，但我也只能暂时陪你到这了。真心地希望这一段旅程、这一个专栏能让你感受到成长，能帮助你到达一个新的阶段。</p><p>我知道很多同学都喜欢默默地学习，当一个“潜水党”，在专栏结束的今天，我希望能听到你的声音，听到你学习这门课的收获。最后，谢谢你看到这，我想对你说，为了世界第一的梦想，朋友们，我们江湖再见！</p><p><a href=\"https://jinshuju.net/f/xuZpdo\"><img src=\"https://static001.geekbang.org/resource/image/3f/a0/3f48e5fb2346b2290fa10e7043710ba0.jpg?wh=1142x801\" alt=\"\"></a></p>","float_qrcode":"https://static001.geekbang.org/resource/image/83/1c/83e244537b714e899fd2e7896e22371c.png","article_cover_hidden":false,"is_required":true,"score":"71640534400","like_count":3,"article_subtitle":"","audio_download_url":"https://static001.geekbang.org/resource/audio/c6/d1/c6dc52a130cae32de9f4668a70df96d1.mp3","had_viewed":false,"article_title":"结束语 | 自主基础软件开发的那片星辰大海","column_bgcolor":"#F6F7FB","offline_package":"{}","audio_title":"12.26 海纳 25_the_end_01.MP3","audio_time_arr":{"m":"11","s":"51","h":"00"},"text_read_percent":0,"cid":450,"article_cshort":"<p>你好，我是海纳。</p><p>送君千里，终有一别，我们的专栏到这里就结束了。<strong>我希望这个专栏对你来说不是一个终点，而是让你学习基础软件的起点。</strong></p><p>这个专栏虽然不可能让你通过短短的十五万字和一百来幅图，就搞明白操作系统、CPU和编译器的每一个细节。<strong>但我觉得它仍然可以帮你建立起一个大的框架，让你对计算机内存的各种工作原理有一个全面的认识</strong>。今天，我想以一种既轻松又严肃的心情，和你聊聊自主基础软件开发的那片星辰大海。</p><h2>星辰大海</h2><p>不知道你有没有听说过“程序员的三大浪漫”？有的人说这三大浪漫是指基础软件中的操作系统、编译器和数据库技术；当然也有人觉得三大浪漫是指操作系统、编译器和图形学。我个人觉得<strong>这四门学科不管是在过去、现在，还是未来，都会是计算机工业的基石</strong>。</p><p>十五年前流行的ASP、JSP技术已经被扫入历史的垃圾堆里，十年前的Struts、Hibernate也逐渐销声匿迹。一代代的应用技术就是这样不断崛起、不断消亡，但是基础软件领域却一直保持着自己的节奏正常演进。</p><p>有同学问我，下一个十年的热点技术会是什么？我说我不知道，也许是物联网？也许是增强现实、虚拟现实？甚至也可能是区块链加元宇宙，或者是单人轨道交通和自动驾驶？这些我都无从得知。</p>","video_width":0,"column_could_sub":true,"video_id":"","sku":"100094901","video_cover":"","author_name":"海纳","column_is_onboard":true,"audio_url":"https://res001.geekbang.org/media/audio/c6/d1/c6dc52a130cae32de9f4668a70df96d1/ld/ld.m3u8","chapter_id":"2478","column_had_sub":true,"column_cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg","neighbors":{"left":{"article_title":"期末测试 | 来赴一场满分之约吧！","id":471878},"right":[]},"rate_percent":0,"footer_cover_data":{"img_url":"https://static001.geekbang.org/resource/image/f0/fe/f0c47daf3262a8df6cf352d89896abfe.png","link_url":"https://time.geekbang.org/article/427012","mp_url":""},"float_app_qrcode":"","column_is_experience":false,"rate":{"1":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"2":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0},"3":{"cur_version":0,"max_rate":0,"cur_rate":0,"is_finished":false,"total_rate":0,"learned_seconds":0}},"product_id":100094901,"had_liked":false,"id":472403,"free_get":false,"is_video_preview":false,"article_summary":"当我们在做选择的时候，还是选择有益于生态的方向，不能与生态对抗；要抓住平台切换的机遇，这是弯道超车的机会；还要考虑一个行业在技术上是否已经到顶。","column_sale_type":0,"float_qrcode_jump":"https://time.geekbang.org/serv/v4/misc/jump?uri=https%3A%2F%2Ftime.geekbang.org%2Fhybrid%2Fmp%2Fjump%3Furl%3Dhttps%253A%252F%252Fstatic001.geekbang.org%252Fresource%252Fimage%252Fee%252Fb2%252Feedd4090668aabfac91b2b445d27cbb2.png","column_id":450,"article_sharetitle":"结束语 | 自主基础软件开发的那片星辰大海","article_poster_wxlite":"https://static001.geekbang.org/render/screen/32/16/32e0879903e2197581c5d37d87540716.jpeg","article_features":0,"comment_count":8,"audio_md5":"c6dc52a130cae32de9f4668a70df96d1","offline":{"size":12005178,"file_name":"36e12afe5d8c619db1a81048dd85c86f","download_url":"https://static-acl-001.geekbang.org/resource/zip/article/472403/36e12afe5d8c619db1a81048dd85c86f.zip?auth_key=1641482492-33fd5e1e09a946a4b8c4db402ac68d8c-0-1611f088e5f76f2b068983ee4514e6e9"},"video_size":0,"hls_videos":[],"video_time":"","article_could_preview":false,"article_ctime":1640534400,"share":{"content":"学好内存，掌握系统开发诀窍","title":"海纳 · 编程高手必学的内存知识","poster":"https://static001.geekbang.org/resource/image/bc/c5/bcaf3ff69e538f8749970edf2301bbc5.jpg","cover":"https://static001.geekbang.org/resource/image/97/25/9730a2f044d45d396e8e9e40e5968e25.jpg"}}}